{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome to my first book. It's free, it's open source, and it's comprehensive. Those attributes also describe two of my favorite technologies, and I'm basing my first book on them. The first is Xamarin Forms , a technology that allows you to develop cross-platform mobile applications using C# and the .NET framework. The second is Azure Mobile Apps , a technology that allows you to connect your mobile app to resources that are important in cloud connected mobile applications such as table data, authentication, and push notifications. This book does not tell you everything there is to know about either topic. It focuses on the topics necessary to get your mobile apps connected to the cloud. UPDATE: .NET Client v4.2.0 In November 2020, Microsoft released v4.2.0 of the .NET Client, which upgraded support to .NET Standard 2.0, along with support for the latest versions of the iOS and Android operating systems. As a result, some instructions are out of date. I'm in progress of updating the documentation to reflect this change. What are Cloud Connected Mobile Apps? \u00b6 I guess I should define some of the terminology that I am going to use. When I refer to a mobile application or mobile app , I mean every piece of software that is related to the application you want to use. This includes, for example, the mobile client . This is the piece of code you run on your iPhone or Android phone. It also includes the mobile backend which is the service that you run in the cloud to provide important services to your mobile client. A cloud connected mobile application is a mobile client that connects to a mobile backend for shared services. Quite a few of the apps on your phone are cloud connected already. For example, Instagram uses the cloud for photo storage, and Facebook uses the cloud to store the news feeds of you and your friends. Why Cross-Platform Native Development is important? \u00b6 It should come as no surprise that Apple and Google have pretty much won the mobile OS wars. Over 90% of the smart phones being sold today run either iOS or Android. However, these are two very different mobile operating systems, with different programming models. iOS is based on either Swift or Objective-C. Android is based on Java . If you want to develop for the 80% case (and you should), then you need to know both Swift and Java. That's a tall order even for the most dedicated mobile developer. However, there are alternatives out there. Most notable, you can write your mobile application with one code-base and just write exceptions for when the platforms diverge. You have to pick a single language and a tool set that supports cross-platform development to do this. Not all cross-platform tool sets are created equal, however. Some do not compile your code to native binaries, which means that you do not get access to all the functionality of the mobile platforms you are targeting. Xamarin, recently acquired by Microsoft, allows you to target all major platforms - iOS, Android and Windows - to gain greater than 95% coverage of the mobile smart phone market. It does this by leveraging the .NET framework and compiling your code to a native binary for each platform. Xamarin.Forms is a cross-platform framework, based on XAML and .NET, that allows you to use common UI pages to develop your apps. Why Azure Mobile Apps? \u00b6 When you think about the major apps in the marketplace for each mobile platform, the thing that they have in common is that they have some sort of cloud infrastructure driving them. It might be as simple as storing your task list, or as complex as your Facebook news feed. It could be a gaming leader board, or the social sharing of your photos. Whatever it is, cloud connectivity is a must. Not all clouds are created equal. There are some common features that you should think about including irrespective of the application. I like to use Azure Mobile Apps for these features because they are all included and you can get started with most of the features for zero cost. Even the features that cannot be obtained without spending a little money are relatively cheap. Info Azure Mobile Apps is a feature of Azure App Service. Azure App Service is a collection of services that commonly are used together to develop modern Internet Apps. This includes web hosting, API hosting and Mobile SDKs. Features of Cloud Connected Mobile Apps \u00b6 A cloud connected mobile application will use one or more services in the following areas: Authentication Storage of structured data (like a task list) Storage of unstructured data (like photographs) Push notifications Invocation of Custom Code I am going to cover each of these in great detail. In addition, I will also cover some common issues and solutions that developers run into while developing cloud connected mobile applications such as testing and going to production. Aside from the actual features of mobile apps, there are other things to consider while developing your mobile application. Here is my list, in no particular order: Continuous Deployment Slots or Staging Sites Automatic Scalability Database Backups Combined Web The point here is that my intent is to write a production quality application. I need to be able to deploy my site with confidence without resorting to jumping through hoops. I want to run multiple versions of the backend so that I can run a staging site for testing purposes. I want to be able to roll back my production site to a previous version at a moments notice. I want to be able to handle the load when my app is successful, and I want things to be backed up (since bad things inevitably happen when I am least prepared). All of these features are available in Azure App Service, and the Mobile Apps SDK that I will use throughout the book is supported only on Azure App Service. Who is This Book For? \u00b6 This book is for intermediate to experienced C# developers who have already built a mobile app with Xamarin and want to take their mobile apps to the next level by utilizing cloud services. This book is not for the beginner. Explicitly, I already expect you to know how to develop mobile applications with C# and Xamarin technologies. If you are unfamiliar with the C# language, you can get started with a free course on the Internet. The basics of the language can be learned at www.learncs.org . Once you have the language basics under your belt, you can move on to building mobile applications with Xamarin. You can learn more about developing cross-platform mobile development with Xamarin at the Xamarin website. Although you do not need to understand ASP.NET to get value out of this book, be aware that the mobile back ends that I will be covering are written in C# and ASP.NET. A good understanding of ASP.NET will assist you. Things You Should Know! \u00b6 Before you get started with development, spend some time learning the tools of the trade. The command prompt on the Mac is bash and the command prompt on the PC is PowerShell . You should be proficient in the shell on the platforms that you use. Additionally, you should become familiar with the source code control system that you will use. For most, this means becoming familiar with git . Don't even think of developing without using source control. What You Will Need \u00b6 The list of hardware and software for mobile development is longer than your typical development projects. It is still, thankfully, relatively short and easy to acquire. Hardware \u00b6 You will want a computer on which to develop code. If you develop iOS applications, then you MUST have a Mac running the latest version of Mac OSX. If you develop Universal Windows applications, then you MUST have a PC running Windows 10. Android applications can be developed on either platform. My own experience has taught me that the tooling for developing mobile backends in C# and ASP.NET (our primary languages during the course of this book) are better on a PC running Windows 10. Thus, my hardware choice is a rather beefy Windows 10 PC for my main development system. In addition, I have a Mac Mini underneath my desk that I use to build the iOS portions of the applications. Software \u00b6 All of the following software are freely available. You should install each package and update it (if appropriate) so that it is fully patched. On your Mac \u00b6 XCode (available on the Mac App Store) Visual Studio for Mac Android Studio and Tools (if you intend to build Android apps on the Mac) You must run XCode at least once after installation so that you can accept the license agreement. On your Windows PC \u00b6 Android Studio and Tools Visual Studio 2017 Community When installing Visual Studio, you will want to install the following workloads: Universal Windows Platform development. ASP.NET and web development. Azure development. Data storage and processing. Mobile development with .NET. If you have already installed Visual Studio and did not install these components, run the installer again to add the components. You can also use earlier versions of Visual Studio. If you do use an earlier version, then the screen shots I provide will not match up. You will also need the latest version of the Azure SDK installed. Tip Development Tools are big, multi-gigabyte installers. If you are on a slow or restricted link, you may want to download the installers onto a thumb drive for local installation. Cloud Services \u00b6 You will need an Azure account to complete most of the tutorials in this book. In fact, you won't be able to get very far without one. If you have an MSDN account, you already have access to free Azure resources. You just need to log into your MSDN account and activate your Azure benefit. Students may be able to get access to Microsoft Imagine from school resources, but this is not suitable for developing mobile applications. This is because storage costs money. If you don't have MSDN, then there is a free trial available. Once the trial period ends, you can move to a Pay-As-You-Go account and continue to use free services without incurring a charge. I'll point out when you are going to incur charges on your Azure account, but I will be using free resources most of the time. Aside from Azure resources, you will want some place to store your code. This doesn't have to be in the cloud. If you want to use the cloud, you can use GitHub or Visual Studio Team Services. Both are free to use. GitHub provides public repositories for free. Visual Studio Team Services provides private respositories for free. Visual Studio Team Services also includes other services that I will talk about during the course of the book, some of which may incur cost. I will be publishing all my samples and tutorial code on GitHub so that you can easily download it. You don't have to use one of these resources, but I won't be covering other service usage. You will need a Developer Account for the appropriate app store if you intend to distribute your mobile clients or if you intend to use specific cloud services. Apple is specific - if you intend to use push notifications or distribute iOS apps, then you need an Apple Developer Account , Google Developer Account and/or Windows Store Developer Account . The terms of the accounts are changed constantly, so review the current terms when you sign up. My recommendation is to defer signing up for these programs until you need something they offer. Now, let's get developing! Our next section is dependent on where you are developing: On a Mac, skip ahead to the Mac section . On a PC, the next section covers Windows.","title":"Getting Started"},{"location":"#introduction","text":"Welcome to my first book. It's free, it's open source, and it's comprehensive. Those attributes also describe two of my favorite technologies, and I'm basing my first book on them. The first is Xamarin Forms , a technology that allows you to develop cross-platform mobile applications using C# and the .NET framework. The second is Azure Mobile Apps , a technology that allows you to connect your mobile app to resources that are important in cloud connected mobile applications such as table data, authentication, and push notifications. This book does not tell you everything there is to know about either topic. It focuses on the topics necessary to get your mobile apps connected to the cloud. UPDATE: .NET Client v4.2.0 In November 2020, Microsoft released v4.2.0 of the .NET Client, which upgraded support to .NET Standard 2.0, along with support for the latest versions of the iOS and Android operating systems. As a result, some instructions are out of date. I'm in progress of updating the documentation to reflect this change.","title":"Introduction"},{"location":"#what-are-cloud-connected-mobile-apps","text":"I guess I should define some of the terminology that I am going to use. When I refer to a mobile application or mobile app , I mean every piece of software that is related to the application you want to use. This includes, for example, the mobile client . This is the piece of code you run on your iPhone or Android phone. It also includes the mobile backend which is the service that you run in the cloud to provide important services to your mobile client. A cloud connected mobile application is a mobile client that connects to a mobile backend for shared services. Quite a few of the apps on your phone are cloud connected already. For example, Instagram uses the cloud for photo storage, and Facebook uses the cloud to store the news feeds of you and your friends.","title":"What are Cloud Connected Mobile Apps?"},{"location":"#why-cross-platform-native-development-is-important","text":"It should come as no surprise that Apple and Google have pretty much won the mobile OS wars. Over 90% of the smart phones being sold today run either iOS or Android. However, these are two very different mobile operating systems, with different programming models. iOS is based on either Swift or Objective-C. Android is based on Java . If you want to develop for the 80% case (and you should), then you need to know both Swift and Java. That's a tall order even for the most dedicated mobile developer. However, there are alternatives out there. Most notable, you can write your mobile application with one code-base and just write exceptions for when the platforms diverge. You have to pick a single language and a tool set that supports cross-platform development to do this. Not all cross-platform tool sets are created equal, however. Some do not compile your code to native binaries, which means that you do not get access to all the functionality of the mobile platforms you are targeting. Xamarin, recently acquired by Microsoft, allows you to target all major platforms - iOS, Android and Windows - to gain greater than 95% coverage of the mobile smart phone market. It does this by leveraging the .NET framework and compiling your code to a native binary for each platform. Xamarin.Forms is a cross-platform framework, based on XAML and .NET, that allows you to use common UI pages to develop your apps.","title":"Why Cross-Platform Native Development is important?"},{"location":"#why-azure-mobile-apps","text":"When you think about the major apps in the marketplace for each mobile platform, the thing that they have in common is that they have some sort of cloud infrastructure driving them. It might be as simple as storing your task list, or as complex as your Facebook news feed. It could be a gaming leader board, or the social sharing of your photos. Whatever it is, cloud connectivity is a must. Not all clouds are created equal. There are some common features that you should think about including irrespective of the application. I like to use Azure Mobile Apps for these features because they are all included and you can get started with most of the features for zero cost. Even the features that cannot be obtained without spending a little money are relatively cheap. Info Azure Mobile Apps is a feature of Azure App Service. Azure App Service is a collection of services that commonly are used together to develop modern Internet Apps. This includes web hosting, API hosting and Mobile SDKs.","title":"Why Azure Mobile Apps?"},{"location":"#features-of-cloud-connected-mobile-apps","text":"A cloud connected mobile application will use one or more services in the following areas: Authentication Storage of structured data (like a task list) Storage of unstructured data (like photographs) Push notifications Invocation of Custom Code I am going to cover each of these in great detail. In addition, I will also cover some common issues and solutions that developers run into while developing cloud connected mobile applications such as testing and going to production. Aside from the actual features of mobile apps, there are other things to consider while developing your mobile application. Here is my list, in no particular order: Continuous Deployment Slots or Staging Sites Automatic Scalability Database Backups Combined Web The point here is that my intent is to write a production quality application. I need to be able to deploy my site with confidence without resorting to jumping through hoops. I want to run multiple versions of the backend so that I can run a staging site for testing purposes. I want to be able to roll back my production site to a previous version at a moments notice. I want to be able to handle the load when my app is successful, and I want things to be backed up (since bad things inevitably happen when I am least prepared). All of these features are available in Azure App Service, and the Mobile Apps SDK that I will use throughout the book is supported only on Azure App Service.","title":"Features of Cloud Connected Mobile Apps"},{"location":"#who-is-this-book-for","text":"This book is for intermediate to experienced C# developers who have already built a mobile app with Xamarin and want to take their mobile apps to the next level by utilizing cloud services. This book is not for the beginner. Explicitly, I already expect you to know how to develop mobile applications with C# and Xamarin technologies. If you are unfamiliar with the C# language, you can get started with a free course on the Internet. The basics of the language can be learned at www.learncs.org . Once you have the language basics under your belt, you can move on to building mobile applications with Xamarin. You can learn more about developing cross-platform mobile development with Xamarin at the Xamarin website. Although you do not need to understand ASP.NET to get value out of this book, be aware that the mobile back ends that I will be covering are written in C# and ASP.NET. A good understanding of ASP.NET will assist you.","title":"Who is This Book For?"},{"location":"#things-you-should-know","text":"Before you get started with development, spend some time learning the tools of the trade. The command prompt on the Mac is bash and the command prompt on the PC is PowerShell . You should be proficient in the shell on the platforms that you use. Additionally, you should become familiar with the source code control system that you will use. For most, this means becoming familiar with git . Don't even think of developing without using source control.","title":"Things You Should Know!"},{"location":"#what-you-will-need","text":"The list of hardware and software for mobile development is longer than your typical development projects. It is still, thankfully, relatively short and easy to acquire.","title":"What You Will Need"},{"location":"#hardware","text":"You will want a computer on which to develop code. If you develop iOS applications, then you MUST have a Mac running the latest version of Mac OSX. If you develop Universal Windows applications, then you MUST have a PC running Windows 10. Android applications can be developed on either platform. My own experience has taught me that the tooling for developing mobile backends in C# and ASP.NET (our primary languages during the course of this book) are better on a PC running Windows 10. Thus, my hardware choice is a rather beefy Windows 10 PC for my main development system. In addition, I have a Mac Mini underneath my desk that I use to build the iOS portions of the applications.","title":"Hardware"},{"location":"#software","text":"All of the following software are freely available. You should install each package and update it (if appropriate) so that it is fully patched.","title":"Software"},{"location":"#on-your-mac","text":"XCode (available on the Mac App Store) Visual Studio for Mac Android Studio and Tools (if you intend to build Android apps on the Mac) You must run XCode at least once after installation so that you can accept the license agreement.","title":"On your Mac"},{"location":"#on-your-windows-pc","text":"Android Studio and Tools Visual Studio 2017 Community When installing Visual Studio, you will want to install the following workloads: Universal Windows Platform development. ASP.NET and web development. Azure development. Data storage and processing. Mobile development with .NET. If you have already installed Visual Studio and did not install these components, run the installer again to add the components. You can also use earlier versions of Visual Studio. If you do use an earlier version, then the screen shots I provide will not match up. You will also need the latest version of the Azure SDK installed. Tip Development Tools are big, multi-gigabyte installers. If you are on a slow or restricted link, you may want to download the installers onto a thumb drive for local installation.","title":"On your Windows PC"},{"location":"#cloud-services","text":"You will need an Azure account to complete most of the tutorials in this book. In fact, you won't be able to get very far without one. If you have an MSDN account, you already have access to free Azure resources. You just need to log into your MSDN account and activate your Azure benefit. Students may be able to get access to Microsoft Imagine from school resources, but this is not suitable for developing mobile applications. This is because storage costs money. If you don't have MSDN, then there is a free trial available. Once the trial period ends, you can move to a Pay-As-You-Go account and continue to use free services without incurring a charge. I'll point out when you are going to incur charges on your Azure account, but I will be using free resources most of the time. Aside from Azure resources, you will want some place to store your code. This doesn't have to be in the cloud. If you want to use the cloud, you can use GitHub or Visual Studio Team Services. Both are free to use. GitHub provides public repositories for free. Visual Studio Team Services provides private respositories for free. Visual Studio Team Services also includes other services that I will talk about during the course of the book, some of which may incur cost. I will be publishing all my samples and tutorial code on GitHub so that you can easily download it. You don't have to use one of these resources, but I won't be covering other service usage. You will need a Developer Account for the appropriate app store if you intend to distribute your mobile clients or if you intend to use specific cloud services. Apple is specific - if you intend to use push notifications or distribute iOS apps, then you need an Apple Developer Account , Google Developer Account and/or Windows Store Developer Account . The terms of the accounts are changed constantly, so review the current terms when you sign up. My recommendation is to defer signing up for these programs until you need something they offer. Now, let's get developing! Our next section is dependent on where you are developing: On a Mac, skip ahead to the Mac section . On a PC, the next section covers Windows.","title":"Cloud Services"},{"location":"android_appendix/","text":"Android Developer Notes \u00b6 This chapter contains random notes that I discovered while developing mobile apps with Xamarin Forms on the Android platform. I hope they are useful to you. Unfortunately, your app has stopped \u00b6 Sometimes, when operating with Android emulators, things will just break. Some of the ideas here may assist with getting things going again: Don't use a Shared Mono Runtime In Visual Studio, right-click the Android project, then select Properties or Options . In the Android Build section, select the General tab. Uncheck the checkbox next to Use Shared Mono Runtime . Disable non-required ABIs Also in Visual Studio, right-click the Android project, then select Properties or Options . In the Android Build section, select the Advanced tab. Select only the ABIs that you are using. For example, if you are running on the x86 emulator, you will want to disable the armeabi ABI. Rebuild the entire project and then try running again. Missing libaot-mscorlib.dll.so \u00b6 When running an application is debug mode, I sometimes saw the following deployment issue: D/Mono ( 1366): AOT module 'mscorlib.dll.so' not found: dlopen failed: library \"/data/app-lib/TaskList.Droid-2/libaot-mscorlib.dll.so\" not found To fix this: Right-click on the Droid project and select Properties . Select the Android Options tab. Uncheck the Use Fast Deployment option. Save the properties sheet. Redeploy the application. Fixing Errors with the Visual Studio Emulator for Android \u00b6 One of the issues I found while running on the Visual Studio Emulator for Android involved debugging. The Android app starts, then immediately closes and debugging stops. In the output window, you see Could not connect to the debugger . To fix this: Close the Android Emulator window. Open the Hyper-V Manager . Right-click the emulator you are trying to use and select Settings... . Expand the Processor node and select Compatibility . Check the Migrate to a physical computer with a different processor version box. Click on OK . It's a good idea to do this on all the emulators. When you start the emulator, this error should be gone. Disabling Visual Studio Emulator for Android \u00b6 There are two emulators for Android. One is supplied by Microsoft (the Visual Studio Emulator for Android) and one is supplied by Google (the Google Android Emulator). You can only use one of them. To enable the usage of the Google Android Emulator, you have to disable the Visual Studio Emulator for Android. To disable the Visual Studio Emulator for Android, disable Hyper-V with this command: bcdedit /set hypervisorlaunchtype off You should then reboot. If you wish to switch back to using the Visual Studio Emulator for Android, set the hypervisorlaunchtype to auto and reboot again.","title":"Android Developer Notes"},{"location":"android_appendix/#android-developer-notes","text":"This chapter contains random notes that I discovered while developing mobile apps with Xamarin Forms on the Android platform. I hope they are useful to you.","title":"Android Developer Notes"},{"location":"android_appendix/#unfortunately-your-app-has-stopped","text":"Sometimes, when operating with Android emulators, things will just break. Some of the ideas here may assist with getting things going again: Don't use a Shared Mono Runtime In Visual Studio, right-click the Android project, then select Properties or Options . In the Android Build section, select the General tab. Uncheck the checkbox next to Use Shared Mono Runtime . Disable non-required ABIs Also in Visual Studio, right-click the Android project, then select Properties or Options . In the Android Build section, select the Advanced tab. Select only the ABIs that you are using. For example, if you are running on the x86 emulator, you will want to disable the armeabi ABI. Rebuild the entire project and then try running again.","title":"Unfortunately, your app has stopped"},{"location":"android_appendix/#missing-libaot-mscorlibdllso","text":"When running an application is debug mode, I sometimes saw the following deployment issue: D/Mono ( 1366): AOT module 'mscorlib.dll.so' not found: dlopen failed: library \"/data/app-lib/TaskList.Droid-2/libaot-mscorlib.dll.so\" not found To fix this: Right-click on the Droid project and select Properties . Select the Android Options tab. Uncheck the Use Fast Deployment option. Save the properties sheet. Redeploy the application.","title":"Missing libaot-mscorlib.dll.so"},{"location":"android_appendix/#fixing-errors-with-the-visual-studio-emulator-for-android","text":"One of the issues I found while running on the Visual Studio Emulator for Android involved debugging. The Android app starts, then immediately closes and debugging stops. In the output window, you see Could not connect to the debugger . To fix this: Close the Android Emulator window. Open the Hyper-V Manager . Right-click the emulator you are trying to use and select Settings... . Expand the Processor node and select Compatibility . Check the Migrate to a physical computer with a different processor version box. Click on OK . It's a good idea to do this on all the emulators. When you start the emulator, this error should be gone.","title":"Fixing Errors with the Visual Studio Emulator for Android"},{"location":"android_appendix/#disabling-visual-studio-emulator-for-android","text":"There are two emulators for Android. One is supplied by Microsoft (the Visual Studio Emulator for Android) and one is supplied by Google (the Google Android Emulator). You can only use one of them. To enable the usage of the Google Android Emulator, you have to disable the Visual Studio Emulator for Android. To disable the Visual Studio Emulator for Android, disable Hyper-V with this command: bcdedit /set hypervisorlaunchtype off You should then reboot. If you wish to switch back to using the Visual Studio Emulator for Android, set the hypervisorlaunchtype to auto and reboot again.","title":"Disabling Visual Studio Emulator for Android"},{"location":"credits/","text":"Credits \u00b6 I got a lot of assistance during the writing of this book. The following people assisted (in no particular order): Austin Emser Chris Gillum Chris Risner Donna Malayeri Erich Brunner Fabio Cavalcante James Montemagno Kevin Hilscher Marthin Freij Mike James Pierce Boggan Steve Lee In addition, several people helped by correcting formatting and spelling mistakes. A huge thank you to everyone who participated in this book. It would not have been possible without you.","title":"Credits"},{"location":"credits/#credits","text":"I got a lot of assistance during the writing of this book. The following people assisted (in no particular order): Austin Emser Chris Gillum Chris Risner Donna Malayeri Erich Brunner Fabio Cavalcante James Montemagno Kevin Hilscher Marthin Freij Mike James Pierce Boggan Steve Lee In addition, several people helped by correcting formatting and spelling mistakes. A huge thank you to everyone who participated in this book. It would not have been possible without you.","title":"Credits"},{"location":"references/","text":"References \u00b6 Further Reading \u00b6 API References \u00b6 Samples \u00b6 How to get Help \u00b6","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#further-reading","text":"","title":"Further Reading"},{"location":"references/#api-references","text":"","title":"API References"},{"location":"references/#samples","text":"","title":"Samples"},{"location":"references/#how-to-get-help","text":"","title":"How to get Help"},{"location":"vs_extensions/","text":"Visual Studio Extensions \u00b6 This is the list of Visual Studio Extensions I use when developing for a combined web and mobile application: .ignore \u00b6 There are many files that get added to projects that don't really need an editor. They get set once and then forgotten about. One of those is .gitignore . There are others that follow this same pattern. So there is an extension for adding them and dealing with them. More Information Add New File \u00b6 Not all files that you need to create have an extension. A couple that I tend to have to deal with are README and LICENSE. They have no extension. Others include the .babelrc file for configuring BabelJS. With this extension, you can create any file with any extension you need. More Information Editor Enhancements \u00b6 I love small, single function extensions like this. This one provides HTML and URL encoding - something I have to do a lot of when I am developing for the web. More Information EditorConfig \u00b6 There is a specification for a file called .editorconfig that will configure your editor for the requirements of a project in terms of character set, line ending, what a tab means and so on. This extension reads that file and configured the settings for the project accordingly. More Information File Icons \u00b6 Visual Studio comes with a pretty good list of file icons that appear in the Solution Explorer to tell you what sort of file it is. But it isn't as exhaustive as it could be. Two of our extensions thus far have dealt with adding files that Visual Studio can't handle - this gives them icons. More Information Glyphfriend 2015 \u00b6 In our web applications, we use a lot of glyphs. These are available through web frameworks like Bootstrap . This extensions shows what the glyph actually looks like right in the editor, which helps me ensure what I am doing is correct. More Information Indent Guides \u00b6 I sometimes write really long blocks of code. This extension tells me where the blocks start and end. More Information NPM Task Runner \u00b6 If you are doing web development, this is not only recommended; it's vital. It allows you to run npm commands from within the Task Runner Explorer, thus relieving you of the process of dropping down to the command line. More Information Open Command Line \u00b6 For those occassions when you just can't avoid dropping down to the command line, this will ensure your command line starts at the right place. Package Installer \u00b6 In web development, sometimes you need to grab a package from elsewhere. Unfortunately, there are a dozen different ways of grabbing that package. This extension deals with all the myriad ways of getting the package. More Information Regex Tester \u00b6 Are you a regex master? Me neither, which is why I like to test all my regular expressions before they go in my code. This extension adds that functionality as a window. More Information Roslynator \u00b6 Roslyn was a major step forward in the power of C#, and there are a number of ways that it makes your code cleaner. Unfortunately, you have to learn them - all 160+ of them. This extension looks for common refactorings for you, so you can carry on coding as before. Over time, you will learn how to do those things in Roslyn right the first time. More Information SQLite / SQL Server Compact Toolbox \u00b6 Azure Mobile Apps uses SQLite as the basis of its offline cache. That means that occassionally you are going to need to peek inside the SQLite database, even if you are only curious. More Information SQLite for Universal Windows Platform \u00b6 Did I mention Azure Mobile Apps uses SQLite? If you are developing UWP applications, you will need this extension. More Information Trailing Whitespace Visualizer \u00b6 Languages are sometimes a little problematic when it comes to embedded white space at the end of lines, especially in multi-line strings. This extension shows them up in the editor, allowing you to easily find and destroy them. More Information Web Essentials \u00b6 There are few extensions that are more needed when you switch to web development. This provides capabilities for all the common file types used for web development, including style sheets and JavaScript files. More Information Web Extension Package \u00b6 This isn't one extension. It's several. There are several extension modules for image optimizations, bundling, icon handling and accessibility monitoring. This one extension installs all the others. Use this if you are going to be doing web + mobile development. More Information Xamarin Forms Player \u00b6 One of the big gotchas in Xamarin Forms development is the process by which the XAML is parsed, built and reviewed. If you install the Xamarin Forms Player onto a device, this extension will push your XAML to that app and the app will render the page for you, allowing a much tighter development cycle. More Information Xamarin Forms Templates \u00b6 The default Xamarin Forms templates include projects for iOS and Android. I wanted UWP, Windows Phone 8.1 and potentially others as well. This set of templates gives me those additional templates. More Information Xamarin Test Recorder \u00b6 Creating UI tests is painful. Running them on a large number of mobile devices is also painful. Fortunately, this extension handles the former - creating UI tests. You can run the same UI tests across thousands of devices by using Visual Studio Mobile Center testing facilities. More Information XAML Styler \u00b6 I find organizing XAML tedious. Fortunately, I can clean up my XAML and give it a consistent style by using this extension. You may not agree with its opinions, but at least it will make your code consistent. More Information Taking a look at all these extensions, a big shout-out goes to Mads Kristensen . He may work at Microsoft, but he puts out the great web extensions that make Visual Studio a major player in that space.","title":"Visual Studio Extensions"},{"location":"vs_extensions/#visual-studio-extensions","text":"This is the list of Visual Studio Extensions I use when developing for a combined web and mobile application:","title":"Visual Studio Extensions"},{"location":"vs_extensions/#ignore","text":"There are many files that get added to projects that don't really need an editor. They get set once and then forgotten about. One of those is .gitignore . There are others that follow this same pattern. So there is an extension for adding them and dealing with them. More Information","title":".ignore"},{"location":"vs_extensions/#add-new-file","text":"Not all files that you need to create have an extension. A couple that I tend to have to deal with are README and LICENSE. They have no extension. Others include the .babelrc file for configuring BabelJS. With this extension, you can create any file with any extension you need. More Information","title":"Add New File"},{"location":"vs_extensions/#editor-enhancements","text":"I love small, single function extensions like this. This one provides HTML and URL encoding - something I have to do a lot of when I am developing for the web. More Information","title":"Editor Enhancements"},{"location":"vs_extensions/#editorconfig","text":"There is a specification for a file called .editorconfig that will configure your editor for the requirements of a project in terms of character set, line ending, what a tab means and so on. This extension reads that file and configured the settings for the project accordingly. More Information","title":"EditorConfig"},{"location":"vs_extensions/#file-icons","text":"Visual Studio comes with a pretty good list of file icons that appear in the Solution Explorer to tell you what sort of file it is. But it isn't as exhaustive as it could be. Two of our extensions thus far have dealt with adding files that Visual Studio can't handle - this gives them icons. More Information","title":"File Icons"},{"location":"vs_extensions/#glyphfriend-2015","text":"In our web applications, we use a lot of glyphs. These are available through web frameworks like Bootstrap . This extensions shows what the glyph actually looks like right in the editor, which helps me ensure what I am doing is correct. More Information","title":"Glyphfriend 2015"},{"location":"vs_extensions/#indent-guides","text":"I sometimes write really long blocks of code. This extension tells me where the blocks start and end. More Information","title":"Indent Guides"},{"location":"vs_extensions/#npm-task-runner","text":"If you are doing web development, this is not only recommended; it's vital. It allows you to run npm commands from within the Task Runner Explorer, thus relieving you of the process of dropping down to the command line. More Information","title":"NPM Task Runner"},{"location":"vs_extensions/#open-command-line","text":"For those occassions when you just can't avoid dropping down to the command line, this will ensure your command line starts at the right place.","title":"Open Command Line"},{"location":"vs_extensions/#package-installer","text":"In web development, sometimes you need to grab a package from elsewhere. Unfortunately, there are a dozen different ways of grabbing that package. This extension deals with all the myriad ways of getting the package. More Information","title":"Package Installer"},{"location":"vs_extensions/#regex-tester","text":"Are you a regex master? Me neither, which is why I like to test all my regular expressions before they go in my code. This extension adds that functionality as a window. More Information","title":"Regex Tester"},{"location":"vs_extensions/#roslynator","text":"Roslyn was a major step forward in the power of C#, and there are a number of ways that it makes your code cleaner. Unfortunately, you have to learn them - all 160+ of them. This extension looks for common refactorings for you, so you can carry on coding as before. Over time, you will learn how to do those things in Roslyn right the first time. More Information","title":"Roslynator"},{"location":"vs_extensions/#sqlite-sql-server-compact-toolbox","text":"Azure Mobile Apps uses SQLite as the basis of its offline cache. That means that occassionally you are going to need to peek inside the SQLite database, even if you are only curious. More Information","title":"SQLite / SQL Server Compact Toolbox"},{"location":"vs_extensions/#sqlite-for-universal-windows-platform","text":"Did I mention Azure Mobile Apps uses SQLite? If you are developing UWP applications, you will need this extension. More Information","title":"SQLite for Universal Windows Platform"},{"location":"vs_extensions/#trailing-whitespace-visualizer","text":"Languages are sometimes a little problematic when it comes to embedded white space at the end of lines, especially in multi-line strings. This extension shows them up in the editor, allowing you to easily find and destroy them. More Information","title":"Trailing Whitespace Visualizer"},{"location":"vs_extensions/#web-essentials","text":"There are few extensions that are more needed when you switch to web development. This provides capabilities for all the common file types used for web development, including style sheets and JavaScript files. More Information","title":"Web Essentials"},{"location":"vs_extensions/#web-extension-package","text":"This isn't one extension. It's several. There are several extension modules for image optimizations, bundling, icon handling and accessibility monitoring. This one extension installs all the others. Use this if you are going to be doing web + mobile development. More Information","title":"Web Extension Package"},{"location":"vs_extensions/#xamarin-forms-player","text":"One of the big gotchas in Xamarin Forms development is the process by which the XAML is parsed, built and reviewed. If you install the Xamarin Forms Player onto a device, this extension will push your XAML to that app and the app will render the page for you, allowing a much tighter development cycle. More Information","title":"Xamarin Forms Player"},{"location":"vs_extensions/#xamarin-forms-templates","text":"The default Xamarin Forms templates include projects for iOS and Android. I wanted UWP, Windows Phone 8.1 and potentially others as well. This set of templates gives me those additional templates. More Information","title":"Xamarin Forms Templates"},{"location":"vs_extensions/#xamarin-test-recorder","text":"Creating UI tests is painful. Running them on a large number of mobile devices is also painful. Fortunately, this extension handles the former - creating UI tests. You can run the same UI tests across thousands of devices by using Visual Studio Mobile Center testing facilities. More Information","title":"Xamarin Test Recorder"},{"location":"vs_extensions/#xaml-styler","text":"I find organizing XAML tedious. Fortunately, I can clean up my XAML and give it a consistent style by using this extension. You may not agree with its opinions, but at least it will make your code consistent. More Information Taking a look at all these extensions, a big shout-out goes to Mads Kristensen . He may work at Microsoft, but he puts out the great web extensions that make Visual Studio a major player in that space.","title":"XAML Styler"},{"location":"xamarin_tips/","text":"Xamarin Forms Tips \u00b6 Over the many months I have spent coding Xamarin Forms, a number of people from the community and the Xamarin team have given me little tips to improve my Xamarin Forms code. I hope you find them as useful as I did. Improve your ListView performance \u00b6 It should be no surprise that performance matters. Little things like smooth scrolling and fast load times are a must. A lot of improvement can be gained by the techniques we have shown in this book since most of the perceived delays are caused by the back end loading times. Eventually, a lot of apps generate a list. The normal way for implementing this is with a ListView that has an ObservableCollection You load your data into the ObservableCollection and then update that whenever the data changes. That, in turn, updates the ListView. There are two problems that are inherent here. The first is in the ObservableCollection and the second is in the ListView. Let's tackle the ObservableCollection first. The problem with the ObservableCollection is that it's very hard to update. What normally ends up happening is code like this: // Earlier in the code var listContents = new ObservableCollection<Model>(); // When updating the code var items = await table.ReadAllItemsAsync(); listContents.Clear(); for (var item in items) { listContents.Add(item); } The point of the ObservableCollection is that it emits an event whenever the list changes. In the case where the table has thousands of entries, thousands of events will cause thousands of redraws, causing a major slow down in your code that you probably won't know until you have a large enough data set to note the problem. Fortunately, one of the top Xamarin Evangelists, James Montemagno , has created a set of helper classes that assist with this sort of problem. The solution to this problem is to use the ObservableRangeCollection , like this: // Earlier in the code var listContents = new ObservableRangeCollection<Model>(); // When updating the code var items = await table.ReadAllItemsAsync(); listContents.ReplaceRange(items); With this code, Xamarin Forms gets notified once instead of thousands of times. There are actually several versions of this same behavior in the NuGet repository. As to the second problem. A ListView with thousands of items will not be showing all the items at once. A ListView will update all the items that have been updated, irrespective of whether they are visible or not. The answer is to use a caching strategy. There are two potential caching strategies. With RetainElement , the ListView will generate a cell for each item in the list. This is the default, but it's really only good for certain situations (most notably when the cell has a large number of bindings). For almost all situations, the RecycleElement caching strategy should be used. In this caching strategy, the ListView will minimize the memory foot print and execution speed by only updating cells when they are in the viewable area. This is good pretty much all the time, but explicitly when the cell has a small number of bindings or when each cell in the list has the same template. All data about the cell must come from the binding context when using the RecycleElement caching strategy. You can set the caching strategy right in the XAML: <ListView CachingStrategy=\"RecycleElement\" ...> Alternatively, if you are creating a ListView in code, you can specify the caching strategy in the constructor: var listView = new ListView(ListViewCachingStrategy.RecycleElement); There are more techniques for improving ListView performance in the Xamarin documentation Build a Floating Action Button \u00b6 One of the things that I wanted to do to my apps was to give them a little more in the way of normal UI design. My original design (which I introduced back in [Chapter 1][ch1]) had teal buttons at the bottom of the page. These buttons scrolled off the page when there were more items on the page than could reasonably be fit on the page. To fix this, I wanted to create a button that was always relative to the viewport. There are two steps to this. Firstly, you need to convert the layout to a RelativeLayout . For instance, my new ListView.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.TaskList\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" Title=\"{Binding Title}\"> <ContentPage.Content> <RelativeLayout> <StackLayout RelativeLayout.HeightConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Height, Factor=1}\" RelativeLayout.WidthConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Width, Factor=1}\"> The original StackLayout layout renderer is placed inside the newly added RelativeLayout . The height and width constraints tell the StackLayout to consume the whole screen. At the bottom of the page, I can add my button: </StackLayout> <!-- The Floating Button --> <Button BackgroundColor=\"Teal\" Command=\"{Binding AddNewItemCommand}\" RelativeLayout.XConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Width, Factor=1, Constant=-60}\" RelativeLayout.YConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Height, Factor=1, Constant=-60}\" Text=\"+\" TextColor=\"White\" /> </RelativeLayout> </ContentPage.Content> </ContentPage> The StackLayout is the end of the StackLayout I introduced in the previous listing. Now I can add a button (or any other View type control, including a custom control that I may have downloaded from NuGet or the Xamarin Plugins site) as well. The button will float above the other content since it is added later. The constraints in this case provide the location of the button. There is more work to do. For instance, you cannot click on the thing behind the button - the button always receives the click, which will (in this case) initiate the addition of a new item. A custom control will allow you to provide iconography for the button, handle situations where you want to scroll behind and provide for the circular styling of the button which seems to be in-vogue right now. If you want to place commands that are not normally used, you may want to consider the ToolbarItem area of the ContentPage. Here is a snippet: <ContentPage.ToolbarItems> <ToolbarItem Name=\"Refresh\" Command=\"{Binding RefreshCommand}\" Icon=\"refresh.png\" Order=\"Primary\" Priority=\"0\" /> </ContentPage.ToolbarItems> On Universal Windows (where this is actually important due to a lack of \"pull-to-refresh\" logic), you will see the familiar triple-dot on the app bar - clicking on the triple dot gives you access to the refresh command. Work with a Picker in a ViewModel \u00b6 There are some controls that do not work well with a BindingContext . One of these is the Picker . It has a number of Items that indicate the elements in the drop-down. However, the Items are not bindable, which means that they cannot be accessed via a ViewModel. Let's say you wish to use the drop-down Picker from a view model and use an async command to update the elements in the Picker. In the XAML file, give your picker a name: <Picker x:Name=\"myPicker\" /> This makes myPicker a private variable in the generated class. You can use this in the XAML code-behind file to pass the picker to your view-model by overriding the OnBindingContextChanged method: protected override void OnBindingContextChanged() { base.OnBindingContextChanged(); MyViewModel vm = BindingContext as MyViewModel; if (vm != null) { vm.MyPicker = myPicker; vm.RefreshPickerCommand.Execute(null); } } In your view-model, you can create a standard async command: async Task RefreshPickerAsync() { if (MyPicker.Items.Count == 0) { /* This section gets the list of items in the picker */ var table = await CloudService.GetTableAsync<Tag>(); var tags = await tagTable.ReadAllItemsAsync(); /* This section updates MyPicker with the items */ foreach (var item in tags) { MyPicker.Items.Add(item.TagName); // You may want to do a test for something in your item // here and set SelectedIndex to the index if it matches } } } The RefreshPickerAsync() command gets executed when the binding context is updated, which means the XAML has been executed and bound (thus the myPicker variable is set to the picker). Another way to accomplish this would be to encapsulate a picker in a custom control that does have a bindable Items element. This has been discussed on the Xamarin Forums . Display a Forms Element on only one platform \u00b6 One of the issues I ran into was with the ActivityIndicator . The ListView has a built in activity indicator which is displayed on any platform that has \"pull to refresh\" logic enabled. This is most notably iOS and Android, but not Windows. The ActivityIndicator object needs to be displayed on windows, but not the others. The answer to this conundrum is to wrap the element in a ContentView like this: <ContentView> <OnPlatform x:TypeArguments=\"View\"> <OnPlatform.WinPhone> <ActivityIndicator HorizontalOptions=\"End\" IsRunning=\"{Binding IsBusy, Mode=OneWay}\" VerticalOptions=\"Center\" Color=\"Black\" /> </OnPlatform.WinPhone> </OnPlatform> </ContentView> the ActivityIndicator will be displayed (at the appropriate time) on Windows, but not other platforms. Install NuGet Packages in Multiple Projects \u00b6 One of the common requirements we have in Xamarin Forms is to install NuGet packages in all the Xamarin Forms projects. To do this, right-click on the solution and select Manage NuGet Packages for Solution... . When you install a package here, you can select which projects it should be applied to, allowing you to install a package once across all the dependent projects. Auto-Deploy Universal Windows Apps \u00b6 One of the more annoying things is that you have to Deploy the Universal Windows app. This gets in the way of the build process. I like to build then run. Having the extra Deploy step in there might not seem like much until you find yourself deploying several times an hour. Fortunately, there is a simple fix for this. Set up the Configuration Manager to automatically deploy the right libraries on every successful build. To do this: In Visual Studio, select Build -> Configuration Manager... Check the boxes you can under Deploy Click on Close This setting is saved within the solution, so you only need to do it once per project.","title":"Xamarin Forms Tips"},{"location":"xamarin_tips/#xamarin-forms-tips","text":"Over the many months I have spent coding Xamarin Forms, a number of people from the community and the Xamarin team have given me little tips to improve my Xamarin Forms code. I hope you find them as useful as I did.","title":"Xamarin Forms Tips"},{"location":"xamarin_tips/#improve-your-listview-performance","text":"It should be no surprise that performance matters. Little things like smooth scrolling and fast load times are a must. A lot of improvement can be gained by the techniques we have shown in this book since most of the perceived delays are caused by the back end loading times. Eventually, a lot of apps generate a list. The normal way for implementing this is with a ListView that has an ObservableCollection You load your data into the ObservableCollection and then update that whenever the data changes. That, in turn, updates the ListView. There are two problems that are inherent here. The first is in the ObservableCollection and the second is in the ListView. Let's tackle the ObservableCollection first. The problem with the ObservableCollection is that it's very hard to update. What normally ends up happening is code like this: // Earlier in the code var listContents = new ObservableCollection<Model>(); // When updating the code var items = await table.ReadAllItemsAsync(); listContents.Clear(); for (var item in items) { listContents.Add(item); } The point of the ObservableCollection is that it emits an event whenever the list changes. In the case where the table has thousands of entries, thousands of events will cause thousands of redraws, causing a major slow down in your code that you probably won't know until you have a large enough data set to note the problem. Fortunately, one of the top Xamarin Evangelists, James Montemagno , has created a set of helper classes that assist with this sort of problem. The solution to this problem is to use the ObservableRangeCollection , like this: // Earlier in the code var listContents = new ObservableRangeCollection<Model>(); // When updating the code var items = await table.ReadAllItemsAsync(); listContents.ReplaceRange(items); With this code, Xamarin Forms gets notified once instead of thousands of times. There are actually several versions of this same behavior in the NuGet repository. As to the second problem. A ListView with thousands of items will not be showing all the items at once. A ListView will update all the items that have been updated, irrespective of whether they are visible or not. The answer is to use a caching strategy. There are two potential caching strategies. With RetainElement , the ListView will generate a cell for each item in the list. This is the default, but it's really only good for certain situations (most notably when the cell has a large number of bindings). For almost all situations, the RecycleElement caching strategy should be used. In this caching strategy, the ListView will minimize the memory foot print and execution speed by only updating cells when they are in the viewable area. This is good pretty much all the time, but explicitly when the cell has a small number of bindings or when each cell in the list has the same template. All data about the cell must come from the binding context when using the RecycleElement caching strategy. You can set the caching strategy right in the XAML: <ListView CachingStrategy=\"RecycleElement\" ...> Alternatively, if you are creating a ListView in code, you can specify the caching strategy in the constructor: var listView = new ListView(ListViewCachingStrategy.RecycleElement); There are more techniques for improving ListView performance in the Xamarin documentation","title":"Improve your ListView performance"},{"location":"xamarin_tips/#build-a-floating-action-button","text":"One of the things that I wanted to do to my apps was to give them a little more in the way of normal UI design. My original design (which I introduced back in [Chapter 1][ch1]) had teal buttons at the bottom of the page. These buttons scrolled off the page when there were more items on the page than could reasonably be fit on the page. To fix this, I wanted to create a button that was always relative to the viewport. There are two steps to this. Firstly, you need to convert the layout to a RelativeLayout . For instance, my new ListView.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.TaskList\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" Title=\"{Binding Title}\"> <ContentPage.Content> <RelativeLayout> <StackLayout RelativeLayout.HeightConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Height, Factor=1}\" RelativeLayout.WidthConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Width, Factor=1}\"> The original StackLayout layout renderer is placed inside the newly added RelativeLayout . The height and width constraints tell the StackLayout to consume the whole screen. At the bottom of the page, I can add my button: </StackLayout> <!-- The Floating Button --> <Button BackgroundColor=\"Teal\" Command=\"{Binding AddNewItemCommand}\" RelativeLayout.XConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Width, Factor=1, Constant=-60}\" RelativeLayout.YConstraint=\"{ConstraintExpression Type=RelativeToParent, Property=Height, Factor=1, Constant=-60}\" Text=\"+\" TextColor=\"White\" /> </RelativeLayout> </ContentPage.Content> </ContentPage> The StackLayout is the end of the StackLayout I introduced in the previous listing. Now I can add a button (or any other View type control, including a custom control that I may have downloaded from NuGet or the Xamarin Plugins site) as well. The button will float above the other content since it is added later. The constraints in this case provide the location of the button. There is more work to do. For instance, you cannot click on the thing behind the button - the button always receives the click, which will (in this case) initiate the addition of a new item. A custom control will allow you to provide iconography for the button, handle situations where you want to scroll behind and provide for the circular styling of the button which seems to be in-vogue right now. If you want to place commands that are not normally used, you may want to consider the ToolbarItem area of the ContentPage. Here is a snippet: <ContentPage.ToolbarItems> <ToolbarItem Name=\"Refresh\" Command=\"{Binding RefreshCommand}\" Icon=\"refresh.png\" Order=\"Primary\" Priority=\"0\" /> </ContentPage.ToolbarItems> On Universal Windows (where this is actually important due to a lack of \"pull-to-refresh\" logic), you will see the familiar triple-dot on the app bar - clicking on the triple dot gives you access to the refresh command.","title":"Build a Floating Action Button"},{"location":"xamarin_tips/#work-with-a-picker-in-a-viewmodel","text":"There are some controls that do not work well with a BindingContext . One of these is the Picker . It has a number of Items that indicate the elements in the drop-down. However, the Items are not bindable, which means that they cannot be accessed via a ViewModel. Let's say you wish to use the drop-down Picker from a view model and use an async command to update the elements in the Picker. In the XAML file, give your picker a name: <Picker x:Name=\"myPicker\" /> This makes myPicker a private variable in the generated class. You can use this in the XAML code-behind file to pass the picker to your view-model by overriding the OnBindingContextChanged method: protected override void OnBindingContextChanged() { base.OnBindingContextChanged(); MyViewModel vm = BindingContext as MyViewModel; if (vm != null) { vm.MyPicker = myPicker; vm.RefreshPickerCommand.Execute(null); } } In your view-model, you can create a standard async command: async Task RefreshPickerAsync() { if (MyPicker.Items.Count == 0) { /* This section gets the list of items in the picker */ var table = await CloudService.GetTableAsync<Tag>(); var tags = await tagTable.ReadAllItemsAsync(); /* This section updates MyPicker with the items */ foreach (var item in tags) { MyPicker.Items.Add(item.TagName); // You may want to do a test for something in your item // here and set SelectedIndex to the index if it matches } } } The RefreshPickerAsync() command gets executed when the binding context is updated, which means the XAML has been executed and bound (thus the myPicker variable is set to the picker). Another way to accomplish this would be to encapsulate a picker in a custom control that does have a bindable Items element. This has been discussed on the Xamarin Forums .","title":"Work with a Picker in a ViewModel"},{"location":"xamarin_tips/#display-a-forms-element-on-only-one-platform","text":"One of the issues I ran into was with the ActivityIndicator . The ListView has a built in activity indicator which is displayed on any platform that has \"pull to refresh\" logic enabled. This is most notably iOS and Android, but not Windows. The ActivityIndicator object needs to be displayed on windows, but not the others. The answer to this conundrum is to wrap the element in a ContentView like this: <ContentView> <OnPlatform x:TypeArguments=\"View\"> <OnPlatform.WinPhone> <ActivityIndicator HorizontalOptions=\"End\" IsRunning=\"{Binding IsBusy, Mode=OneWay}\" VerticalOptions=\"Center\" Color=\"Black\" /> </OnPlatform.WinPhone> </OnPlatform> </ContentView> the ActivityIndicator will be displayed (at the appropriate time) on Windows, but not other platforms.","title":"Display a Forms Element on only one platform"},{"location":"xamarin_tips/#install-nuget-packages-in-multiple-projects","text":"One of the common requirements we have in Xamarin Forms is to install NuGet packages in all the Xamarin Forms projects. To do this, right-click on the solution and select Manage NuGet Packages for Solution... . When you install a package here, you can select which projects it should be applied to, allowing you to install a package once across all the dependent projects.","title":"Install NuGet Packages in Multiple Projects"},{"location":"xamarin_tips/#auto-deploy-universal-windows-apps","text":"One of the more annoying things is that you have to Deploy the Universal Windows app. This gets in the way of the build process. I like to build then run. Having the extra Deploy step in there might not seem like much until you find yourself deploying several times an hour. Fortunately, there is a simple fix for this. Set up the Configuration Manager to automatically deploy the right libraries on every successful build. To do this: In Visual Studio, select Build -> Configuration Manager... Check the boxes you can under Deploy Click on Close This setting is saved within the solution, so you only need to do it once per project.","title":"Auto-Deploy Universal Windows Apps"},{"location":"chapter1/firstapp_mac/","text":"Your First Mobile App on a Mac \u00b6 There is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects. First, wouldn't it be nice to write some code and get something working? Microsoft Azure has a great first-steps tutorial that takes you via the quickest possible route from creating a mobile backend to having a functional backend. I would like to take things a little slower so that we can understand what is going on while we are doing the process. We will have practically the same application at the end. The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly. If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up. This chapter covers the set up required for a MacOS computer. If you wish to develop your applications on a Windows PC, then skip to the prior section . The application we are going to build together is a simple task list. The mobile client will have three screens - an entry screen, a task list and a task details page. I have mocked these pages up using MockingBot . Tip Mocking your screens before you start coding is a great habit to get into. There are some great tools available including free tools like MockingBot . Doing mockups before you start coding is a good way to prevent wasted time later on. Tip If you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one. Other platforms will need it though, so it's best to start with the least common denominator. It's the same reason I add a refresh button even though it's only valid on Windows Phone! My ideas for this app include: Tapping on a task title in the task list will bring up the details page. Toggling the completed link in the task list will set the completed flag. Tapping the spinner will initiate a network refresh. Now that we have our client screens planned out, we can move onto the thinking about the mobile backend. The Mobile Backend \u00b6 The mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python). Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service. Creating a Simple Azure Mobile Apps Backend \u00b6 To get started: Fire up Visual Studio for Mac . Create a new solution with File -> New Solution... . In the New Project window, choose Other -> ASP.NET , select Empty ASP.NET Project , then click Next . In the Configure your Web project window, check the Web API box and uncheck the Include Unit Test Project , then click Next . In the second Configure your new project window, enter Backend for the for the Project Name and Chapter1 for the Solution name. Click Create to generate the files. At this point, Visual Studio for Mac will lay down the template files on your disk and download the core ASP.NET libraries from NuGet. You will need to accept the licenses for the downloaded NuGet packages. Visual Studio for Mac does not have the range of templates that Visual Studio for the PC has. As a result, we will need to do some additional work putting together a base project. The core of Azure Mobile Apps runs on .NET Framework 4.6.x. Right-click your Backend project in the Solution Explorer and choose Options . The target framework setting is located in the Build -> General section. You can choose any .NET Framework in the 4.6.x range. Click OK to accept the change and close the Project Options. Although a core set of NuGet packages are installed during project creation, we need to add the Azure Mobile Apps NuGet packages. The easiest way to do this is to install the Azure Mobile .NET Server Quickstart NuGet package, which contains dependencies on all the other Azure Mobile Apps packages. Expand the Backend project in the solution explorer, right-click Packages , then select Add Packages... . Use the search box to find the appropriate NuGet package. You also need to add the Owin System host Microsoft.Owin.Host.Systemweb NuGet package. You should take the opportunity to update any NuGet packages that were automatically added to the project. To do so, right-click Packages , then choose Update . Start by configuring the Azure Mobile Apps SDK. The Owin process runs the Startup.cs object to configure itself. To create the Startup.cs class: Right-click the Backend folder. Select Add -> New File... . Select General -> Empty Class , and set the name of the class to Startup.cs . Click New . Leave off the .cs on the end If your filename does not have a dot in it (and some that we use do), you can leave off the .cs trailing extension. Visual Studio for Mac will add it for you. The Startup.cs class looks like this: using Microsoft.Owin; using Owin; [assembly: OwinStartup(typeof(Backend.Startup))] namespace Backend { public partial class Startup { public void Configuration(IAppBuilder app) { ConfigureMobileApp(app); } } } The ConfigureMobileApp() method is located in the App_Start\\Startup.MobileApp.cs file that we will create next. Create the file the same way you did the Startup.cs file, then enter the following into the new file: using System; using System.Collections.Generic; using System.Configuration; using System.Data.Entity; using System.Web.Http; using Microsoft.Azure.Mobile.Server; using Microsoft.Azure.Mobile.Server.Authentication; using Microsoft.Azure.Mobile.Server.Config; using Backend.DataObjects; using Backend.Models; using Owin; namespace Backend { public partial class Startup { public static void ConfigureMobileApp(IAppBuilder app) { var config = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(config); Database.SetInitializer(new MobileServiceInitializer()); app.UseWebApi(config); } } public class MobileServiceInitializer : CreateDatabaseIfNotExists<MobileServiceContext> { protected override void Seed(MobileServiceContext context) { List<TodoItem> todoItems = new List<TodoItem> { new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"First item\", Complete = false }, new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"Second item\", Complete = false } }; foreach (TodoItem todoItem in todoItems) { context.Set<TodoItem>().Add(todoItem); } base.Seed(context); } } } Let's break this down a little bit. The ConfigureMobileApp() method is called to configure Azure Mobile Apps when the service starts. The code tells the SDK that we want to use tables and that those tables are backed with Entity Framework. We also want to initialize the database that we are going to use. That database is going to use a DbContext called MobileServiceContext . The initialization code will create the database and seed it with two new items if it doesn't already exist. If it exists, then we assume that we don't need to seed the database with data. The MobileServiceContext is used to configure the tables within the database and to project those tables into Entity Framework. It relies on a model for each table. In Azure Mobile Apps, this is called the Data Transfer Object or DTO. The server will serialize a DTO to JSON for transmission. Our DTO is in a new directory called DataObjects (right-click on Backend and choose Add -> New Folder... to create it) and is called TodoItem.cs : using Microsoft.Azure.Mobile.Server; namespace Backend.DataObjects { public class TodoItem : EntityData { public string Text { get; set; } public bool Complete { get; set; } } } We base each DTO we use on the EntityData class, since we are using Entity Framework. This sets up some additional columns in the data model so that we can keep track on mobile device changes. We will be discussing this in more detail in the Data Access and Offline Sync chapter. We also need the Models\\MobileServiceContext.cs which sets up the tables for us: using System.Data.Entity; using System.Data.Entity.ModelConfiguration.Conventions; using System.Linq; using Microsoft.Azure.Mobile.Server; using Microsoft.Azure.Mobile.Server.Tables; using Backend.DataObjects; namespace Backend.Models { public class MobileServiceContext : DbContext { private const string connectionStringName = \"Name=MS_TableConnectionString\"; public MobileServiceContext() : base(connectionStringName) { } public DbSet<TodoItem> TodoItems { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); } } } A DbSet<> statement is needed for each table we wish to expose to the mobile clients. There are a couple of important items here. Firstly, our connection string is called MS_TableConnectionString . This is set up in the Web.config file and overridden in the Azure Portal so that you can do both local development and run against a production database in the cloud. Do not change this name. The OnModelCreating() method sets up the tables to handle the service columns that are contained within the EntityData class used by the DTO. Certain fields need to be indexed and triggers need to be added to keep the values updated properly. Finally (in terms of code), we need to create a table controller. This is the endpoint that is exposed on the Internet that our mobile clients will access to send and receive data. Create a Controllers folder and add the following TodoItemController.cs class: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) => DeleteAsync(id); } } The TableController is the central processing for the database access layer. It handles all the OData capabilities for us and exposes these as REST endpoints within our WebAPI. This means that the actual code for this controller is tiny - just 12 lines of code - but very powerful. Info OData is a specification for accessing table data on the Internet. It provides a mechanism for querying and manipulating data within a table. Entity Framework is a common data access layer for ASP.NET applications. Our last step in our backend before publishing it is to edit the Web.config file. The Web.config file tells IIS about the run-time settings for this application. We need to set up the MS_TableConnectionString and several app settings. Inevitably, I copy a created Web.config rather than starting from scratch: <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <configSections> <section name=\"entityFramework\" type=\"System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\" requirePermission=\"false\" /> </configSections> <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"Data Source=(localdb)\\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\\aspnet-Backend-20170308080621.mdf;Initial Catalog=aspnet-Backend-20170308080621;Integrated Security=True;MultipleActiveResultSets=True\" providerName=\"System.Data.SqlClient\" /> </connectionStrings> <appSettings> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> </appSettings> <system.web> <httpRuntime targetFramework=\"4.6\" /> <compilation debug=\"true\" targetFramework=\"4.6\" /> </system.web> <system.webServer> <validation validateIntegratedModeConfiguration=\"false\" /> <modules runAllManagedModulesForAllRequests=\"true\" /> <handlers> <remove name=\"ExtensionlessUrlHandler-Integrated-4.0\" /> <remove name=\"OPTIONSVerbHandler\" /> <remove name=\"TRACEVerbHandler\" /> <add name=\"ExtensionlessUrlHandler-Integrated-4.0\" path=\"*.\" verb=\"*\" type=\"System.Web.Handlers.TransferRequestHandler\" preCondition=\"integratedMode,runtimeVersionv4.0\" /> </handlers> </system.webServer> <runtime> <assemblyBinding xmlns=\"urn:schemas-microsoft-com:asm.v1\" xmlns:bcl=\"urn:schemas-microsoft-com:bcl\"> <dependentAssembly> <assemblyIdentity name=\"Newtonsoft.Json\" publicKeyToken=\"30ad4fe6b2a6aeed\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-9.0.0.0\" newVersion=\"9.0.0.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"System.Web.Http\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.2.3.0\" newVersion=\"5.2.3.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"System.Net.Http.Formatting\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.2.3.0\" newVersion=\"5.2.3.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"Microsoft.Data.Edm\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.8.1.0\" newVersion=\"5.8.1.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"Microsoft.Data.OData\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.8.1.0\" newVersion=\"5.8.1.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"System.Spatial\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.8.1.0\" newVersion=\"5.8.1.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"Microsoft.Owin\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-3.0.1.0\" newVersion=\"3.0.1.0\" /> </dependentAssembly> </assemblyBinding> </runtime> <entityFramework> <defaultConnectionFactory type=\"System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework\" /> <providers> <provider invariantName=\"System.Data.SqlClient\" type=\"System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer\" /> </providers> </entityFramework> </configuration> Choose Build All from the Build menu and ensure your project compiles without errors. Building an Azure App Service for Mobile Apps \u00b6 The next step in the process is to build the resources on Azure that will run your mobile backend. Start by logging into the Azure portal , then follow these instructions: Click the big + New button in the top-left corner. Click Web + Mobile , then Mobile App . Enter a unique name in the App name box. Tip Since the name doesn't matter and it has to be unique, you can use a GUID generator to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID (example: chapter1-GUID) to aid in discovery later on. Generally, the first four digits of a GUID are enough to identify individual resources. If you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the Subscription drop-down. Select Create new under resource group and enter a name for this mobile application. Resource Groups Resource groups are great for grouping all the resources associated with a mobile application together. During development, it means you can delete all the resources in one operation. For production, it means you can see how much the service is costing you and how the resources are being used. Finally, select or create a new App Service Plan . App Service Plan The App Service Plan is the thing that actually bills you - not the web or mobile backend. You can run a number of web or mobile backends on the same App Service Plan. I tend to create a new App Service Plan for each mobile application. This is because the App Service Plan lives inside the Resource Group that you create. The process for creating an App Service Plan is straight forward. You have two decisions to make. The first decision is where is the service going to run. In a production environment, the correct choice is \"near your customers\". \"Close to the developers\" is a good choice during development. Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location. With 16 regions to choose from, you have a lot of choice. The second decision you have to make is what to run the service on; also known as the Pricing tier. If you Click View all , you will see you have lots of choices. F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota. That leaves Basic, Standard and Premium. Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks. Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example. Finally, there is a number after the plan. This tells you how big the virtual machine is that the plan is running on. The numbers differ by number of cores and memory. For our purposes, an F1 Free site is enough to run this small demonstration project. More complex development projects should use something in the Basic range of pricing plans. Production apps should be set up in Standard or Premium pricing plans. Once you have created your app service plan and saved it, Click Create . The creation of the service can take a couple of minutes. You can monitor the process of deployment by clicking on the Notifications icon. This is in the top bar on the right-hand side and looks like a Bell. Clicking on a specific notification will provide more information about the activity. Once you have created your app service, the App Service blade will open. We will also want a place to store our data. This role is taken on by a SQL Azure instance. We could link an existing database if we had one defined. However, we can also create a test database. Tip Creating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database. This option is not normally available through other SQL database creation flows. Before we can create a database, we need to create a logical server for the database. The SQL Server (the logical server) sets the region and the login credentials for all the contained databases: Click Resource groups in the left hand side menu. Click the resource group you created. Click Add at the top of the blade. Enter SQL Server into the search box, then press Enter. Click SQL Server (logical server) . Click Create . Enter the information required by the form: A server name (which must be unique in the world - this is a great place to use a GUID). A username and password for accessing the databases on the server. Select the existing resource group. Pick the same Location as you did for the App Service Plan. Click Create . Once the deployment has completed, you can move on to creating and linking a database. You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner. To create and link the database: Click Resource groups in the left hand side menu. Click the resource group you created. Click the App Service your created. Tip If you pinned your App Service to the dashboard, you can Click the pinned App Service instead. It will bring you to the same place. Click Data connections in the MOBILE menu. You can also search for Data connections in the left hand menu. Click Add . In the Type box, select SQL Database . Click the unconfigured SQL Database link: In the Database blade, select Create a new database . Enter a name for the database (like chapter1-db ). Click the Target server box and select the logical server you created earlier. Select a Pricing Tier, then click Apply . Click Select to close the SQL Database blade. Click the Connection string box. Enter the username and password you set up for the SQL logical server. Click OK . The username and password will be validated before proceeding. Click OK to close the Add data connection blade. This produces another deployment step that creates a SQL database with your settings and binds it to the App Service. Once complete, the connection MS_TableConnectionString will be listed in Data Connections blade. Deploying the Azure Mobile Apps Backend \u00b6 One of the areas I love Visual Studio for the PC over the Mac edition is in publishing to Azure. On the PC, that's a right-click action. There is no publish action in Visual Studio for Mac. However, Azure App Service has enough other tools available to publish a service. These include continuous integration technologies that link to Visual Studio Team Services or GitHub, integrations with cloud storage providers like OneDrive and the venerable but trusty ftp mechanisms. If none of those suit you, you can use drag-and-drop, which is what I am going to do here. Return to the browser and log into the Azure portal . Go to the Settings blade for your Mobile App. Click Advanced Tools in the DEVELOPMENT TOOLS menu. Click Go in the Advanced Tools blade. The page that loads should match https://{YourMobileApp}.scm.azurewebsites.net/. Select the Debug Console menu from the top and choose CMD . Within the file structure listing, click site , then wwwroot . Remove the hostingstart.html file; click the circle with a minus symbol in it to the left of that file and confirm the dialog to delete this file. On your Mac, use the Finder to navigate to the folder that contains your Mobile App Backend. Select the following folder and files: bin packages.config Web.config Drag and drop those files into the browser window where the hostingstart.html file used to be. A progress indicator should appear near the top right. Upon completion you should see the files appear in the file list: You can test your deployed app by browsing to https://{yourmobileapp}.azurewebsites.net/tables/todoitem?ZUMO-API-VERSION=2.0.0 - this is the same URL that your mobile app will connect to later on. Replace {yourmobileapp} with the name of the App Service that you created. If everything is working, you should see some JSON results in your window: The first request will take some time as the App Service is waking up your service, which is initializing the database and seeding the data into that database. Info You will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code. ZUMO was the original code name within Microsoft for A ZU re MO bile. The Mobile Client \u00b6 Now that the mobile backend is created and deployed, we can move onto the mobile application that your users would install on their phones. We are going to use Xamarin.Forms to produce a cross-platform application for iOS and Android, and the majority of the code will be placed in a shared project that both platforms will use. You can get as high as 95% code re-use using Xamarin.Forms which is a major productivity boost in complex applications. Info When you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows. Right-click the Chapter1 solution, then select Add -> Add New Project . This will bring up the familiar New Project dialog. Select the Multiplatform -> App -> Forms App template, then click Next . Give the project a name such as TaskList , ensure Android and iOS are both selected, select Use Shared Library and click Next . Click Create on the next screen to create the projects. You will see that three new projects are created: a common library that you named, plus a project for each platform that you choise (in this case two platforms - Android and iOS): Most of our work will happen in the common library in this walkthrough. However, we can introduce platform-specific code at any point. The platform-specific code is stored in the platform-specific project. There is one final item we must do before we leave the set up of the project. There are a number of platform upgrades that inevitably have to happen. The Xamarin Platform is updated much more often than the project templates in Visual Studio for Mac. Updates to the Xamarin platform are released via NuGet. Since we are going to be integrating the Azure Mobile Apps client SDK, you should add the Microsoft.Azure.Mobile.Client NuGet package. This will need to be done within each platform-specific project (TaskList.Droid and TaskList.iOS). Warn Although it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services. There are many differences between the wire protocols of the two products. You can start by updating the existing NuGet packages. Right-click the Packages folder in each project, then select Update . Then add the Microsoft.Azure.Mobile.Client SDK. Right-click the Packages folder again, select Add Packages... , then enter the package name in the search box. Info Android generally has more updates than the other platforms. Ensure that you update the main Xamarin.Forms package and then refresh the update list. This will ensure the right list of support packages is updated. Building the Common Library \u00b6 There are two parts that we must concentrate on within the common library. Firstly, we must deal with the connection to our mobile backend using the Azure Mobile Apps client SDK. Secondly, we need to create the three pages that our application will show. Start by adding the following folders to your Shared Library project: Abstractions Models Pages Services ViewModels Our application will use MVVM (Model-View-ViewModel) for the interaction. The UI will be written in XAML, and the ViewModel will be used to provide information to that UI. The Services folder will hold the classes necessary to communicate with the Azure Mobile Apps backend. Finally, we have a number of interfaces and common classes that we will need to make everything work. Remove the other contents of the shared project. They will not be required. Building an Azure Mobile Apps Connection \u00b6 One of the things I often stress is to set yourself up assuming you are going to test your product. Testing is a good thing. In cloud development, we often set up a \"mock\" service where we swap out the \"real\" service and use something that has the same interface, but deals with local data only. This application will have two interfaces. The first represents a cloud service, which will have a collection of tables we want to access. This will be defined in Abstractions\\ICloudService.cs . namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; } } We haven't defined ICloudTable<> nor TableData yet. The ICloudTable<T> interface defines what the application can do to a table. It will be defined in Abstractions\\ICloudTable.cs : using System.Collections.Generic; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); } } The ICloudTable<T> interface defines the normal CRUD operations: Create, Read, Update, and Delete. However, it does so asynchronously. We are dealing with network operations and it is easy for those operations to tie up the UI thread for an appreciable amount of time. Making them async provides the ability to respond to other events and ensure your application is responsive to the user. There are some fields that every single record within an Azure Mobile Apps table provides. These fields are required for offline sync capabilities like incremental sync and conflict resolution. On the server, this is represented by the EntityData class. We cannot use that class as it contains the Entity Framework additions for indexing and triggers. The fields are instead provided by a new abstract base class on the client called Abstractions\\TableData : using System; namespace TaskList.Abstractions { public abstract class TableData { public string Id { get; set; } public DateTimeOffset? UpdatedAt { get; set; } public DateTimeOffset? CreatedAt { get; set; } public byte[] Version { get; set; } } } As we will learn when we deal with table data , these fields need to be defined with the same name and semantics as on the server. Our model on the server was sub-classed from EntityData and the EntityData class on the server defines these fields. It's tempting to call the client version of the class the same as the server version. If we did that, the models on both the client and server would look the same. However, I find that this confuses the issue. The models on the client and server are not the same. They are missing the Deleted flag and they do not contain any relationship information on the client. I choose to deliberately call the base class something else on the client to avoid this confusion. We will be adding to these interfaces in future chapters as we add more capabilities to the application. The concrete implementations of these classes are similarly easily defined. The Azure Mobile Apps Client SDK does most of the work for us. Here is the concrete implementation of the ICloudService (in Services\\AzureCloudService.cs ): using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(\"https://my-backend.azurewebsites.net\"); } public ICloudTable<T> GetTable<T>() where T : TableData { return new AzureCloudTable<T>(client); } } } Ensure you use HTTPS If you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint. You must provide the https version of the endpoint when using App Service. The http endpoint redirects to https and the standard HttpClient does not handle redirects. The Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published. Just swap my-backend out for the name of your mobile backend and the rest is silently dealt with. Warn The name Microsoft.WindowsAzure.MobileServices is a hold-over from the old Azure Mobile Services code-base. Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps. We also need a concrete implementation of the ICloudTable<T> interface (in Services\\AzureCloudTable.cs ): using System.Collections.Generic; using System.Collections.ObjectModel; using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudTable<T> : ICloudTable<T> where T : TableData { MobileServiceClient client; IMobileServiceTable<T> table; public AzureCloudTable(MobileServiceClient client) { this.client = client; this.table = client.GetTable<T>(); } #region ICloudTable implementation public async Task<T> CreateItemAsync(T item) { await table.InsertAsync(item); return item; } public async Task DeleteItemAsync(T item) { await table.DeleteAsync(item); } public async Task<ICollection<T>> ReadAllItemsAsync() { return await table.ToListAsync(); } public async Task<T> ReadItemAsync(string id) { return await table.LookupAsync(id); } public async Task<T> UpdateItemAsync(T item) { await table.UpdateAsync(item); return item; } #endregion } } The Azure Mobile Apps Client SDK does a lot of the work for us. In fact, we are just wrapping the basic interface here. The majority of the code for dealing with the remote server is done for us. Tip You can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as: public async Task DeleteItemAsync(T item) => await table.DeleteAsync(item); You may see this sort of short hand in samples. We also need to create the model that we will use for the data. This should look very similar to the model on the server - including having the same name and fields. In this case, it's Models\\TodoItem.cs : using TaskList.Abstractions; namespace TaskList.Models { public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } } } We have a final piece of code to write before we move on to the views, but it's an important piece. The ICloudService must be a singleton in the client. We will add authentication and offline sync capabilities in future versions of this code. The singleton becomes critical when using those features. For right now, it's good practice and saves on memory if you only have one copy of the ICloudService in your mobile client. Since there is only one copy of the App.cs in any given app, I can place it there. Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this. Here is the App.cs : using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { CloudService = new AzureCloudService(); MainPage = new NavigationPage(new Pages.EntryPage()); } // There are life cycle methods here... } } We haven't written Pages.EntryPage yet, but that's coming. This file replaces the App.xaml and App.xaml.cs files from the original shared project. If you have not done so already, ensure you remove those files now. Building the UI for the App \u00b6 Earlier, I showed the mockup for my UI. It included three pages - an entry page, a list page, and a detail page. These pages have three elements - a XAML definition file, a (simple) code-behind file, and a view model. Info This book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML. If you wish to have that sort of introduction, then I recommend reading the excellent book by Charles Petzold: Creating Mobile Apps with Xamarin.Forms . I tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications. It's a nice clean pattern and is well understood and documented. In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements. In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed. To do this, we will use a BaseViewModel class that implements the base functionality for each view. Aside from the INotifyPropertyChanged interface, there are some common properties we need for each page. Each page needs a title, for example, and an indicator of network activity. These can be placed in the Abstractions\\BaseViewModel.cs class: using System; using System.Collections.Generic; using System.ComponentModel; namespace TaskList.Abstractions { public class BaseViewModel : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged; string _propTitle = string.Empty; bool _propIsBusy; public string Title { get { return _propTitle; } set { SetProperty(ref _propTitle, value, \"Title\"); } } public bool IsBusy { get { return _propIsBusy; } set { SetProperty(ref _propIsBusy, value, \"IsBusy\"); } } protected void SetProperty<T>(ref T store, T value, string propName, Action onChanged = null) { if (EqualityComparer<T>.Default.Equals(store, value)) return; store = value; if (onChanged != null) onChanged(); OnPropertyChanged(propName); } public void OnPropertyChanged(string propName) { if (PropertyChanged == null) return; PropertyChanged(this, new PropertyChangedEventArgs(propName)); } } } This is a fairly common INotifyPropertyChanged interface implementation pattern. Each property that we want to expose is a standard property, but the set operation is replaced by the SetProperty() call. The SetProperty() call deals with the notification; calling the event emitter if the property has changed value. We only need two properties on the BaseViewModel : the title and the network indicator. I tend to write my apps in two stages. I concentrate on the functionality of the app in the first stage. There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking. The page is all about the functionality of the various interactions. Once I have the functionality working, I work on the styling of the page. We won't be doing any styling work in the demonstration apps that we write during the course of this book. The EntryPage has just one thing to do. It provides a button that enters the app. When we cover authentication later on, we'll use this to log in to the backend. If you are looking at the perfect app, this is a great place to put the introductory screen. Creating a XAML file is relatively simple. We already created a Pages directory to hold the pages of our application. Right-click the Pages directory in the solution explorer and choose Add -> New File... . In the Add New File dialog, pick Forms -> Forms ContentPage Xaml . Name the new page EntryPage . This will create two files - EntryPage.xaml and EntryPage.xaml.cs . Let's center a button on the page and wire it up with a command. Here is the Pages\\EntryPage.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.EntryPage\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Vertical\" VerticalOptions=\"Center\"> <Button BackgroundColor=\"Teal\" BorderRadius=\"10\" Command=\"{Binding LoginCommand}\" Text=\"Login\" TextColor=\"White\" /> </StackLayout> </ContentPage.Content> </ContentPage> The StackLayout element is our layout element. It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are. The only contents are a button. There are two bindings. These are bound to properties in the view-model. We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command. When the button is tapped, the login command will be run. We'll get onto that in the view-model later. The other file created is the code-behind file. Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial: using TaskList.ViewModels; using Xamarin.Forms; namespace TaskList.Pages { public partial class EntryPage : ContentPage { public EntryPage() { InitializeComponent(); BindingContext = new EntryPageViewModel(); } } } This is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM. We initialize the UI, then bind all the bindings to a new instance of the view model. Speaking of which, the view-model just needs to handle the login click. Note that the location or namespace is TaskList.ViewModels . I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together. However, just about all the sample code that I see has the view-models in a separate namespace. Which one is correct? I'll copy the samples for now. Here is the code for ViewModels\\EntryPageViewModel.cs : using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using Xamarin.Forms; namespace TaskList.ViewModels { public class EntryPageViewModel : BaseViewModel { public EntryPageViewModel() { Title = \"Task List\"; } Command loginCmd; public Command LoginCommand => loginCmd ?? (loginCmd = new Command(async () => await ExecuteLoginCommand())); async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[Login] Error = {ex.Message}\"); } finally { IsBusy = false; } } } } This is a fairly simple view-model but there are some patterns here that are worth explaining. Firstly, note the way we create the LoginCommand property. This is the property that is bound to the Command parameter in the Button of our view. This recipe is the method of invoking a UI action asynchronously. It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity. The second is the pattern for the ExecuteLoginCommand method. Firstly, I ensure nothing else is happening by checking the IsBusy flag. If nothing is happening, I set the IsBusy flag. Then I do what I need to do in a try/catch block. If an exception is thrown, I deal with it. Most of the time this involves displaying an error condition. There are several cross-platform dialog packages to choose from or you can roll your own. That is not covered here. We just write a debug log statement so we can see the result in the debug log. Once everything is done, we clear the IsBusy flag. The only thing we are doing now is swapping out our main page for a new main page. This is where we will attach authentication later on. The next page is the Task List page, which is in Pages\\TaskList.xaml : <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskList\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout> <ListView BackgroundColor=\"#7F7F7F\" CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"True\" IsRefreshing=\"{Binding IsBusy, Mode=OneWay}\" ItemsSource=\"{Binding Items}\" RefreshCommand=\"{Binding RefreshCommand}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout HorizontalOptions=\"FillAndExpand\" Orientation=\"Horizontal\" Padding=\"10\" VerticalOptions=\"CenterAndExpand\"> <Label HorizontalOptions=\"FillAndExpand\" Text=\"{Binding Text}\" TextColor=\"#272832\" /> <Switch IsToggled=\"{Binding Complete, Mode=OneWay}\" /> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"Teal\" Command=\"{Binding AddNewItemCommand}\" Text=\"Add New Item\" TextColor=\"White\" /> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> Note that some bindings here are one-way. This means that the value in the view-model drives the value in the UI. There is nothing within the UI that you can do to alter the state of the underlying property. Some bindings are two-way. Doing something in the UI (for example, toggling the switch) alters the underlying property. This view is a little more complex. It can be split into two parts - the list at the top of the page and the button area at the bottom of the page. The list area uses a template to help with the display of each item. Note that the ListView object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand. It also has an indicator that I have wired up to the IsBusy indicator. Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does. The code behind for the TaskList can be found in Pages\\TaskList.xaml.cs : using TaskList.ViewModels;using Xamarin.Forms; namespace TaskList.Pages { public partial class TaskList : ContentPage { public TaskList() { InitializeComponent(); BindingContext = new TaskListViewModel(); } } } There is a view-model that goes along with the view (in ViewModels\\TaskListViewModel.cs ): using System; using System.Collections.ObjectModel; using System.Collections.Specialized; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskListViewModel : BaseViewModel { public TaskListViewModel() { Title = \"Task List\"; RefreshList(); } ObservableCollection<TodoItem> items = new ObservableCollection<TodoItem>(); public ObservableCollection<TodoItem> Items { get { return items; } set { SetProperty(ref items, value, \"Items\"); } } TodoItem selectedItem; public TodoItem SelectedItem { get { return selectedItem; } set { SetProperty(ref selectedItem, value, \"SelectedItem\"); if (selectedItem != null) { Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem)); SelectedItem = null; } } } Command refreshCmd; public Command RefreshCommand => refreshCmd ?? (refreshCmd = new Command(async () => await ExecuteRefreshCommand())); async Task ExecuteRefreshCommand() { if (IsBusy) return; IsBusy = true; try { var table = App.CloudService.GetTable<TodoItem>(); var list = await table.ReadAllItemsAsync(); Items.Clear(); foreach (var item in list) Items.Add(item); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error loading items: {ex.Message}\"); } finally { IsBusy = false; } } Command addNewCmd; public Command AddNewItemCommand => addNewCmd ?? (addNewCmd = new Command(async () => await ExecuteAddNewItemCommand())); async Task ExecuteAddNewItemCommand() { if (IsBusy) return; IsBusy = true; try { await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail()); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error in AddNewItem: {ex.Message}\"); } finally { IsBusy = false; } } async Task RefreshList() { await ExecuteRefreshCommand(); MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); } } } This is a combination of the patterns we have seen earlier. The Add New Item and Refresh commands should be fairly normal patterns now. We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the SelectedItem property through a two-way binding) and when the user clicks on the Add New Item button. When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed. It is fairly common to use an ObservableCollection or another class that uses the ICollectionChanged event handler for the list storage. Doing so allows the UI to react to changes in the items. Note the use of the ICloudTable interface here. We are using the ReadAllItemsAsync() method to get a list of items, then we copy the items we received into the ObservableCollection . Finally, there is the TaskDetail page. This is defined in the Pages\\TaskDetail.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskDetail\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout Padding=\"10\" Spacing=\"10\"> <Label Text=\"What should I be doing?\"/> <Entry Text=\"{Binding Item.Text}\"/> <Label Text=\"Completed?\"/> <Switch IsToggled=\"{Binding Item.Complete}\"/> <StackLayout VerticalOptions=\"CenterAndExpand\"/> <StackLayout Orientation=\"Vertical\" VerticalOptions=\"End\"> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"#A6E55E\" Command=\"{Binding SaveCommand}\" Text=\"Save\" TextColor=\"White\"/> <Button BackgroundColor=\"Red\" Command=\"{Binding DeleteCommand}\" Text=\"Delete\" TextColor=\"White\"/> </StackLayout> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> This page is a simple form with just two buttons that need to have commands wired up. However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture. As a result of this, we need to handle the passing of the item to be edited. This is done in the Pages\\TaskDetail.xaml.cs code-behind file: using TaskList.Models; using TaskList.ViewModels; using Xamarin.Forms; namespace TaskList.Pages { public partial class TaskDetail : ContentPage { public TaskDetail(TodoItem item = null) { InitializeComponent(); BindingContext = new TaskDetailViewModel(item); } } } The item that is passed in from the TaskList page is used to create a specific view-model for that item. The view-model is similarly configured to use that item: using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskDetailViewModel : BaseViewModel { ICloudTable<TodoItem> table = App.CloudService.GetTable<TodoItem>(); public TaskDetailViewModel(TodoItem item = null) { if (item != null) { Item = item; Title = item.Text; } else { Item = new TodoItem { Text = \"New Item\", Complete = false }; Title = \"New Item\"; } } public TodoItem Item { get; set; } Command cmdSave; public Command SaveCommand => cmdSave ?? (cmdSave = new Command(async () => await ExecuteSaveCommand())); async Task ExecuteSaveCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id == null) { await table.CreateItemAsync(Item); } else { await table.UpdateItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } Command cmdDelete; public Command DeleteCommand => cmdDelete ?? (cmdDelete = new Command(async () => await ExecuteDeleteCommand())); async Task ExecuteDeleteCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id != null) { await table.DeleteItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } } } The save command uses the ICloudTable interface again - this time doing either CreateItemAsync() or UpdateItemAsync() to create or update the item. The delete command, as you would expect, deletes the item with the DeleteItemAsync() method. The final thing to note from our views is that I am using the MessagingCenter to communicate between the TaskDetail and TaskList views. If I change the item in the TaskDetail list, then I want to update the list in the TaskList view. Note that all the code we have added to the solution thus far is in the common TaskList project. Nothing is required for this simple example in a platform specific project. That isn't normal, as we shall see. Building the Client for Android \u00b6 Now we're ready to build our client applications. We'll start with the Android version. Prior to running the application, we need to make two additional changes. Go to your Android project and open the MainActivity.cs file. In the OnCreate method we need to add an initalizer for our Mobile Apps SDK: protected override void OnCreate(Bundle bundle) { TabLayoutResource = Resource.Layout.Tabbar; ToolbarResource = Resource.Layout.Toolbar; base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); LoadApplication(new App()); } Finally, as Android has an explicit permission model (this has somewhat changed in the latest version of Android), we need to say the application requires the Internet permission. Right-click on the Android project and go to Options . Select Android Application from under the Build section. At the bottom of the options panel, you'll see a list of permissions. Find Internet and check it and then click the OK button. Next we need to configure the solution to run the Android project. Right-click the TaskList.Droid project, then select Set as StartUp Project . Right-click the TaskList.Droid project again, then select Build TaskList.Droid . The drop-down between the run button in the top left of Visual Studio for Mac and the Build status at the top of Visual Studio for Mac, now allows you to choose an emulator or device to run your app against. By default, Visual Studio for Mac will create several emulators for you. You can also use the Manage Google Emulators... option to create additional Android Virtual Devices (AVDs) and download other images online. Tip When testing the mobile client manually through the Android Emulator, you are likely to need to rebuild the application. You do not have to shut down the emulator between runs. You can leave it running. The application will be stopped and replaced before starting again. This can significantly speed up the debug cycle since you are not waiting for the emulator to start each time. Watch the Output window. If the debugger won't connect or the application won't start, you may need to restart your computer or the emulator to get the network working. Tip I have had some reports of the TaskList app crashing when using an x86 emulator. If you run into these problems, check out the Android Tips section. If everything is working, you should see the Android Emulator display your mobile client: Warn You can also build the Android version on Windows with Visual Studio. However, I find that version mismatches between Mono (which is used on the mac) and Visual Studio - particularly in reference to the version of the .NET framework - cause issues when swapping between the two environments. For best results, stay in one environment. Note that the task list view is a \"dark\" style and the rest of the app is a \"light\" style. This is because the default styling on an Android device is light. We are using the default styling on two of the pages and specifying colors on the list page. Fortunately, Xamarin Forms allows for platform-specific styling . The final sample has platform-specific styling for the list page. Building the Client for iOS \u00b6 With Android done, we can now turn to the iOS platform. Like we did for Android, we must first initalize our Mobile Apps SDK for our platform. Open the AppDelegate.cs file in your iOS project. In the FinishedLaunching method, we will initalize our SDK: public override bool FinishedLaunching(UIApplication app, NSDictionary options) { global::Xamarin.Forms.Forms.Init(); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); LoadApplication(new App()); return base.FinishedLaunching(app, options); } Now we can build and run our app: Right-click the TaskList.iOS project and select Set as StartUp Project . Change the Debug configuration (top bar) to Debug | iPhoneSimulator . Right-click the TaskList.iOS project and select Build TaskList.iOS . If you have never used Visual Studio for Mac to build and run an iOS app before, it is possible that you will receive an error having to do with code signing keys, provisioning profiles, or signing identities. This is because you did not select the iPhoneSimulator prior to building. Right-click the TaskList.iOS project and select Clean TaskList.iOS , then restart the process, but selecting the iPhoneSimulator configuration. You can now select from several simulator options from the drop-down to the left of the build status. You should only use Device if you have signed up for the Apple Developer Program and linked a provisioning profile to your XCode configuration. Pick one of the simulator options like the iPhone 7 iOS 10.0 simulator, then click on Run to run the simulator. The final product screens look like this: Some Final Thoughts \u00b6 If you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations. There are a lot of places where things can go wrong. You are really integrating the build systems across Android, iOS, Xamarin, Xcode, and Azure. Fortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work. The following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order. If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.","title":"Your First App - Mac Edition"},{"location":"chapter1/firstapp_mac/#your-first-mobile-app-on-a-mac","text":"There is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects. First, wouldn't it be nice to write some code and get something working? Microsoft Azure has a great first-steps tutorial that takes you via the quickest possible route from creating a mobile backend to having a functional backend. I would like to take things a little slower so that we can understand what is going on while we are doing the process. We will have practically the same application at the end. The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly. If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up. This chapter covers the set up required for a MacOS computer. If you wish to develop your applications on a Windows PC, then skip to the prior section . The application we are going to build together is a simple task list. The mobile client will have three screens - an entry screen, a task list and a task details page. I have mocked these pages up using MockingBot . Tip Mocking your screens before you start coding is a great habit to get into. There are some great tools available including free tools like MockingBot . Doing mockups before you start coding is a good way to prevent wasted time later on. Tip If you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one. Other platforms will need it though, so it's best to start with the least common denominator. It's the same reason I add a refresh button even though it's only valid on Windows Phone! My ideas for this app include: Tapping on a task title in the task list will bring up the details page. Toggling the completed link in the task list will set the completed flag. Tapping the spinner will initiate a network refresh. Now that we have our client screens planned out, we can move onto the thinking about the mobile backend.","title":"Your First Mobile App on a Mac"},{"location":"chapter1/firstapp_mac/#the-mobile-backend","text":"The mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python). Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service.","title":"The Mobile Backend"},{"location":"chapter1/firstapp_mac/#creating-a-simple-azure-mobile-apps-backend","text":"To get started: Fire up Visual Studio for Mac . Create a new solution with File -> New Solution... . In the New Project window, choose Other -> ASP.NET , select Empty ASP.NET Project , then click Next . In the Configure your Web project window, check the Web API box and uncheck the Include Unit Test Project , then click Next . In the second Configure your new project window, enter Backend for the for the Project Name and Chapter1 for the Solution name. Click Create to generate the files. At this point, Visual Studio for Mac will lay down the template files on your disk and download the core ASP.NET libraries from NuGet. You will need to accept the licenses for the downloaded NuGet packages. Visual Studio for Mac does not have the range of templates that Visual Studio for the PC has. As a result, we will need to do some additional work putting together a base project. The core of Azure Mobile Apps runs on .NET Framework 4.6.x. Right-click your Backend project in the Solution Explorer and choose Options . The target framework setting is located in the Build -> General section. You can choose any .NET Framework in the 4.6.x range. Click OK to accept the change and close the Project Options. Although a core set of NuGet packages are installed during project creation, we need to add the Azure Mobile Apps NuGet packages. The easiest way to do this is to install the Azure Mobile .NET Server Quickstart NuGet package, which contains dependencies on all the other Azure Mobile Apps packages. Expand the Backend project in the solution explorer, right-click Packages , then select Add Packages... . Use the search box to find the appropriate NuGet package. You also need to add the Owin System host Microsoft.Owin.Host.Systemweb NuGet package. You should take the opportunity to update any NuGet packages that were automatically added to the project. To do so, right-click Packages , then choose Update . Start by configuring the Azure Mobile Apps SDK. The Owin process runs the Startup.cs object to configure itself. To create the Startup.cs class: Right-click the Backend folder. Select Add -> New File... . Select General -> Empty Class , and set the name of the class to Startup.cs . Click New . Leave off the .cs on the end If your filename does not have a dot in it (and some that we use do), you can leave off the .cs trailing extension. Visual Studio for Mac will add it for you. The Startup.cs class looks like this: using Microsoft.Owin; using Owin; [assembly: OwinStartup(typeof(Backend.Startup))] namespace Backend { public partial class Startup { public void Configuration(IAppBuilder app) { ConfigureMobileApp(app); } } } The ConfigureMobileApp() method is located in the App_Start\\Startup.MobileApp.cs file that we will create next. Create the file the same way you did the Startup.cs file, then enter the following into the new file: using System; using System.Collections.Generic; using System.Configuration; using System.Data.Entity; using System.Web.Http; using Microsoft.Azure.Mobile.Server; using Microsoft.Azure.Mobile.Server.Authentication; using Microsoft.Azure.Mobile.Server.Config; using Backend.DataObjects; using Backend.Models; using Owin; namespace Backend { public partial class Startup { public static void ConfigureMobileApp(IAppBuilder app) { var config = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(config); Database.SetInitializer(new MobileServiceInitializer()); app.UseWebApi(config); } } public class MobileServiceInitializer : CreateDatabaseIfNotExists<MobileServiceContext> { protected override void Seed(MobileServiceContext context) { List<TodoItem> todoItems = new List<TodoItem> { new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"First item\", Complete = false }, new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"Second item\", Complete = false } }; foreach (TodoItem todoItem in todoItems) { context.Set<TodoItem>().Add(todoItem); } base.Seed(context); } } } Let's break this down a little bit. The ConfigureMobileApp() method is called to configure Azure Mobile Apps when the service starts. The code tells the SDK that we want to use tables and that those tables are backed with Entity Framework. We also want to initialize the database that we are going to use. That database is going to use a DbContext called MobileServiceContext . The initialization code will create the database and seed it with two new items if it doesn't already exist. If it exists, then we assume that we don't need to seed the database with data. The MobileServiceContext is used to configure the tables within the database and to project those tables into Entity Framework. It relies on a model for each table. In Azure Mobile Apps, this is called the Data Transfer Object or DTO. The server will serialize a DTO to JSON for transmission. Our DTO is in a new directory called DataObjects (right-click on Backend and choose Add -> New Folder... to create it) and is called TodoItem.cs : using Microsoft.Azure.Mobile.Server; namespace Backend.DataObjects { public class TodoItem : EntityData { public string Text { get; set; } public bool Complete { get; set; } } } We base each DTO we use on the EntityData class, since we are using Entity Framework. This sets up some additional columns in the data model so that we can keep track on mobile device changes. We will be discussing this in more detail in the Data Access and Offline Sync chapter. We also need the Models\\MobileServiceContext.cs which sets up the tables for us: using System.Data.Entity; using System.Data.Entity.ModelConfiguration.Conventions; using System.Linq; using Microsoft.Azure.Mobile.Server; using Microsoft.Azure.Mobile.Server.Tables; using Backend.DataObjects; namespace Backend.Models { public class MobileServiceContext : DbContext { private const string connectionStringName = \"Name=MS_TableConnectionString\"; public MobileServiceContext() : base(connectionStringName) { } public DbSet<TodoItem> TodoItems { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); } } } A DbSet<> statement is needed for each table we wish to expose to the mobile clients. There are a couple of important items here. Firstly, our connection string is called MS_TableConnectionString . This is set up in the Web.config file and overridden in the Azure Portal so that you can do both local development and run against a production database in the cloud. Do not change this name. The OnModelCreating() method sets up the tables to handle the service columns that are contained within the EntityData class used by the DTO. Certain fields need to be indexed and triggers need to be added to keep the values updated properly. Finally (in terms of code), we need to create a table controller. This is the endpoint that is exposed on the Internet that our mobile clients will access to send and receive data. Create a Controllers folder and add the following TodoItemController.cs class: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) => DeleteAsync(id); } } The TableController is the central processing for the database access layer. It handles all the OData capabilities for us and exposes these as REST endpoints within our WebAPI. This means that the actual code for this controller is tiny - just 12 lines of code - but very powerful. Info OData is a specification for accessing table data on the Internet. It provides a mechanism for querying and manipulating data within a table. Entity Framework is a common data access layer for ASP.NET applications. Our last step in our backend before publishing it is to edit the Web.config file. The Web.config file tells IIS about the run-time settings for this application. We need to set up the MS_TableConnectionString and several app settings. Inevitably, I copy a created Web.config rather than starting from scratch: <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <configSections> <section name=\"entityFramework\" type=\"System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\" requirePermission=\"false\" /> </configSections> <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"Data Source=(localdb)\\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\\aspnet-Backend-20170308080621.mdf;Initial Catalog=aspnet-Backend-20170308080621;Integrated Security=True;MultipleActiveResultSets=True\" providerName=\"System.Data.SqlClient\" /> </connectionStrings> <appSettings> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> </appSettings> <system.web> <httpRuntime targetFramework=\"4.6\" /> <compilation debug=\"true\" targetFramework=\"4.6\" /> </system.web> <system.webServer> <validation validateIntegratedModeConfiguration=\"false\" /> <modules runAllManagedModulesForAllRequests=\"true\" /> <handlers> <remove name=\"ExtensionlessUrlHandler-Integrated-4.0\" /> <remove name=\"OPTIONSVerbHandler\" /> <remove name=\"TRACEVerbHandler\" /> <add name=\"ExtensionlessUrlHandler-Integrated-4.0\" path=\"*.\" verb=\"*\" type=\"System.Web.Handlers.TransferRequestHandler\" preCondition=\"integratedMode,runtimeVersionv4.0\" /> </handlers> </system.webServer> <runtime> <assemblyBinding xmlns=\"urn:schemas-microsoft-com:asm.v1\" xmlns:bcl=\"urn:schemas-microsoft-com:bcl\"> <dependentAssembly> <assemblyIdentity name=\"Newtonsoft.Json\" publicKeyToken=\"30ad4fe6b2a6aeed\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-9.0.0.0\" newVersion=\"9.0.0.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"System.Web.Http\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.2.3.0\" newVersion=\"5.2.3.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"System.Net.Http.Formatting\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.2.3.0\" newVersion=\"5.2.3.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"Microsoft.Data.Edm\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.8.1.0\" newVersion=\"5.8.1.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"Microsoft.Data.OData\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.8.1.0\" newVersion=\"5.8.1.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"System.Spatial\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-5.8.1.0\" newVersion=\"5.8.1.0\" /> </dependentAssembly> <dependentAssembly> <assemblyIdentity name=\"Microsoft.Owin\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" /> <bindingRedirect oldVersion=\"0.0.0.0-3.0.1.0\" newVersion=\"3.0.1.0\" /> </dependentAssembly> </assemblyBinding> </runtime> <entityFramework> <defaultConnectionFactory type=\"System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework\" /> <providers> <provider invariantName=\"System.Data.SqlClient\" type=\"System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer\" /> </providers> </entityFramework> </configuration> Choose Build All from the Build menu and ensure your project compiles without errors.","title":"Creating a Simple Azure Mobile Apps Backend"},{"location":"chapter1/firstapp_mac/#building-an-azure-app-service-for-mobile-apps","text":"The next step in the process is to build the resources on Azure that will run your mobile backend. Start by logging into the Azure portal , then follow these instructions: Click the big + New button in the top-left corner. Click Web + Mobile , then Mobile App . Enter a unique name in the App name box. Tip Since the name doesn't matter and it has to be unique, you can use a GUID generator to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID (example: chapter1-GUID) to aid in discovery later on. Generally, the first four digits of a GUID are enough to identify individual resources. If you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the Subscription drop-down. Select Create new under resource group and enter a name for this mobile application. Resource Groups Resource groups are great for grouping all the resources associated with a mobile application together. During development, it means you can delete all the resources in one operation. For production, it means you can see how much the service is costing you and how the resources are being used. Finally, select or create a new App Service Plan . App Service Plan The App Service Plan is the thing that actually bills you - not the web or mobile backend. You can run a number of web or mobile backends on the same App Service Plan. I tend to create a new App Service Plan for each mobile application. This is because the App Service Plan lives inside the Resource Group that you create. The process for creating an App Service Plan is straight forward. You have two decisions to make. The first decision is where is the service going to run. In a production environment, the correct choice is \"near your customers\". \"Close to the developers\" is a good choice during development. Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location. With 16 regions to choose from, you have a lot of choice. The second decision you have to make is what to run the service on; also known as the Pricing tier. If you Click View all , you will see you have lots of choices. F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota. That leaves Basic, Standard and Premium. Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks. Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example. Finally, there is a number after the plan. This tells you how big the virtual machine is that the plan is running on. The numbers differ by number of cores and memory. For our purposes, an F1 Free site is enough to run this small demonstration project. More complex development projects should use something in the Basic range of pricing plans. Production apps should be set up in Standard or Premium pricing plans. Once you have created your app service plan and saved it, Click Create . The creation of the service can take a couple of minutes. You can monitor the process of deployment by clicking on the Notifications icon. This is in the top bar on the right-hand side and looks like a Bell. Clicking on a specific notification will provide more information about the activity. Once you have created your app service, the App Service blade will open. We will also want a place to store our data. This role is taken on by a SQL Azure instance. We could link an existing database if we had one defined. However, we can also create a test database. Tip Creating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database. This option is not normally available through other SQL database creation flows. Before we can create a database, we need to create a logical server for the database. The SQL Server (the logical server) sets the region and the login credentials for all the contained databases: Click Resource groups in the left hand side menu. Click the resource group you created. Click Add at the top of the blade. Enter SQL Server into the search box, then press Enter. Click SQL Server (logical server) . Click Create . Enter the information required by the form: A server name (which must be unique in the world - this is a great place to use a GUID). A username and password for accessing the databases on the server. Select the existing resource group. Pick the same Location as you did for the App Service Plan. Click Create . Once the deployment has completed, you can move on to creating and linking a database. You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner. To create and link the database: Click Resource groups in the left hand side menu. Click the resource group you created. Click the App Service your created. Tip If you pinned your App Service to the dashboard, you can Click the pinned App Service instead. It will bring you to the same place. Click Data connections in the MOBILE menu. You can also search for Data connections in the left hand menu. Click Add . In the Type box, select SQL Database . Click the unconfigured SQL Database link: In the Database blade, select Create a new database . Enter a name for the database (like chapter1-db ). Click the Target server box and select the logical server you created earlier. Select a Pricing Tier, then click Apply . Click Select to close the SQL Database blade. Click the Connection string box. Enter the username and password you set up for the SQL logical server. Click OK . The username and password will be validated before proceeding. Click OK to close the Add data connection blade. This produces another deployment step that creates a SQL database with your settings and binds it to the App Service. Once complete, the connection MS_TableConnectionString will be listed in Data Connections blade.","title":"Building an Azure App Service for Mobile Apps"},{"location":"chapter1/firstapp_mac/#deploying-the-azure-mobile-apps-backend","text":"One of the areas I love Visual Studio for the PC over the Mac edition is in publishing to Azure. On the PC, that's a right-click action. There is no publish action in Visual Studio for Mac. However, Azure App Service has enough other tools available to publish a service. These include continuous integration technologies that link to Visual Studio Team Services or GitHub, integrations with cloud storage providers like OneDrive and the venerable but trusty ftp mechanisms. If none of those suit you, you can use drag-and-drop, which is what I am going to do here. Return to the browser and log into the Azure portal . Go to the Settings blade for your Mobile App. Click Advanced Tools in the DEVELOPMENT TOOLS menu. Click Go in the Advanced Tools blade. The page that loads should match https://{YourMobileApp}.scm.azurewebsites.net/. Select the Debug Console menu from the top and choose CMD . Within the file structure listing, click site , then wwwroot . Remove the hostingstart.html file; click the circle with a minus symbol in it to the left of that file and confirm the dialog to delete this file. On your Mac, use the Finder to navigate to the folder that contains your Mobile App Backend. Select the following folder and files: bin packages.config Web.config Drag and drop those files into the browser window where the hostingstart.html file used to be. A progress indicator should appear near the top right. Upon completion you should see the files appear in the file list: You can test your deployed app by browsing to https://{yourmobileapp}.azurewebsites.net/tables/todoitem?ZUMO-API-VERSION=2.0.0 - this is the same URL that your mobile app will connect to later on. Replace {yourmobileapp} with the name of the App Service that you created. If everything is working, you should see some JSON results in your window: The first request will take some time as the App Service is waking up your service, which is initializing the database and seeding the data into that database. Info You will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code. ZUMO was the original code name within Microsoft for A ZU re MO bile.","title":"Deploying the Azure Mobile Apps Backend"},{"location":"chapter1/firstapp_mac/#the-mobile-client","text":"Now that the mobile backend is created and deployed, we can move onto the mobile application that your users would install on their phones. We are going to use Xamarin.Forms to produce a cross-platform application for iOS and Android, and the majority of the code will be placed in a shared project that both platforms will use. You can get as high as 95% code re-use using Xamarin.Forms which is a major productivity boost in complex applications. Info When you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows. Right-click the Chapter1 solution, then select Add -> Add New Project . This will bring up the familiar New Project dialog. Select the Multiplatform -> App -> Forms App template, then click Next . Give the project a name such as TaskList , ensure Android and iOS are both selected, select Use Shared Library and click Next . Click Create on the next screen to create the projects. You will see that three new projects are created: a common library that you named, plus a project for each platform that you choise (in this case two platforms - Android and iOS): Most of our work will happen in the common library in this walkthrough. However, we can introduce platform-specific code at any point. The platform-specific code is stored in the platform-specific project. There is one final item we must do before we leave the set up of the project. There are a number of platform upgrades that inevitably have to happen. The Xamarin Platform is updated much more often than the project templates in Visual Studio for Mac. Updates to the Xamarin platform are released via NuGet. Since we are going to be integrating the Azure Mobile Apps client SDK, you should add the Microsoft.Azure.Mobile.Client NuGet package. This will need to be done within each platform-specific project (TaskList.Droid and TaskList.iOS). Warn Although it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services. There are many differences between the wire protocols of the two products. You can start by updating the existing NuGet packages. Right-click the Packages folder in each project, then select Update . Then add the Microsoft.Azure.Mobile.Client SDK. Right-click the Packages folder again, select Add Packages... , then enter the package name in the search box. Info Android generally has more updates than the other platforms. Ensure that you update the main Xamarin.Forms package and then refresh the update list. This will ensure the right list of support packages is updated.","title":"The Mobile Client"},{"location":"chapter1/firstapp_mac/#building-the-common-library","text":"There are two parts that we must concentrate on within the common library. Firstly, we must deal with the connection to our mobile backend using the Azure Mobile Apps client SDK. Secondly, we need to create the three pages that our application will show. Start by adding the following folders to your Shared Library project: Abstractions Models Pages Services ViewModels Our application will use MVVM (Model-View-ViewModel) for the interaction. The UI will be written in XAML, and the ViewModel will be used to provide information to that UI. The Services folder will hold the classes necessary to communicate with the Azure Mobile Apps backend. Finally, we have a number of interfaces and common classes that we will need to make everything work. Remove the other contents of the shared project. They will not be required.","title":"Building the Common Library"},{"location":"chapter1/firstapp_mac/#building-an-azure-mobile-apps-connection","text":"One of the things I often stress is to set yourself up assuming you are going to test your product. Testing is a good thing. In cloud development, we often set up a \"mock\" service where we swap out the \"real\" service and use something that has the same interface, but deals with local data only. This application will have two interfaces. The first represents a cloud service, which will have a collection of tables we want to access. This will be defined in Abstractions\\ICloudService.cs . namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; } } We haven't defined ICloudTable<> nor TableData yet. The ICloudTable<T> interface defines what the application can do to a table. It will be defined in Abstractions\\ICloudTable.cs : using System.Collections.Generic; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); } } The ICloudTable<T> interface defines the normal CRUD operations: Create, Read, Update, and Delete. However, it does so asynchronously. We are dealing with network operations and it is easy for those operations to tie up the UI thread for an appreciable amount of time. Making them async provides the ability to respond to other events and ensure your application is responsive to the user. There are some fields that every single record within an Azure Mobile Apps table provides. These fields are required for offline sync capabilities like incremental sync and conflict resolution. On the server, this is represented by the EntityData class. We cannot use that class as it contains the Entity Framework additions for indexing and triggers. The fields are instead provided by a new abstract base class on the client called Abstractions\\TableData : using System; namespace TaskList.Abstractions { public abstract class TableData { public string Id { get; set; } public DateTimeOffset? UpdatedAt { get; set; } public DateTimeOffset? CreatedAt { get; set; } public byte[] Version { get; set; } } } As we will learn when we deal with table data , these fields need to be defined with the same name and semantics as on the server. Our model on the server was sub-classed from EntityData and the EntityData class on the server defines these fields. It's tempting to call the client version of the class the same as the server version. If we did that, the models on both the client and server would look the same. However, I find that this confuses the issue. The models on the client and server are not the same. They are missing the Deleted flag and they do not contain any relationship information on the client. I choose to deliberately call the base class something else on the client to avoid this confusion. We will be adding to these interfaces in future chapters as we add more capabilities to the application. The concrete implementations of these classes are similarly easily defined. The Azure Mobile Apps Client SDK does most of the work for us. Here is the concrete implementation of the ICloudService (in Services\\AzureCloudService.cs ): using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(\"https://my-backend.azurewebsites.net\"); } public ICloudTable<T> GetTable<T>() where T : TableData { return new AzureCloudTable<T>(client); } } } Ensure you use HTTPS If you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint. You must provide the https version of the endpoint when using App Service. The http endpoint redirects to https and the standard HttpClient does not handle redirects. The Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published. Just swap my-backend out for the name of your mobile backend and the rest is silently dealt with. Warn The name Microsoft.WindowsAzure.MobileServices is a hold-over from the old Azure Mobile Services code-base. Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps. We also need a concrete implementation of the ICloudTable<T> interface (in Services\\AzureCloudTable.cs ): using System.Collections.Generic; using System.Collections.ObjectModel; using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudTable<T> : ICloudTable<T> where T : TableData { MobileServiceClient client; IMobileServiceTable<T> table; public AzureCloudTable(MobileServiceClient client) { this.client = client; this.table = client.GetTable<T>(); } #region ICloudTable implementation public async Task<T> CreateItemAsync(T item) { await table.InsertAsync(item); return item; } public async Task DeleteItemAsync(T item) { await table.DeleteAsync(item); } public async Task<ICollection<T>> ReadAllItemsAsync() { return await table.ToListAsync(); } public async Task<T> ReadItemAsync(string id) { return await table.LookupAsync(id); } public async Task<T> UpdateItemAsync(T item) { await table.UpdateAsync(item); return item; } #endregion } } The Azure Mobile Apps Client SDK does a lot of the work for us. In fact, we are just wrapping the basic interface here. The majority of the code for dealing with the remote server is done for us. Tip You can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as: public async Task DeleteItemAsync(T item) => await table.DeleteAsync(item); You may see this sort of short hand in samples. We also need to create the model that we will use for the data. This should look very similar to the model on the server - including having the same name and fields. In this case, it's Models\\TodoItem.cs : using TaskList.Abstractions; namespace TaskList.Models { public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } } } We have a final piece of code to write before we move on to the views, but it's an important piece. The ICloudService must be a singleton in the client. We will add authentication and offline sync capabilities in future versions of this code. The singleton becomes critical when using those features. For right now, it's good practice and saves on memory if you only have one copy of the ICloudService in your mobile client. Since there is only one copy of the App.cs in any given app, I can place it there. Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this. Here is the App.cs : using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { CloudService = new AzureCloudService(); MainPage = new NavigationPage(new Pages.EntryPage()); } // There are life cycle methods here... } } We haven't written Pages.EntryPage yet, but that's coming. This file replaces the App.xaml and App.xaml.cs files from the original shared project. If you have not done so already, ensure you remove those files now.","title":"Building an Azure Mobile Apps Connection"},{"location":"chapter1/firstapp_mac/#building-the-ui-for-the-app","text":"Earlier, I showed the mockup for my UI. It included three pages - an entry page, a list page, and a detail page. These pages have three elements - a XAML definition file, a (simple) code-behind file, and a view model. Info This book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML. If you wish to have that sort of introduction, then I recommend reading the excellent book by Charles Petzold: Creating Mobile Apps with Xamarin.Forms . I tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications. It's a nice clean pattern and is well understood and documented. In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements. In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed. To do this, we will use a BaseViewModel class that implements the base functionality for each view. Aside from the INotifyPropertyChanged interface, there are some common properties we need for each page. Each page needs a title, for example, and an indicator of network activity. These can be placed in the Abstractions\\BaseViewModel.cs class: using System; using System.Collections.Generic; using System.ComponentModel; namespace TaskList.Abstractions { public class BaseViewModel : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged; string _propTitle = string.Empty; bool _propIsBusy; public string Title { get { return _propTitle; } set { SetProperty(ref _propTitle, value, \"Title\"); } } public bool IsBusy { get { return _propIsBusy; } set { SetProperty(ref _propIsBusy, value, \"IsBusy\"); } } protected void SetProperty<T>(ref T store, T value, string propName, Action onChanged = null) { if (EqualityComparer<T>.Default.Equals(store, value)) return; store = value; if (onChanged != null) onChanged(); OnPropertyChanged(propName); } public void OnPropertyChanged(string propName) { if (PropertyChanged == null) return; PropertyChanged(this, new PropertyChangedEventArgs(propName)); } } } This is a fairly common INotifyPropertyChanged interface implementation pattern. Each property that we want to expose is a standard property, but the set operation is replaced by the SetProperty() call. The SetProperty() call deals with the notification; calling the event emitter if the property has changed value. We only need two properties on the BaseViewModel : the title and the network indicator. I tend to write my apps in two stages. I concentrate on the functionality of the app in the first stage. There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking. The page is all about the functionality of the various interactions. Once I have the functionality working, I work on the styling of the page. We won't be doing any styling work in the demonstration apps that we write during the course of this book. The EntryPage has just one thing to do. It provides a button that enters the app. When we cover authentication later on, we'll use this to log in to the backend. If you are looking at the perfect app, this is a great place to put the introductory screen. Creating a XAML file is relatively simple. We already created a Pages directory to hold the pages of our application. Right-click the Pages directory in the solution explorer and choose Add -> New File... . In the Add New File dialog, pick Forms -> Forms ContentPage Xaml . Name the new page EntryPage . This will create two files - EntryPage.xaml and EntryPage.xaml.cs . Let's center a button on the page and wire it up with a command. Here is the Pages\\EntryPage.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.EntryPage\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Vertical\" VerticalOptions=\"Center\"> <Button BackgroundColor=\"Teal\" BorderRadius=\"10\" Command=\"{Binding LoginCommand}\" Text=\"Login\" TextColor=\"White\" /> </StackLayout> </ContentPage.Content> </ContentPage> The StackLayout element is our layout element. It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are. The only contents are a button. There are two bindings. These are bound to properties in the view-model. We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command. When the button is tapped, the login command will be run. We'll get onto that in the view-model later. The other file created is the code-behind file. Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial: using TaskList.ViewModels; using Xamarin.Forms; namespace TaskList.Pages { public partial class EntryPage : ContentPage { public EntryPage() { InitializeComponent(); BindingContext = new EntryPageViewModel(); } } } This is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM. We initialize the UI, then bind all the bindings to a new instance of the view model. Speaking of which, the view-model just needs to handle the login click. Note that the location or namespace is TaskList.ViewModels . I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together. However, just about all the sample code that I see has the view-models in a separate namespace. Which one is correct? I'll copy the samples for now. Here is the code for ViewModels\\EntryPageViewModel.cs : using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using Xamarin.Forms; namespace TaskList.ViewModels { public class EntryPageViewModel : BaseViewModel { public EntryPageViewModel() { Title = \"Task List\"; } Command loginCmd; public Command LoginCommand => loginCmd ?? (loginCmd = new Command(async () => await ExecuteLoginCommand())); async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[Login] Error = {ex.Message}\"); } finally { IsBusy = false; } } } } This is a fairly simple view-model but there are some patterns here that are worth explaining. Firstly, note the way we create the LoginCommand property. This is the property that is bound to the Command parameter in the Button of our view. This recipe is the method of invoking a UI action asynchronously. It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity. The second is the pattern for the ExecuteLoginCommand method. Firstly, I ensure nothing else is happening by checking the IsBusy flag. If nothing is happening, I set the IsBusy flag. Then I do what I need to do in a try/catch block. If an exception is thrown, I deal with it. Most of the time this involves displaying an error condition. There are several cross-platform dialog packages to choose from or you can roll your own. That is not covered here. We just write a debug log statement so we can see the result in the debug log. Once everything is done, we clear the IsBusy flag. The only thing we are doing now is swapping out our main page for a new main page. This is where we will attach authentication later on. The next page is the Task List page, which is in Pages\\TaskList.xaml : <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskList\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout> <ListView BackgroundColor=\"#7F7F7F\" CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"True\" IsRefreshing=\"{Binding IsBusy, Mode=OneWay}\" ItemsSource=\"{Binding Items}\" RefreshCommand=\"{Binding RefreshCommand}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout HorizontalOptions=\"FillAndExpand\" Orientation=\"Horizontal\" Padding=\"10\" VerticalOptions=\"CenterAndExpand\"> <Label HorizontalOptions=\"FillAndExpand\" Text=\"{Binding Text}\" TextColor=\"#272832\" /> <Switch IsToggled=\"{Binding Complete, Mode=OneWay}\" /> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"Teal\" Command=\"{Binding AddNewItemCommand}\" Text=\"Add New Item\" TextColor=\"White\" /> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> Note that some bindings here are one-way. This means that the value in the view-model drives the value in the UI. There is nothing within the UI that you can do to alter the state of the underlying property. Some bindings are two-way. Doing something in the UI (for example, toggling the switch) alters the underlying property. This view is a little more complex. It can be split into two parts - the list at the top of the page and the button area at the bottom of the page. The list area uses a template to help with the display of each item. Note that the ListView object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand. It also has an indicator that I have wired up to the IsBusy indicator. Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does. The code behind for the TaskList can be found in Pages\\TaskList.xaml.cs : using TaskList.ViewModels;using Xamarin.Forms; namespace TaskList.Pages { public partial class TaskList : ContentPage { public TaskList() { InitializeComponent(); BindingContext = new TaskListViewModel(); } } } There is a view-model that goes along with the view (in ViewModels\\TaskListViewModel.cs ): using System; using System.Collections.ObjectModel; using System.Collections.Specialized; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskListViewModel : BaseViewModel { public TaskListViewModel() { Title = \"Task List\"; RefreshList(); } ObservableCollection<TodoItem> items = new ObservableCollection<TodoItem>(); public ObservableCollection<TodoItem> Items { get { return items; } set { SetProperty(ref items, value, \"Items\"); } } TodoItem selectedItem; public TodoItem SelectedItem { get { return selectedItem; } set { SetProperty(ref selectedItem, value, \"SelectedItem\"); if (selectedItem != null) { Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem)); SelectedItem = null; } } } Command refreshCmd; public Command RefreshCommand => refreshCmd ?? (refreshCmd = new Command(async () => await ExecuteRefreshCommand())); async Task ExecuteRefreshCommand() { if (IsBusy) return; IsBusy = true; try { var table = App.CloudService.GetTable<TodoItem>(); var list = await table.ReadAllItemsAsync(); Items.Clear(); foreach (var item in list) Items.Add(item); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error loading items: {ex.Message}\"); } finally { IsBusy = false; } } Command addNewCmd; public Command AddNewItemCommand => addNewCmd ?? (addNewCmd = new Command(async () => await ExecuteAddNewItemCommand())); async Task ExecuteAddNewItemCommand() { if (IsBusy) return; IsBusy = true; try { await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail()); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error in AddNewItem: {ex.Message}\"); } finally { IsBusy = false; } } async Task RefreshList() { await ExecuteRefreshCommand(); MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); } } } This is a combination of the patterns we have seen earlier. The Add New Item and Refresh commands should be fairly normal patterns now. We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the SelectedItem property through a two-way binding) and when the user clicks on the Add New Item button. When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed. It is fairly common to use an ObservableCollection or another class that uses the ICollectionChanged event handler for the list storage. Doing so allows the UI to react to changes in the items. Note the use of the ICloudTable interface here. We are using the ReadAllItemsAsync() method to get a list of items, then we copy the items we received into the ObservableCollection . Finally, there is the TaskDetail page. This is defined in the Pages\\TaskDetail.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskDetail\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout Padding=\"10\" Spacing=\"10\"> <Label Text=\"What should I be doing?\"/> <Entry Text=\"{Binding Item.Text}\"/> <Label Text=\"Completed?\"/> <Switch IsToggled=\"{Binding Item.Complete}\"/> <StackLayout VerticalOptions=\"CenterAndExpand\"/> <StackLayout Orientation=\"Vertical\" VerticalOptions=\"End\"> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"#A6E55E\" Command=\"{Binding SaveCommand}\" Text=\"Save\" TextColor=\"White\"/> <Button BackgroundColor=\"Red\" Command=\"{Binding DeleteCommand}\" Text=\"Delete\" TextColor=\"White\"/> </StackLayout> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> This page is a simple form with just two buttons that need to have commands wired up. However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture. As a result of this, we need to handle the passing of the item to be edited. This is done in the Pages\\TaskDetail.xaml.cs code-behind file: using TaskList.Models; using TaskList.ViewModels; using Xamarin.Forms; namespace TaskList.Pages { public partial class TaskDetail : ContentPage { public TaskDetail(TodoItem item = null) { InitializeComponent(); BindingContext = new TaskDetailViewModel(item); } } } The item that is passed in from the TaskList page is used to create a specific view-model for that item. The view-model is similarly configured to use that item: using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskDetailViewModel : BaseViewModel { ICloudTable<TodoItem> table = App.CloudService.GetTable<TodoItem>(); public TaskDetailViewModel(TodoItem item = null) { if (item != null) { Item = item; Title = item.Text; } else { Item = new TodoItem { Text = \"New Item\", Complete = false }; Title = \"New Item\"; } } public TodoItem Item { get; set; } Command cmdSave; public Command SaveCommand => cmdSave ?? (cmdSave = new Command(async () => await ExecuteSaveCommand())); async Task ExecuteSaveCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id == null) { await table.CreateItemAsync(Item); } else { await table.UpdateItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } Command cmdDelete; public Command DeleteCommand => cmdDelete ?? (cmdDelete = new Command(async () => await ExecuteDeleteCommand())); async Task ExecuteDeleteCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id != null) { await table.DeleteItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } } } The save command uses the ICloudTable interface again - this time doing either CreateItemAsync() or UpdateItemAsync() to create or update the item. The delete command, as you would expect, deletes the item with the DeleteItemAsync() method. The final thing to note from our views is that I am using the MessagingCenter to communicate between the TaskDetail and TaskList views. If I change the item in the TaskDetail list, then I want to update the list in the TaskList view. Note that all the code we have added to the solution thus far is in the common TaskList project. Nothing is required for this simple example in a platform specific project. That isn't normal, as we shall see.","title":"Building the UI for the App"},{"location":"chapter1/firstapp_mac/#building-the-client-for-android","text":"Now we're ready to build our client applications. We'll start with the Android version. Prior to running the application, we need to make two additional changes. Go to your Android project and open the MainActivity.cs file. In the OnCreate method we need to add an initalizer for our Mobile Apps SDK: protected override void OnCreate(Bundle bundle) { TabLayoutResource = Resource.Layout.Tabbar; ToolbarResource = Resource.Layout.Toolbar; base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); LoadApplication(new App()); } Finally, as Android has an explicit permission model (this has somewhat changed in the latest version of Android), we need to say the application requires the Internet permission. Right-click on the Android project and go to Options . Select Android Application from under the Build section. At the bottom of the options panel, you'll see a list of permissions. Find Internet and check it and then click the OK button. Next we need to configure the solution to run the Android project. Right-click the TaskList.Droid project, then select Set as StartUp Project . Right-click the TaskList.Droid project again, then select Build TaskList.Droid . The drop-down between the run button in the top left of Visual Studio for Mac and the Build status at the top of Visual Studio for Mac, now allows you to choose an emulator or device to run your app against. By default, Visual Studio for Mac will create several emulators for you. You can also use the Manage Google Emulators... option to create additional Android Virtual Devices (AVDs) and download other images online. Tip When testing the mobile client manually through the Android Emulator, you are likely to need to rebuild the application. You do not have to shut down the emulator between runs. You can leave it running. The application will be stopped and replaced before starting again. This can significantly speed up the debug cycle since you are not waiting for the emulator to start each time. Watch the Output window. If the debugger won't connect or the application won't start, you may need to restart your computer or the emulator to get the network working. Tip I have had some reports of the TaskList app crashing when using an x86 emulator. If you run into these problems, check out the Android Tips section. If everything is working, you should see the Android Emulator display your mobile client: Warn You can also build the Android version on Windows with Visual Studio. However, I find that version mismatches between Mono (which is used on the mac) and Visual Studio - particularly in reference to the version of the .NET framework - cause issues when swapping between the two environments. For best results, stay in one environment. Note that the task list view is a \"dark\" style and the rest of the app is a \"light\" style. This is because the default styling on an Android device is light. We are using the default styling on two of the pages and specifying colors on the list page. Fortunately, Xamarin Forms allows for platform-specific styling . The final sample has platform-specific styling for the list page.","title":"Building the Client for Android"},{"location":"chapter1/firstapp_mac/#building-the-client-for-ios","text":"With Android done, we can now turn to the iOS platform. Like we did for Android, we must first initalize our Mobile Apps SDK for our platform. Open the AppDelegate.cs file in your iOS project. In the FinishedLaunching method, we will initalize our SDK: public override bool FinishedLaunching(UIApplication app, NSDictionary options) { global::Xamarin.Forms.Forms.Init(); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); LoadApplication(new App()); return base.FinishedLaunching(app, options); } Now we can build and run our app: Right-click the TaskList.iOS project and select Set as StartUp Project . Change the Debug configuration (top bar) to Debug | iPhoneSimulator . Right-click the TaskList.iOS project and select Build TaskList.iOS . If you have never used Visual Studio for Mac to build and run an iOS app before, it is possible that you will receive an error having to do with code signing keys, provisioning profiles, or signing identities. This is because you did not select the iPhoneSimulator prior to building. Right-click the TaskList.iOS project and select Clean TaskList.iOS , then restart the process, but selecting the iPhoneSimulator configuration. You can now select from several simulator options from the drop-down to the left of the build status. You should only use Device if you have signed up for the Apple Developer Program and linked a provisioning profile to your XCode configuration. Pick one of the simulator options like the iPhone 7 iOS 10.0 simulator, then click on Run to run the simulator. The final product screens look like this:","title":"Building the Client for iOS"},{"location":"chapter1/firstapp_mac/#some-final-thoughts","text":"If you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations. There are a lot of places where things can go wrong. You are really integrating the build systems across Android, iOS, Xamarin, Xcode, and Azure. Fortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work. The following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order. If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.","title":"Some Final Thoughts"},{"location":"chapter1/firstapp_pc/","text":"Your First Mobile App \u00b6 There is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects. First, wouldn't it be nice to write some code and get something working? Microsoft Azure has a great first-steps tutorial that takes you via the quickest possible route from creating a mobile backend to having a functional backend. I would like to take things a little slower so that we can understand what is going on while we are doing the process. We will have practically the same application at the end. The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly. If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up. This chapter covers the set up required for a Windows PC. If you wish to develop your applications on a Mac, then skip to the next section . The application we are going to build together is a simple task list. The mobile client will have three screens - an entry screen, a task list and a task details page. I have mocked these pages up using MockingBot . Tip Mocking your screens before you start coding is a great habit to get into. There are some great tools available including free tools like MockingBot . Doing mockups before you start coding is a good way to prevent wasted time later on. Tip If you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one. Other platforms will need it though, so it's best to start with the least common denominator. It's the same reason I add a refresh button even though it's only valid on Windows Phone! My ideas for this app include: Tapping on a task title in the task list will bring up the details page. Toggling the completed link in the task list will set the completed flag. Tapping the spinner will initiate a network refresh. Now that we have our client screens planned out, we can move onto the thinking about the mobile backend. The Mobile Backend \u00b6 The mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python). Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service. Creating a Simple Azure Mobile Apps Backend \u00b6 Microsoft Azure has included a comprehensive starter kit template in the Azure SDK. To get started: Fire up Visual Studio. Add a new project with File -> New -> Project... In the New Project window: Open up Templates -> Visual C# -> Web and select ASP.NET Web Application (.NET Framework) . Enter Backend for the Name and Chapter1 for the Solution name. Select .NET Framework 4.6 in the framework dropdown at the top. Pick a suitable directory for the Location field. Click OK. In the New ASP.NET Web Application window: Click Azure Mobile App . Do NOT check \"Host in the cloud\" or any other checkboxes. Click OK. At this point, Visual Studio will create your backend project. There are a few files of which you should take note. The Mobile Apps SDK is initialized within App_Start\\Startup.MobileApp.cs (with the call to the configuration routine happening within Startup.cs ). The default startup routine is reasonable but it hides what it is doing behind extension methods. This technique is fairly common in ASP.NET programs. Let's expand the configuration routine to only include what we need: public static void ConfigureMobileApp(IAppBuilder app) { var config = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(config); Database.SetInitializer(new MobileServiceInitializer()); app.UseWebApi(config); } The minimal version of the mobile backend initialization is actually shorter than the original. It also only includes a data access layer. Other services like authentication, storage and push notifications are not configured. There is another method in the App_Start\\Startup.MobileApp.cs file for seeding data into the database for us. We can leave that alone for now, but remember it is there in case you need to seed data into a new database for your own backend. Info We refer to \"seeding data\" into a database. This means that we are going to introduce some data into the database so that we aren't operating on an empty database. The data will be there when we query the database later on. The next important file is the DbContext - located in Models\\MobileServiceContext.cs . Azure Mobile Apps is heavily dependent on Entity Framework v6.x and the DbContext is a central part of that library. Fortunately, we don't need to do anything to this file right now. Finally, we get to the meat of the backend. The whole point of this demonstration is to project a single database table - the TodoItem table - into the mobile realm with the aid of an opinionated OData v3 feed. To that end, we need three items: A DbSet<> within the DbContext A Data Transfer Object (or DTO) A Table Controller When we create the project, a sample of each one of these for the TodoItem table is added for us. You can see the DbSet<> in the Models\\MobileServiceContext.cs file, for example. Let's take a look at the DTO and Table Controller for this example table as well. The DTO for the TodoItem table is located within the DataObjects directory: using Microsoft.Azure.Mobile.Server; namespace Backend.DataObjects { public class TodoItem : EntityData { public string Text { get; set; } public bool Complete { get; set; } } } Note that the model uses EntityData as a base class. The EntityData class adds five additional properties to the class - we'll discuss those in more details during the Data Access and Offline Sync chapter. Finally, let's look at the table controller for the example TodoItem table. This is located in Controllers\\TodoItemController.cs : using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) => DeleteAsync(id); } } The TableController is the central processing for the database access layer. It handles all the OData capabilities for us and exposes these as REST endpoints within our WebAPI. This means that the actual code for this controller is tiny - just 12 lines of code. Info OData is a specification for accessing table data on the Internet. It provides a mechanism for querying and manipulating data within a table. Entity Framework is a common data access layer for ASP.NET applications. We can build the project at this point. If Visual Studio hasn't done so already, the missing NuGet packages for Azure Mobile Apps will be downloaded. There should not be any errors. If there are, check the typing for any changes you made. Building an Azure App Service for Mobile Apps \u00b6 The next step in the process is to build the resources on Azure that will run your mobile backend. Start by logging into the Azure Portal , then follow these instructions: Click the big + New button in the top-left corner. Click Web + Mobile , then Mobile App . Enter a unique name in the App name box. Tip Since the name doesn't matter and it has to be unique, you can use a GUID generator to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID (example: chapter1-GUID) to aid in discovery later on. Generally, the first four digits of a GUID are enough to identify individual resources. If you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the Subscription drop-down. Select Create new under resource group and enter a name for this mobile application. Resource Groups Resource groups are great for grouping all the resources associated with a mobile application together. During development, it means you can delete all the resources in one operation. For production, it means you can see how much the service is costing you and how the resources are being used. Finally, select or create a new App Service Plan . App Service Plan The App Service Plan is the thing that actually bills you - not the web or mobile backend. You can run a number of web or mobile backends on the same App Service Plan. I tend to create a new App Service Plan for each mobile application. This is because the App Service Plan lives inside the Resource Group that you create. The process for creating an App Service Plan is straight forward. You have two decisions to make. The first decision is where is the service going to run. In a production environment, the correct choice is \"near your customers\". \"Close to the developers\" is a good choice during development. Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location. With 16 regions to choose from, you have a lot of choice. The second decision you have to make is what to run the service on; also known as the Pricing tier. If you Click View all , you will see you have lots of choices. F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota. That leaves Basic, Standard and Premium. Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks. Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example. Finally, there is a number after the plan. This tells you how big the virtual machine is that the plan is running on. The numbers differ by number of cores and memory. For our purposes, an F1 Free site is enough to run this small demonstration project. More complex development projects should use something in the Basic range of pricing plans. Production apps should be set up in Standard or Premium pricing plans. Once you have created your app service plan and saved it, Click Create . The creation of the service can take a couple of minutes. You can monitor the process of deployment by clicking on the Notifications icon. This is in the top bar on the right-hand side and looks like a Bell. Clicking on a specific notification will provide more information about the activity. Once you have created your app service, the App Service blade will open. We will also want a place to store our data. This role is taken on by a SQL Azure instance. We could link an existing database if we had one defined. However, we can also create a test database. Tip Creating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database. This option is not normally available through other SQL database creation flows. Before we can create a database, we need to create a logical server for the database. The SQL Server (the logical server) sets the region and the login credentials for all the contained databases: Click Resource groups in the left hand side menu. Click the resource group you created. Click Add at the top of the blade. Enter SQL Server into the search box, then press Enter. Click SQL Server (logical server) . Click Create . Enter the information required by the form: A server name (which must be unique in the world - this is a great place to use a GUID). A username and password for accessing the databases on the server. Select the existing resource group. Pick the same Location as you did for the App Service Plan. Click Create . Once the deployment has completed, you can move on to creating and linking a database. You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner. To create and link the database: Click Resource groups in the left hand side menu. Click the resource group you created. Click the App Service your created. Tip If you pinned your App Service to the dashboard, you can Click the pinned App Service instead. It will bring you to the same place. Click Data connections in the MOBILE menu. You can also search for Data connections in the left hand menu. Click Add . In the Type box, select SQL Database . Click the unconfigured SQL Database link: In the Database blade, select Create a new database . Enter a name for the database (like chapter1-db ). Click the Target server box and select the logical server you created earlier. Select a Pricing Tier, then click Apply . Click Select to close the SQL Database blade. Click the Connection string box. Enter the username and password you set up for the SQL logical server. Click OK . The username and password will be validated before proceeding. Click OK to close the Add data connection blade. This produces another deployment step that creates a SQL database with your settings and binds it to the App Service. Once complete, the connection MS_TableConnectionString will be listed in Data Connections blade. Deploying the Azure Mobile Apps Backend \u00b6 Deploying to Azure as a developer can be accomplished while entirely within Visual Studio: Right-Click the Backend project, then select Publish... . The following will be shown: If you have an earlier version of Visual Studio, a different screen will be shown. If Azure App Service is not listed, ensure you have the latest version of Azure SDK installed - at least v2.9. Click Microsoft Azure App Service . Click Select Existing , then click Publish . You may be prompted to enter your Azure credentials here. Enter the same information that you enter to access the Azure Portal. In the lower box, expand the resource group that you created and select the app service you created in the portal. Click OK . Visual Studio will open a browser pointing to the root of your Azure App Service. Add /tables/todoitem?ZUMO-API-VERSION=2.0.0 to the end of the URL. This will show the JSON contents of the table that was defined as a table controller in the backend. Info You will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code. ZUMO was the original code name within Microsoft for A ZU re MO bile. Building The Mobile Client \u00b6 Info When you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows Now that the mobile backend is created and deployed, we can move onto the client side of things. Right-Click the solution and select Add -> New Project... . This will bring up the familiar New Project dialog. Select Visual C# -> Cross-Platform -> Cross Platform App (Xamarin.Forms or Native) . Give the project a name, then Click OK . In the New Cross Platform App window, select Blank App , and use Xamarin.Forms as the UI technology, and a Shared Project for the code sharing strategy. Project creation will take longer than you expect, but there is a lot going on. If you have never created a mobile or UWP project before, you will be prompted to turn on Windows 10 Developer Mode: Developer mode in Windows 10 allows you to run unsigned binaries for development purposes and to turn on debugging so that you can step through your UWP programs within Visual Studio. Visual Studio may also just bring up the appropriate Settings page where you can turn on Developer mode. We will also get asked to choose what version of the Universal Windows platform we want to target: Version 10240 was the first version of Windows 10 that was released to the general public, so that's a good minimum version to pick. In general, the defaults for the Universal Windows Platform choice are good enough. Xamarin allows us to build iOS applications directly from Visual Studio. For this to work, we must have access to a Mac. This could be anything from a MacBook Air/Pro, to a Mac Mini in a drawer or closet in the office, or maybe even a Mac in the cloud . The Xamarin tools use SSH to connect to the Mac, which must be configured to build iOS apps from Visual Studio . Tip If you don't have a Mac and are not interested in building iOS applications, don't give up now! You can cancel through the Mac specific project setup and continue with building a great Android and Universal Windows app. You can also use Visual Studio Mobile Center to build an iOS project. You can delete the iOS specific project after it has been created. When prompted about the Xamarin Mac Agent, Click OK to get the list of local mac agents: Highlight your mac (in case there are multiples), then Click Connect... . If your mac is not listed or you are using a Mac in the cloud, then you can always enter the IP address for your mac. Tip For more troubleshooting tips, visit The Xamarin Troubleshooting Site . You will be prompted for your username and password: Just enter the (real) username and password for your account on your mac and click on Login . Tip Apple tries very hard to hide the real username of your account from you. The easiest way to find your mac username is to open up the Finder. The name next to your home icon is the name of your account. If the connection is successful, you will see a green icon in the Xamarin Visual Studio toolbar area. It may take a minute or two to connect and verify that the mac can be used. Once the project is created, you will see that four new projects have been created: a common library which you named plus one project for each platform that has been chosen. Since we chose a project with three platforms, we get four projects: Most of our work will happen in the common library. However, we can introduce platform-specific code at any point. The platform-specific code is stored in the platform-specific project. There is one final item we must do before we leave the set up of the project. There are a number of platform upgrades that inevitably have to happen. The Xamarin Platform is updated much more often than the Visual Studio plugin - the updates are released via NuGet: the standard method of distributing libraries for .NET applications. Warn Although it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services. There are many differences between the wire protocols of the two products. You can install the NuGet packages by right-clicking on the solution and selecting Manage NuGet Packages for Solution... . You can generally select all the updates. However, do NOT update the Jwt package (System.IdentityModel.Tokens.Jwt) as this will break the server project. You can update the System.IdentityModel.Tokens.Jwt to the latest v4.x release. Do NOT install a v5.x release. Info Android generally has more updates than the other platforms. Ensure that you update the main Xamarin.Forms package and then refresh the update list. This will ensure the right list of packages is updated. You should also install the Microsoft.Azure.Mobile.Client library in all the client projects. Building the Common Library \u00b6 There are two parts that we must concentrate on within the common library. The first is the connection to Azure Mobile Apps and the second is in the pages that the user interacts with. In both cases, there are best practices to observe. Building an Azure Mobile Apps Connection \u00b6 We will rely on interfaces for defining the shape for the class for any service that we interact with. This is really not important in small projects like this one. This technique allows us to mock the backend service, as we shall see later on. Mocking the backend service is a great technique to rapidly iterate on the front end mobile client without getting tied into what the backend is doing. Let's start with the cloud service - this is defined in Abstractions\\ICloudService.cs . It is used for initializing the connection and getting a table definition: namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; } } The ICloudTable generic interface represents a CRUD interface into a table and is defined in Abstractions\\ICloudTable.cs : using System.Collections.Generic; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); } } The ICloudTable<T> interface defines the normal CRUD operations: Create, Read, Update and Delete. However, it does so asynchronously. We are dealing with network operations in general so it is easy for those operations to tie up the UI thread for an appreciable amount of time. Making them async provides the ability to respond to other events. I also provide a ReadAllItemsAsync() method that returns a collection of all the items. There are some fields that every single record within an Azure Mobile Apps table provides. These fields are required for offline sync capabilities like incremental sync and conflict resolution. The fields are provided by an abstract base class on the client called TableData : using System; namespace TaskList.Abstractions { public abstract class TableData { public string Id { get; set; } public DateTimeOffset? UpdatedAt { get; set; } public DateTimeOffset? CreatedAt { get; set; } public byte[] Version { get; set; } } } As we will learn when we deal with table data , these fields need to be defined with the same name and semantics as on the server. Our model on the server was sub-classed from EntityData and the EntityData class on the server defines these fields. It's tempting to call the client version of the class the same as the server version. If we did that, the models on both the client and server would look the same. However, I find that this confuses the issue. The models on the client and server are not the same. They are missing the Deleted flag and they do not contain any relationship information on the client. I choose to deliberately call the base class something else on the client to avoid this confusion. We will be adding to these interfaces in future chapters as we add more capabilities to the application. The concrete implementations of these classes are similarly easily defined. The Azure Mobile Apps Client SDK does most of the work for us. Here is the concrete implementation of the ICloudService (in Services\\AzureCloudService.cs ): using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(\"https://my-backend.azurewebsites.net\"); } public ICloudTable<T> GetTable<T>() where T : TableData { return new AzureCloudTable<T>(client); } } } Ensure you use HTTPS If you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint. You must provide the https version of the endpoint when using App Service. The http endpoint redirects to https and the standard HttpClient does not handle redirects. The Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published. Just swap out the name of your mobile backend and the rest is silently dealt with. Warn The name Microsoft.WindowsAzure.MobileServices is a hold-over from the old Azure Mobile Services code-base. Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps. We also need a concrete implementation of the ICloudTable<T> interface (in Services\\AzureCloudTable.cs ): using System.Collections.Generic; using System.Collections.ObjectModel; using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudTable<T> : ICloudTable<T> where T : TableData { MobileServiceClient client; IMobileServiceTable<T> table; public AzureCloudTable(MobileServiceClient client) { this.client = client; this.table = client.GetTable<T>(); } #region ICloudTable implementation public async Task<T> CreateItemAsync(T item) { await table.InsertAsync(item); return item; } public async Task DeleteItemAsync(T item) { await table.DeleteAsync(item); } public async Task<ICollection<T>> ReadAllItemsAsync() { return await table.ToListAsync(); } public async Task<T> ReadItemAsync(string id) { return await table.LookupAsync(id); } public async Task<T> UpdateItemAsync(T item) { await table.UpdateAsync(item); return item; } #endregion } } It's important to note here that the Azure Mobile Apps Client SDK does a lot of the work for us. In fact, we are just wrapping the basic interface here. This won't normally be the case, but you can see that the majority of the code for dealing with the remote server is done for us. Tip You can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as: public async Task DeleteItemAsync(T item) => await table.DeleteAsync(item); You may see this sort of short hand in samples. We also need to create the model that we will use for the data. This should look very similar to the model on the server - including having the same name and fields. In this case, it's Models\\TodoItem.cs : using TaskList.Abstractions namespace TaskList.Models { public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } } } We have a final piece of code to write before we move on to the views, but it's an important piece. The ICloudService must be a singleton in the client. We will add authentication and offline sync capabilities in future versions of this code. The singleton becomes critical when using those features. For right now, it's good practice and saves on memory if you only have one copy of the ICloudService in your mobile client. Since there is only one copy of the App.cs in any given app, I can place it there. Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this. Here is the App.cs : using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { CloudService = new AzureCloudService(); MainPage = new NavigationPage(new Pages.EntryPage()); } // There are lifecycle methods here... } } We haven't written Pages.EntryPage yet, but that's coming. The original App.cs class file had several methods for handling lifecycle events like starting, suspending or resuming the app. I did not touch those methods for this example. Building the UI for the App \u00b6 Earlier, I showed the mockup for my UI. It included three pages - an entry page, a list page and a detail page. These pages have three elements - a XAML definition file, a (simple) code-behind file and a view model. Info This book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML. If you wish to have that sort of introduction, then I recommend reading the excellent book by Charles Petzold: Creating Mobile Apps with Xamarin.Forms . I tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications. It's a nice clean pattern and is well understood and documented. In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements. In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed. To do this, we will use a BaseViewModel class that implements the base functionality for each view. Aside from the INotifyPropertyChanged interface, there are some common properties we need for each page. Each page needs a title, for example, and each page needs an indicator of network activity. These can be placed in the Abstractions\\BaseViewModel.cs class: using System; using System.Collections.Generic; using System.ComponentModel; namespace TaskList.Abstractions { public class BaseViewModel : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged; string _propTitle = string.Empty; bool _propIsBusy; public string Title { get { return _propTitle; } set { SetProperty(ref _propTitle, value, \"Title\"); } } public bool IsBusy { get { return _propIsBusy; } set { SetProperty(ref _propIsBusy, value, \"IsBusy\"); } } protected void SetProperty<T>(ref T store, T value, string propName, Action onChanged = null) { if (EqualityComparer<T>.Default.Equals(store, value)) return; store = value; if (onChanged != null) onChanged(); OnPropertyChanged(propName); } public void OnPropertyChanged(string propName) { if (PropertyChanged == null) return; PropertyChanged(this, new PropertyChangedEventArgs(propName)); } } } This is a fairly common INotifyPropertyChanged interface implementation pattern. Each property that we want to expose is a standard property, but the set operation is replaced by the SetProperty() call. The SetProperty() call deals with the notification; calling the event emitter if the property has changed value. We only need two properties on the BaseViewModel : the title and the network indicator. I tend to write my apps in two stages. I concentrate on the functionality of the app in the first stage. There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking. The page is all about the functionality of the various interactions. Once I have the functionality working, I work on the styling of the page. We won't be doing any styling work in the demonstration apps that we write during the course of this book. The EntryPage has just one thing to do. It provides a button that enters the app. When we cover authentication later on, we'll use this to log in to the backend. If you are looking at the perfect app, this is a great place to put the introductory screen. Creating a XAML file is relatively simple. First, create a Pages directory to hold the pages of our application. Then right-Click the Pages directory in the solution explorer and choose Add -> New Item... . In the Add New Item dialog, pick Visual C# -> Cross-Platform -> Forms Blank Content Page Xaml . Name the new page EntryPage.xaml . This will create two files - EntryPage.xaml and EntryPage.xaml.cs . Let's center a button on the page and wire it up with a command. Here is the Pages\\EntryPage.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.EntryPage\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Vertical\" VerticalOptions=\"Center\"> <Button BackgroundColor=\"Teal\" BorderRadius=\"10\" Command=\"{Binding LoginCommand}\" Text=\"Login\" TextColor=\"White\" /> </StackLayout> </ContentPage.Content> </ContentPage> There are a couple of interesting things to note here. The StackLayout element is our layout element. It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are. The only contents are a button. There are two bindings. These are bound from the view-model. We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command. When the button is tapped, the login command will be run. We'll get onto that in the view-model later. The other part of the XAML is the code-behind file. Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial: using TodoList.ViewModels; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TodoList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class EntryPage : ContentPage { public EntryPage () { InitializeComponent (); BindingContext = new EntryPageViewModel(); } } } This is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM. We initialize the UI, then bind all the bindings to a new instantiation of the view model. Talking of which, the view-model needs just to handle the login click. Note that the location or namespace is TaskList.ViewModels . I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together. However, just about all the sample code that I see has the view-models in a separate namespace. Which one is correct? I'll go with copying the samples for now. Here is the code for ViewModels\\EntryPageViewModel.cs : using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using Xamarin.Forms; namespace TaskList.ViewModels { public class EntryPageViewModel : BaseViewModel { public EntryPageViewModel() { Title = \"Task List\"; } Command loginCmd; public Command LoginCommand => loginCmd ?? (loginCmd = new Command(async () => await ExecuteLoginCommand().ConfigureAwait(false))); async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[Login] Error = {ex.Message}\"); } finally { IsBusy = false; } } } } This is a fairly simple view-model but there are some patterns here that are worth explaining. Firstly, note the way we create the LoginCommand property. This is the property that is bound to the Command parameter in the Button of our view. This recipe is the method of invoking a UI action asynchronously. It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity. The second is the pattern for the ExecuteLoginCommand method. Firstly, I ensure nothing else is happening by checking the IsBusy flag. If nothing is happening, I set the IsBusy flag. Then I do what I need to do in a try/catch block. If an exception is thrown, I deal with it. Most of the time this involves displaying an error condition. There are several cross-platform dialog packages to choose from or you can roll your own. That is not covered here. We just write a debug log statement so we can see the result in the debug log. Once everything is done, we clear the IsBusy flag. The only thing we are doing now is swapping out our main page for a new main page. This is where we will attach authentication later on. The next page is the Task List page, which is in Pages\\TaskList.xaml : <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskList\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout> <ListView BackgroundColor=\"#7F7F7F\" CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"True\" IsRefreshing=\"{Binding IsBusy, Mode=OneWay}\" ItemsSource=\"{Binding Items}\" RefreshCommand=\"{Binding RefreshCommand}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout HorizontalOptions=\"FillAndExpand\" Orientation=\"Horizontal\" Padding=\"10\" VerticalOptions=\"CenterAndExpand\"> <Label HorizontalOptions=\"FillAndExpand\" Text=\"{Binding Text}\" TextColor=\"#272832\" /> <Switch IsToggled=\"{Binding Complete, Mode=OneWay}\" /> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"Teal\" Command=\"{Binding AddNewItemCommand}\" Text=\"Add New Item\" TextColor=\"White\" /> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> Note that some bindings here are one-way. This means that the value in the view-model drives the value in the UI. There is nothing within the UI that you can do to alter the state of the underlying property. Some bindings are two-way. Doing something in the UI (for example, toggling the switch) alters the underlying property. This view is a little more complex. It can be split into two parts - the list at the top of the page and the button area at the bottom of the page. The list area uses a template to help with the display of each item. Note that the ListView object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand. It also has an indicator that I have wired up to the IsBusy indicator. Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does. The code behind in Pages\\TaskList.xaml.cs : using TodoList.ViewModels; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TodoList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class TaskList : ContentPage { public TaskList () { InitializeComponent (); BindingContext = new TaskListViewModel(); } } } There is a view-model that goes along with the view (in ViewModels\\TaskListViewModel.cs ): using System; using System.Collections.ObjectModel; using System.Collections.Specialized; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskListViewModel : BaseViewModel { public TaskListViewModel() { Title = \"Task List\"; RefreshList(); } ObservableCollection<TodoItem> items = new ObservableCollection<TodoItem>(); public ObservableCollection<TodoItem> Items { get { return items; } set { SetProperty(ref items, value, \"Items\"); } } TodoItem selectedItem; public TodoItem SelectedItem { get { return selectedItem; } set { SetProperty(ref selectedItem, value, \"SelectedItem\"); if (selectedItem != null) { Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem)); SelectedItem = null; } } } Command refreshCmd; public Command RefreshCommand => refreshCmd ?? (refreshCmd = new Command(async () => await ExecuteRefreshCommand())); async Task ExecuteRefreshCommand() { if (IsBusy) return; IsBusy = true; try { var table = App.CloudService.GetTable<TodoItem>(); var list = await table.ReadAllItemsAsync(); Items.Clear(); foreach (var item in list) Items.Add(item); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error loading items: {ex.Message}\"); } finally { IsBusy = false; } } Command addNewCmd; public Command AddNewItemCommand => addNewCmd ?? (addNewCmd = new Command(async () => await ExecuteAddNewItemCommand())); async Task ExecuteAddNewItemCommand() { if (IsBusy) return; IsBusy = true; try { await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail()); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error in AddNewItem: {ex.Message}\"); } finally { IsBusy = false; } } async Task RefreshList() { await ExecuteRefreshCommand(); MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); } } } This is a combination of the patterns we have seen earlier. The Add New Item and Refresh commands should be fairly normal patterns now. We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the SelectedItem property through a two-way binding) and when the user clicks on the Add New Item button. When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed. It is fairly common to use an ObservableCollection or another class that uses the ICollectionChanged event handler for the list storage. Doing so allows the UI to react to changes in the items. Note the use of the ICloudTable interface here. We are using the ReadAllItemsAsync() method to get a list of items, then we copy the items we received into the ObservableCollection`. Finally, there is the TaskDetail page. This is defined in the Pages\\TaskDetail.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskDetail\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout Padding=\"10\" Spacing=\"10\"> <Label Text=\"What should I be doing?\"/> <Entry Text=\"{Binding Item.Text}\"/> <Label Text=\"Completed?\"/> <Switch IsToggled=\"{Binding Item.Complete}\"/> <StackLayout VerticalOptions=\"CenterAndExpand\"/> <StackLayout Orientation=\"Vertical\" VerticalOptions=\"End\"> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"#A6E55E\" Command=\"{Binding SaveCommand}\" Text=\"Save\" TextColor=\"White\"/> <Button BackgroundColor=\"Red\" Command=\"{Binding DeleteCommand}\" Text=\"Delete\" TextColor=\"White\"/> </StackLayout> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> This page is a simple form with just two buttons that need to have commands wired up. However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture. As a result of this, we need to handle the passing of the item to be edited. This is done in the Pages\\TaskDetail.xaml.cs code-behind file: using TodoList.Models; using TodoList.ViewModels; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TodoList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class TaskDetail : ContentPage { public TaskDetail (TodoItem item = null) { InitializeComponent (); BindingContext = new TaskDetailViewModel(item); } } } The item that is passed in from the TaskList page is used to create a specific view-model for that item. The view-model is similarly configured to use that item: using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskDetailViewModel : BaseViewModel { ICloudTable<TodoItem> table = App.CloudService.GetTable<TodoItem>(); public TaskDetailViewModel(TodoItem item = null) { if (item != null) { Item = item; Title = item.Text; } else { Item = new TodoItem { Text = \"New Item\", Complete = false }; Title = \"New Item\"; } } public TodoItem Item { get; set; } Command cmdSave; public Command SaveCommand => cmdSave ?? (cmdSave = new Command(async () => await ExecuteSaveCommand())); async Task ExecuteSaveCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id == null) { await table.CreateItemAsync(Item); } else { await table.UpdateItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } Command cmdDelete; public Command DeleteCommand => cmdDelete ?? (cmdDelete = new Command(async () => await ExecuteDeleteCommand())); async Task ExecuteDeleteCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id != null) { await table.DeleteItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } } } The save command uses the ICloudTable interface again - this time doing either CreateItemAsync() or UpdateItemAsync() to create or update the item. The delete command, as you would expect, deletes the item with the DeleteItemAsync() method. The final thing to note from our views is that I am using the MessagingCenter to communicate between the TaskDetail and TaskList views. If I change the item in the TaskDetail list, then I want to update the list in the TaskList view. Note that all the code we have added to the solution thus far is in the common TaskList project. Nothing is required for this simple example in a platform specific project. That isn't normal, as we shall see in later chapters. Building the Client for Universal Windows \u00b6 I tend to start by building the Universal Windows mobile client. I'm using Visual Studio, after all, and I don't need to use any emulator. To build the clients: Right-Click the TaskList.UWP (Universal Windows) project, then select Set as StartUp Project . Right-Click the TaskList.UWP (Universal Windows) project again, then select Build . Once the build is complete, Right-Click the TaskList.UWP (Universal Windows) project again, then select Deploy . Click the Local Machine button in your command bar to run the application. Ignore the warning APPX0108 produced during the build. It warns that your certificate has expired (you don't have one yet). You can still run the application because you have turned Developer mode on in the Windows Settings. Here are the three screen screens we generated on Windows: There are some problems with the UWP version. Most notably, the \"pull-to-refresh\" gesture does not exist, so we will need to set up an alternate gesture. This could be as easy as adding a refresh button right next to the Add New Item button. In addition, there is no indication of network activity - this manifests as a significant delay between the TaskList page appearing and the data appearing in the list. Aside from this, I did do some styling work to ensure that the final version looked like my mock-ups (with the exception of the UI form of the switch, which is platform dependent). If you want to see what I did to correct this, check out the final version of the Chapter 1 sample on GitHub. Building for On-Premise If you want to run your backend using on-premise resources (for example, Azure Stack or a local IIS service), your UWP application will need the Private Networks capability. You can add this in the Package.appxmanifest file. Choose the Capabilities tab and add the required capability. If you need to build the project, ensure you redeploy the project after building. It's a step that is easy to miss and can cause some consternation as you change the code and it doesn't seem to have an effect on the application. To aid you in this: Select Build -> Configuration Manager... Set the Active solution platform to Any CPU . Uncheck all the boxes under Build and Deploy . Check the boxes for TodoList.UWP under to Build and Deploy . Click Close . When you click the Local Machine button to start the application, Visual Studio will automatically build and deploy your app. Building the Client for Android \u00b6 Prior to building the Android version of the application, we need to make two additional changes. Go to your Android project and open the MainActivity.cs file. In the OnCreate method we need to add an initalizer for our Mobile Apps SDK: protected override void OnCreate(Bundle bundle) { TabLayoutResource = Resource.Layout.Tabbar; ToolbarResource = Resource.Layout.Toolbar; base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); LoadApplication(new App()); } We also need to set the minimum API version. Azure Mobile Apps only works with API level 19 or above and the provided template set the minimum SDK version to API level 15. Open the Properties\\AndroidManifest.xml file in the TaskList.Android project and update the uses-sdk element: <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"> <uses-sdk android:minSdkVersion=\"19\" /> <application android:label=\"TodoList.Android\"></application> </manifest> Once that is done: Right-Click the TaskList.Droid project, then select Set as StartUp Project . Right-Click the TaskList.Droid project again, then select Build . Updating Xamarin.Android Support Packages If you use NuGet to update the referenced packages, ensure all the Xamarin Android support packages are the same version. Your build will fail if this is not the case. You may see build failures for a variety of reasons. Generally, a search on the Xamarin Forums will turn up the root cause of the failure. Xamarin Android builds seem to have more errors than either Windows or iOS, but they are easily correctable issues with the environment. In Visual Studio 2017, we use the Google Emulator for Android to test our Android application. Four devices will be defined for you by default: Select the VisualStudio_android-23_x86_phone emulator. Disable Hyper-V before using the Google Emulator The Google Android Emulator is incompatible with Hyper-V. If you see deployment errors, check the output window. You may see a message asking you to disable Hyper-V. Open up a PowerShell prompt with \"Run as Administrator\" and type bcdedit /set hypervisorlaunchtype off , then restart your computer. When the computer is restarted, Hyper-V will not be running. If everything is working, you should see the Google Android Emulator display your mobile client: Note that the task list view is a \"light\" style and the rest of the app is a \"dark\" style. This is because the default styling on an Android device is dark. We are using the default styling on two of the pages and specifying colors on the list page. Fortunately, Xamarin Forms allows for platform-specific styling . The final sample has platform-specific styling for the list page. Building the Client for iOS \u00b6 Finally, we get to the iOS platform. You will need to ensure your Mac is turned on and accessible via ssh, that it has XCode installed (and you have run XCode once so that you can accept the license agreement), and it has Visual Studio for Mac installed. Use Visual Studio Mobile Center for Mac Builds If you don't have a Mac handy, you can use Visual Studio Mobile Center to build your iOS version. You will need a signing certificate for this as you will obviously not have access to the iOS Simulator that is available on a Mac. When you created the projects, you were asked to link Visual Studio to your mac. This linkage is used for building the project. In essence, the entire project is sent to the Mac and the build tools that are supplied with Visual Studio for Mac are used to build the project. Right-click the TaskList.iOS project and select Set as StartUp Project . Right-click the TaskList.iOS project and select Build . You knew it was not going to be that easy, right? Here are the errors that I received when building for the first time: The error about No valid iOS code signing keys found in keychain is because we selected (by default) a package to deploy onto an iPhone and have not signed up for an Apple Developer Account and linked it to our Mac development environment. To fix this, select the \"iPhoneSimulator\" Configuration: Once you have selected the iPhoneSimulator configuration, re-build the mobile application. It should work this time. Run the simulator using the normal Visual Studio method. Visual Studio 2017 will connect to the Mac to run the simulator but display the simulator on your PC. Before long, you should see the following: You need a platform initialization call on most platforms for offline sync, and you always need a platform initializer for iOS. In TaskList.Droid, this should be added in MainActivity.cs and in TaskList.iOS, it's AppDelegate.cs . In each of these files, there is a line that initializes the Xamarin Forms system. // Android Version global::Xamarin.Forms.Forms.Init(this, bundle); // iOS Version global::Xamarin.Forms.Forms.Init(); Immediately before this line, you should add the initializer for Azure Mobile Apps. It's important that the Azure Mobile Apps subsystem is initialized prior to Xamarin Forms being initialized and any UI code being called. Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); This initializer is not required on the UWP project. Once this is done and you have re-built the project, you can see the fruits of your labor. The final product screens look like this: Some Final Thoughts \u00b6 If you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations. There are a lot of places where things can go wrong. You are really integrating the build systems across Visual Studio, Android, Xamarin, and XCode. Fortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work. If you would rather develop code on your mac, the next sectiom is for you - it gets into the nitty gritty of developing the exact same app, but using Visual Studio for Mac instead. The following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order. If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.","title":"Your First App - PC Edition"},{"location":"chapter1/firstapp_pc/#your-first-mobile-app","text":"There is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects. First, wouldn't it be nice to write some code and get something working? Microsoft Azure has a great first-steps tutorial that takes you via the quickest possible route from creating a mobile backend to having a functional backend. I would like to take things a little slower so that we can understand what is going on while we are doing the process. We will have practically the same application at the end. The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly. If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up. This chapter covers the set up required for a Windows PC. If you wish to develop your applications on a Mac, then skip to the next section . The application we are going to build together is a simple task list. The mobile client will have three screens - an entry screen, a task list and a task details page. I have mocked these pages up using MockingBot . Tip Mocking your screens before you start coding is a great habit to get into. There are some great tools available including free tools like MockingBot . Doing mockups before you start coding is a good way to prevent wasted time later on. Tip If you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one. Other platforms will need it though, so it's best to start with the least common denominator. It's the same reason I add a refresh button even though it's only valid on Windows Phone! My ideas for this app include: Tapping on a task title in the task list will bring up the details page. Toggling the completed link in the task list will set the completed flag. Tapping the spinner will initiate a network refresh. Now that we have our client screens planned out, we can move onto the thinking about the mobile backend.","title":"Your First Mobile App"},{"location":"chapter1/firstapp_pc/#the-mobile-backend","text":"The mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python). Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service.","title":"The Mobile Backend"},{"location":"chapter1/firstapp_pc/#creating-a-simple-azure-mobile-apps-backend","text":"Microsoft Azure has included a comprehensive starter kit template in the Azure SDK. To get started: Fire up Visual Studio. Add a new project with File -> New -> Project... In the New Project window: Open up Templates -> Visual C# -> Web and select ASP.NET Web Application (.NET Framework) . Enter Backend for the Name and Chapter1 for the Solution name. Select .NET Framework 4.6 in the framework dropdown at the top. Pick a suitable directory for the Location field. Click OK. In the New ASP.NET Web Application window: Click Azure Mobile App . Do NOT check \"Host in the cloud\" or any other checkboxes. Click OK. At this point, Visual Studio will create your backend project. There are a few files of which you should take note. The Mobile Apps SDK is initialized within App_Start\\Startup.MobileApp.cs (with the call to the configuration routine happening within Startup.cs ). The default startup routine is reasonable but it hides what it is doing behind extension methods. This technique is fairly common in ASP.NET programs. Let's expand the configuration routine to only include what we need: public static void ConfigureMobileApp(IAppBuilder app) { var config = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(config); Database.SetInitializer(new MobileServiceInitializer()); app.UseWebApi(config); } The minimal version of the mobile backend initialization is actually shorter than the original. It also only includes a data access layer. Other services like authentication, storage and push notifications are not configured. There is another method in the App_Start\\Startup.MobileApp.cs file for seeding data into the database for us. We can leave that alone for now, but remember it is there in case you need to seed data into a new database for your own backend. Info We refer to \"seeding data\" into a database. This means that we are going to introduce some data into the database so that we aren't operating on an empty database. The data will be there when we query the database later on. The next important file is the DbContext - located in Models\\MobileServiceContext.cs . Azure Mobile Apps is heavily dependent on Entity Framework v6.x and the DbContext is a central part of that library. Fortunately, we don't need to do anything to this file right now. Finally, we get to the meat of the backend. The whole point of this demonstration is to project a single database table - the TodoItem table - into the mobile realm with the aid of an opinionated OData v3 feed. To that end, we need three items: A DbSet<> within the DbContext A Data Transfer Object (or DTO) A Table Controller When we create the project, a sample of each one of these for the TodoItem table is added for us. You can see the DbSet<> in the Models\\MobileServiceContext.cs file, for example. Let's take a look at the DTO and Table Controller for this example table as well. The DTO for the TodoItem table is located within the DataObjects directory: using Microsoft.Azure.Mobile.Server; namespace Backend.DataObjects { public class TodoItem : EntityData { public string Text { get; set; } public bool Complete { get; set; } } } Note that the model uses EntityData as a base class. The EntityData class adds five additional properties to the class - we'll discuss those in more details during the Data Access and Offline Sync chapter. Finally, let's look at the table controller for the example TodoItem table. This is located in Controllers\\TodoItemController.cs : using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) => DeleteAsync(id); } } The TableController is the central processing for the database access layer. It handles all the OData capabilities for us and exposes these as REST endpoints within our WebAPI. This means that the actual code for this controller is tiny - just 12 lines of code. Info OData is a specification for accessing table data on the Internet. It provides a mechanism for querying and manipulating data within a table. Entity Framework is a common data access layer for ASP.NET applications. We can build the project at this point. If Visual Studio hasn't done so already, the missing NuGet packages for Azure Mobile Apps will be downloaded. There should not be any errors. If there are, check the typing for any changes you made.","title":"Creating a Simple Azure Mobile Apps Backend"},{"location":"chapter1/firstapp_pc/#building-an-azure-app-service-for-mobile-apps","text":"The next step in the process is to build the resources on Azure that will run your mobile backend. Start by logging into the Azure Portal , then follow these instructions: Click the big + New button in the top-left corner. Click Web + Mobile , then Mobile App . Enter a unique name in the App name box. Tip Since the name doesn't matter and it has to be unique, you can use a GUID generator to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID (example: chapter1-GUID) to aid in discovery later on. Generally, the first four digits of a GUID are enough to identify individual resources. If you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the Subscription drop-down. Select Create new under resource group and enter a name for this mobile application. Resource Groups Resource groups are great for grouping all the resources associated with a mobile application together. During development, it means you can delete all the resources in one operation. For production, it means you can see how much the service is costing you and how the resources are being used. Finally, select or create a new App Service Plan . App Service Plan The App Service Plan is the thing that actually bills you - not the web or mobile backend. You can run a number of web or mobile backends on the same App Service Plan. I tend to create a new App Service Plan for each mobile application. This is because the App Service Plan lives inside the Resource Group that you create. The process for creating an App Service Plan is straight forward. You have two decisions to make. The first decision is where is the service going to run. In a production environment, the correct choice is \"near your customers\". \"Close to the developers\" is a good choice during development. Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location. With 16 regions to choose from, you have a lot of choice. The second decision you have to make is what to run the service on; also known as the Pricing tier. If you Click View all , you will see you have lots of choices. F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota. That leaves Basic, Standard and Premium. Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks. Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example. Finally, there is a number after the plan. This tells you how big the virtual machine is that the plan is running on. The numbers differ by number of cores and memory. For our purposes, an F1 Free site is enough to run this small demonstration project. More complex development projects should use something in the Basic range of pricing plans. Production apps should be set up in Standard or Premium pricing plans. Once you have created your app service plan and saved it, Click Create . The creation of the service can take a couple of minutes. You can monitor the process of deployment by clicking on the Notifications icon. This is in the top bar on the right-hand side and looks like a Bell. Clicking on a specific notification will provide more information about the activity. Once you have created your app service, the App Service blade will open. We will also want a place to store our data. This role is taken on by a SQL Azure instance. We could link an existing database if we had one defined. However, we can also create a test database. Tip Creating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database. This option is not normally available through other SQL database creation flows. Before we can create a database, we need to create a logical server for the database. The SQL Server (the logical server) sets the region and the login credentials for all the contained databases: Click Resource groups in the left hand side menu. Click the resource group you created. Click Add at the top of the blade. Enter SQL Server into the search box, then press Enter. Click SQL Server (logical server) . Click Create . Enter the information required by the form: A server name (which must be unique in the world - this is a great place to use a GUID). A username and password for accessing the databases on the server. Select the existing resource group. Pick the same Location as you did for the App Service Plan. Click Create . Once the deployment has completed, you can move on to creating and linking a database. You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner. To create and link the database: Click Resource groups in the left hand side menu. Click the resource group you created. Click the App Service your created. Tip If you pinned your App Service to the dashboard, you can Click the pinned App Service instead. It will bring you to the same place. Click Data connections in the MOBILE menu. You can also search for Data connections in the left hand menu. Click Add . In the Type box, select SQL Database . Click the unconfigured SQL Database link: In the Database blade, select Create a new database . Enter a name for the database (like chapter1-db ). Click the Target server box and select the logical server you created earlier. Select a Pricing Tier, then click Apply . Click Select to close the SQL Database blade. Click the Connection string box. Enter the username and password you set up for the SQL logical server. Click OK . The username and password will be validated before proceeding. Click OK to close the Add data connection blade. This produces another deployment step that creates a SQL database with your settings and binds it to the App Service. Once complete, the connection MS_TableConnectionString will be listed in Data Connections blade.","title":"Building an Azure App Service for Mobile Apps"},{"location":"chapter1/firstapp_pc/#deploying-the-azure-mobile-apps-backend","text":"Deploying to Azure as a developer can be accomplished while entirely within Visual Studio: Right-Click the Backend project, then select Publish... . The following will be shown: If you have an earlier version of Visual Studio, a different screen will be shown. If Azure App Service is not listed, ensure you have the latest version of Azure SDK installed - at least v2.9. Click Microsoft Azure App Service . Click Select Existing , then click Publish . You may be prompted to enter your Azure credentials here. Enter the same information that you enter to access the Azure Portal. In the lower box, expand the resource group that you created and select the app service you created in the portal. Click OK . Visual Studio will open a browser pointing to the root of your Azure App Service. Add /tables/todoitem?ZUMO-API-VERSION=2.0.0 to the end of the URL. This will show the JSON contents of the table that was defined as a table controller in the backend. Info You will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code. ZUMO was the original code name within Microsoft for A ZU re MO bile.","title":"Deploying the Azure Mobile Apps Backend"},{"location":"chapter1/firstapp_pc/#building-the-mobile-client","text":"Info When you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows Now that the mobile backend is created and deployed, we can move onto the client side of things. Right-Click the solution and select Add -> New Project... . This will bring up the familiar New Project dialog. Select Visual C# -> Cross-Platform -> Cross Platform App (Xamarin.Forms or Native) . Give the project a name, then Click OK . In the New Cross Platform App window, select Blank App , and use Xamarin.Forms as the UI technology, and a Shared Project for the code sharing strategy. Project creation will take longer than you expect, but there is a lot going on. If you have never created a mobile or UWP project before, you will be prompted to turn on Windows 10 Developer Mode: Developer mode in Windows 10 allows you to run unsigned binaries for development purposes and to turn on debugging so that you can step through your UWP programs within Visual Studio. Visual Studio may also just bring up the appropriate Settings page where you can turn on Developer mode. We will also get asked to choose what version of the Universal Windows platform we want to target: Version 10240 was the first version of Windows 10 that was released to the general public, so that's a good minimum version to pick. In general, the defaults for the Universal Windows Platform choice are good enough. Xamarin allows us to build iOS applications directly from Visual Studio. For this to work, we must have access to a Mac. This could be anything from a MacBook Air/Pro, to a Mac Mini in a drawer or closet in the office, or maybe even a Mac in the cloud . The Xamarin tools use SSH to connect to the Mac, which must be configured to build iOS apps from Visual Studio . Tip If you don't have a Mac and are not interested in building iOS applications, don't give up now! You can cancel through the Mac specific project setup and continue with building a great Android and Universal Windows app. You can also use Visual Studio Mobile Center to build an iOS project. You can delete the iOS specific project after it has been created. When prompted about the Xamarin Mac Agent, Click OK to get the list of local mac agents: Highlight your mac (in case there are multiples), then Click Connect... . If your mac is not listed or you are using a Mac in the cloud, then you can always enter the IP address for your mac. Tip For more troubleshooting tips, visit The Xamarin Troubleshooting Site . You will be prompted for your username and password: Just enter the (real) username and password for your account on your mac and click on Login . Tip Apple tries very hard to hide the real username of your account from you. The easiest way to find your mac username is to open up the Finder. The name next to your home icon is the name of your account. If the connection is successful, you will see a green icon in the Xamarin Visual Studio toolbar area. It may take a minute or two to connect and verify that the mac can be used. Once the project is created, you will see that four new projects have been created: a common library which you named plus one project for each platform that has been chosen. Since we chose a project with three platforms, we get four projects: Most of our work will happen in the common library. However, we can introduce platform-specific code at any point. The platform-specific code is stored in the platform-specific project. There is one final item we must do before we leave the set up of the project. There are a number of platform upgrades that inevitably have to happen. The Xamarin Platform is updated much more often than the Visual Studio plugin - the updates are released via NuGet: the standard method of distributing libraries for .NET applications. Warn Although it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services. There are many differences between the wire protocols of the two products. You can install the NuGet packages by right-clicking on the solution and selecting Manage NuGet Packages for Solution... . You can generally select all the updates. However, do NOT update the Jwt package (System.IdentityModel.Tokens.Jwt) as this will break the server project. You can update the System.IdentityModel.Tokens.Jwt to the latest v4.x release. Do NOT install a v5.x release. Info Android generally has more updates than the other platforms. Ensure that you update the main Xamarin.Forms package and then refresh the update list. This will ensure the right list of packages is updated. You should also install the Microsoft.Azure.Mobile.Client library in all the client projects.","title":"Building The Mobile Client"},{"location":"chapter1/firstapp_pc/#building-the-common-library","text":"There are two parts that we must concentrate on within the common library. The first is the connection to Azure Mobile Apps and the second is in the pages that the user interacts with. In both cases, there are best practices to observe.","title":"Building the Common Library"},{"location":"chapter1/firstapp_pc/#building-an-azure-mobile-apps-connection","text":"We will rely on interfaces for defining the shape for the class for any service that we interact with. This is really not important in small projects like this one. This technique allows us to mock the backend service, as we shall see later on. Mocking the backend service is a great technique to rapidly iterate on the front end mobile client without getting tied into what the backend is doing. Let's start with the cloud service - this is defined in Abstractions\\ICloudService.cs . It is used for initializing the connection and getting a table definition: namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; } } The ICloudTable generic interface represents a CRUD interface into a table and is defined in Abstractions\\ICloudTable.cs : using System.Collections.Generic; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); } } The ICloudTable<T> interface defines the normal CRUD operations: Create, Read, Update and Delete. However, it does so asynchronously. We are dealing with network operations in general so it is easy for those operations to tie up the UI thread for an appreciable amount of time. Making them async provides the ability to respond to other events. I also provide a ReadAllItemsAsync() method that returns a collection of all the items. There are some fields that every single record within an Azure Mobile Apps table provides. These fields are required for offline sync capabilities like incremental sync and conflict resolution. The fields are provided by an abstract base class on the client called TableData : using System; namespace TaskList.Abstractions { public abstract class TableData { public string Id { get; set; } public DateTimeOffset? UpdatedAt { get; set; } public DateTimeOffset? CreatedAt { get; set; } public byte[] Version { get; set; } } } As we will learn when we deal with table data , these fields need to be defined with the same name and semantics as on the server. Our model on the server was sub-classed from EntityData and the EntityData class on the server defines these fields. It's tempting to call the client version of the class the same as the server version. If we did that, the models on both the client and server would look the same. However, I find that this confuses the issue. The models on the client and server are not the same. They are missing the Deleted flag and they do not contain any relationship information on the client. I choose to deliberately call the base class something else on the client to avoid this confusion. We will be adding to these interfaces in future chapters as we add more capabilities to the application. The concrete implementations of these classes are similarly easily defined. The Azure Mobile Apps Client SDK does most of the work for us. Here is the concrete implementation of the ICloudService (in Services\\AzureCloudService.cs ): using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(\"https://my-backend.azurewebsites.net\"); } public ICloudTable<T> GetTable<T>() where T : TableData { return new AzureCloudTable<T>(client); } } } Ensure you use HTTPS If you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint. You must provide the https version of the endpoint when using App Service. The http endpoint redirects to https and the standard HttpClient does not handle redirects. The Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published. Just swap out the name of your mobile backend and the rest is silently dealt with. Warn The name Microsoft.WindowsAzure.MobileServices is a hold-over from the old Azure Mobile Services code-base. Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps. We also need a concrete implementation of the ICloudTable<T> interface (in Services\\AzureCloudTable.cs ): using System.Collections.Generic; using System.Collections.ObjectModel; using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudTable<T> : ICloudTable<T> where T : TableData { MobileServiceClient client; IMobileServiceTable<T> table; public AzureCloudTable(MobileServiceClient client) { this.client = client; this.table = client.GetTable<T>(); } #region ICloudTable implementation public async Task<T> CreateItemAsync(T item) { await table.InsertAsync(item); return item; } public async Task DeleteItemAsync(T item) { await table.DeleteAsync(item); } public async Task<ICollection<T>> ReadAllItemsAsync() { return await table.ToListAsync(); } public async Task<T> ReadItemAsync(string id) { return await table.LookupAsync(id); } public async Task<T> UpdateItemAsync(T item) { await table.UpdateAsync(item); return item; } #endregion } } It's important to note here that the Azure Mobile Apps Client SDK does a lot of the work for us. In fact, we are just wrapping the basic interface here. This won't normally be the case, but you can see that the majority of the code for dealing with the remote server is done for us. Tip You can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as: public async Task DeleteItemAsync(T item) => await table.DeleteAsync(item); You may see this sort of short hand in samples. We also need to create the model that we will use for the data. This should look very similar to the model on the server - including having the same name and fields. In this case, it's Models\\TodoItem.cs : using TaskList.Abstractions namespace TaskList.Models { public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } } } We have a final piece of code to write before we move on to the views, but it's an important piece. The ICloudService must be a singleton in the client. We will add authentication and offline sync capabilities in future versions of this code. The singleton becomes critical when using those features. For right now, it's good practice and saves on memory if you only have one copy of the ICloudService in your mobile client. Since there is only one copy of the App.cs in any given app, I can place it there. Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this. Here is the App.cs : using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { CloudService = new AzureCloudService(); MainPage = new NavigationPage(new Pages.EntryPage()); } // There are lifecycle methods here... } } We haven't written Pages.EntryPage yet, but that's coming. The original App.cs class file had several methods for handling lifecycle events like starting, suspending or resuming the app. I did not touch those methods for this example.","title":"Building an Azure Mobile Apps Connection"},{"location":"chapter1/firstapp_pc/#building-the-ui-for-the-app","text":"Earlier, I showed the mockup for my UI. It included three pages - an entry page, a list page and a detail page. These pages have three elements - a XAML definition file, a (simple) code-behind file and a view model. Info This book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML. If you wish to have that sort of introduction, then I recommend reading the excellent book by Charles Petzold: Creating Mobile Apps with Xamarin.Forms . I tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications. It's a nice clean pattern and is well understood and documented. In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements. In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed. To do this, we will use a BaseViewModel class that implements the base functionality for each view. Aside from the INotifyPropertyChanged interface, there are some common properties we need for each page. Each page needs a title, for example, and each page needs an indicator of network activity. These can be placed in the Abstractions\\BaseViewModel.cs class: using System; using System.Collections.Generic; using System.ComponentModel; namespace TaskList.Abstractions { public class BaseViewModel : INotifyPropertyChanged { public event PropertyChangedEventHandler PropertyChanged; string _propTitle = string.Empty; bool _propIsBusy; public string Title { get { return _propTitle; } set { SetProperty(ref _propTitle, value, \"Title\"); } } public bool IsBusy { get { return _propIsBusy; } set { SetProperty(ref _propIsBusy, value, \"IsBusy\"); } } protected void SetProperty<T>(ref T store, T value, string propName, Action onChanged = null) { if (EqualityComparer<T>.Default.Equals(store, value)) return; store = value; if (onChanged != null) onChanged(); OnPropertyChanged(propName); } public void OnPropertyChanged(string propName) { if (PropertyChanged == null) return; PropertyChanged(this, new PropertyChangedEventArgs(propName)); } } } This is a fairly common INotifyPropertyChanged interface implementation pattern. Each property that we want to expose is a standard property, but the set operation is replaced by the SetProperty() call. The SetProperty() call deals with the notification; calling the event emitter if the property has changed value. We only need two properties on the BaseViewModel : the title and the network indicator. I tend to write my apps in two stages. I concentrate on the functionality of the app in the first stage. There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking. The page is all about the functionality of the various interactions. Once I have the functionality working, I work on the styling of the page. We won't be doing any styling work in the demonstration apps that we write during the course of this book. The EntryPage has just one thing to do. It provides a button that enters the app. When we cover authentication later on, we'll use this to log in to the backend. If you are looking at the perfect app, this is a great place to put the introductory screen. Creating a XAML file is relatively simple. First, create a Pages directory to hold the pages of our application. Then right-Click the Pages directory in the solution explorer and choose Add -> New Item... . In the Add New Item dialog, pick Visual C# -> Cross-Platform -> Forms Blank Content Page Xaml . Name the new page EntryPage.xaml . This will create two files - EntryPage.xaml and EntryPage.xaml.cs . Let's center a button on the page and wire it up with a command. Here is the Pages\\EntryPage.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.EntryPage\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Vertical\" VerticalOptions=\"Center\"> <Button BackgroundColor=\"Teal\" BorderRadius=\"10\" Command=\"{Binding LoginCommand}\" Text=\"Login\" TextColor=\"White\" /> </StackLayout> </ContentPage.Content> </ContentPage> There are a couple of interesting things to note here. The StackLayout element is our layout element. It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are. The only contents are a button. There are two bindings. These are bound from the view-model. We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command. When the button is tapped, the login command will be run. We'll get onto that in the view-model later. The other part of the XAML is the code-behind file. Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial: using TodoList.ViewModels; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TodoList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class EntryPage : ContentPage { public EntryPage () { InitializeComponent (); BindingContext = new EntryPageViewModel(); } } } This is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM. We initialize the UI, then bind all the bindings to a new instantiation of the view model. Talking of which, the view-model needs just to handle the login click. Note that the location or namespace is TaskList.ViewModels . I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together. However, just about all the sample code that I see has the view-models in a separate namespace. Which one is correct? I'll go with copying the samples for now. Here is the code for ViewModels\\EntryPageViewModel.cs : using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using Xamarin.Forms; namespace TaskList.ViewModels { public class EntryPageViewModel : BaseViewModel { public EntryPageViewModel() { Title = \"Task List\"; } Command loginCmd; public Command LoginCommand => loginCmd ?? (loginCmd = new Command(async () => await ExecuteLoginCommand().ConfigureAwait(false))); async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[Login] Error = {ex.Message}\"); } finally { IsBusy = false; } } } } This is a fairly simple view-model but there are some patterns here that are worth explaining. Firstly, note the way we create the LoginCommand property. This is the property that is bound to the Command parameter in the Button of our view. This recipe is the method of invoking a UI action asynchronously. It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity. The second is the pattern for the ExecuteLoginCommand method. Firstly, I ensure nothing else is happening by checking the IsBusy flag. If nothing is happening, I set the IsBusy flag. Then I do what I need to do in a try/catch block. If an exception is thrown, I deal with it. Most of the time this involves displaying an error condition. There are several cross-platform dialog packages to choose from or you can roll your own. That is not covered here. We just write a debug log statement so we can see the result in the debug log. Once everything is done, we clear the IsBusy flag. The only thing we are doing now is swapping out our main page for a new main page. This is where we will attach authentication later on. The next page is the Task List page, which is in Pages\\TaskList.xaml : <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskList\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout> <ListView BackgroundColor=\"#7F7F7F\" CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"True\" IsRefreshing=\"{Binding IsBusy, Mode=OneWay}\" ItemsSource=\"{Binding Items}\" RefreshCommand=\"{Binding RefreshCommand}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout HorizontalOptions=\"FillAndExpand\" Orientation=\"Horizontal\" Padding=\"10\" VerticalOptions=\"CenterAndExpand\"> <Label HorizontalOptions=\"FillAndExpand\" Text=\"{Binding Text}\" TextColor=\"#272832\" /> <Switch IsToggled=\"{Binding Complete, Mode=OneWay}\" /> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"Teal\" Command=\"{Binding AddNewItemCommand}\" Text=\"Add New Item\" TextColor=\"White\" /> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> Note that some bindings here are one-way. This means that the value in the view-model drives the value in the UI. There is nothing within the UI that you can do to alter the state of the underlying property. Some bindings are two-way. Doing something in the UI (for example, toggling the switch) alters the underlying property. This view is a little more complex. It can be split into two parts - the list at the top of the page and the button area at the bottom of the page. The list area uses a template to help with the display of each item. Note that the ListView object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand. It also has an indicator that I have wired up to the IsBusy indicator. Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does. The code behind in Pages\\TaskList.xaml.cs : using TodoList.ViewModels; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TodoList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class TaskList : ContentPage { public TaskList () { InitializeComponent (); BindingContext = new TaskListViewModel(); } } } There is a view-model that goes along with the view (in ViewModels\\TaskListViewModel.cs ): using System; using System.Collections.ObjectModel; using System.Collections.Specialized; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskListViewModel : BaseViewModel { public TaskListViewModel() { Title = \"Task List\"; RefreshList(); } ObservableCollection<TodoItem> items = new ObservableCollection<TodoItem>(); public ObservableCollection<TodoItem> Items { get { return items; } set { SetProperty(ref items, value, \"Items\"); } } TodoItem selectedItem; public TodoItem SelectedItem { get { return selectedItem; } set { SetProperty(ref selectedItem, value, \"SelectedItem\"); if (selectedItem != null) { Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem)); SelectedItem = null; } } } Command refreshCmd; public Command RefreshCommand => refreshCmd ?? (refreshCmd = new Command(async () => await ExecuteRefreshCommand())); async Task ExecuteRefreshCommand() { if (IsBusy) return; IsBusy = true; try { var table = App.CloudService.GetTable<TodoItem>(); var list = await table.ReadAllItemsAsync(); Items.Clear(); foreach (var item in list) Items.Add(item); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error loading items: {ex.Message}\"); } finally { IsBusy = false; } } Command addNewCmd; public Command AddNewItemCommand => addNewCmd ?? (addNewCmd = new Command(async () => await ExecuteAddNewItemCommand())); async Task ExecuteAddNewItemCommand() { if (IsBusy) return; IsBusy = true; try { await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail()); } catch (Exception ex) { Debug.WriteLine($\"[TaskList] Error in AddNewItem: {ex.Message}\"); } finally { IsBusy = false; } } async Task RefreshList() { await ExecuteRefreshCommand(); MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); } } } This is a combination of the patterns we have seen earlier. The Add New Item and Refresh commands should be fairly normal patterns now. We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the SelectedItem property through a two-way binding) and when the user clicks on the Add New Item button. When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed. It is fairly common to use an ObservableCollection or another class that uses the ICollectionChanged event handler for the list storage. Doing so allows the UI to react to changes in the items. Note the use of the ICloudTable interface here. We are using the ReadAllItemsAsync() method to get a list of items, then we copy the items we received into the ObservableCollection`. Finally, there is the TaskDetail page. This is defined in the Pages\\TaskDetail.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"TaskList.Pages.TaskDetail\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout Padding=\"10\" Spacing=\"10\"> <Label Text=\"What should I be doing?\"/> <Entry Text=\"{Binding Item.Text}\"/> <Label Text=\"Completed?\"/> <Switch IsToggled=\"{Binding Item.Complete}\"/> <StackLayout VerticalOptions=\"CenterAndExpand\"/> <StackLayout Orientation=\"Vertical\" VerticalOptions=\"End\"> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"#A6E55E\" Command=\"{Binding SaveCommand}\" Text=\"Save\" TextColor=\"White\"/> <Button BackgroundColor=\"Red\" Command=\"{Binding DeleteCommand}\" Text=\"Delete\" TextColor=\"White\"/> </StackLayout> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> This page is a simple form with just two buttons that need to have commands wired up. However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture. As a result of this, we need to handle the passing of the item to be edited. This is done in the Pages\\TaskDetail.xaml.cs code-behind file: using TodoList.Models; using TodoList.ViewModels; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TodoList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class TaskDetail : ContentPage { public TaskDetail (TodoItem item = null) { InitializeComponent (); BindingContext = new TaskDetailViewModel(item); } } } The item that is passed in from the TaskList page is used to create a specific view-model for that item. The view-model is similarly configured to use that item: using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class TaskDetailViewModel : BaseViewModel { ICloudTable<TodoItem> table = App.CloudService.GetTable<TodoItem>(); public TaskDetailViewModel(TodoItem item = null) { if (item != null) { Item = item; Title = item.Text; } else { Item = new TodoItem { Text = \"New Item\", Complete = false }; Title = \"New Item\"; } } public TodoItem Item { get; set; } Command cmdSave; public Command SaveCommand => cmdSave ?? (cmdSave = new Command(async () => await ExecuteSaveCommand())); async Task ExecuteSaveCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id == null) { await table.CreateItemAsync(Item); } else { await table.UpdateItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } Command cmdDelete; public Command DeleteCommand => cmdDelete ?? (cmdDelete = new Command(async () => await ExecuteDeleteCommand())); async Task ExecuteDeleteCommand() { if (IsBusy) return; IsBusy = true; try { if (Item.Id != null) { await table.DeleteItemAsync(Item); } MessagingCenter.Send<TaskDetailViewModel>(this, \"ItemsChanged\"); await Application.Current.MainPage.Navigation.PopAsync(); } catch (Exception ex) { Debug.WriteLine($\"[TaskDetail] Save error: {ex.Message}\"); } finally { IsBusy = false; } } } } The save command uses the ICloudTable interface again - this time doing either CreateItemAsync() or UpdateItemAsync() to create or update the item. The delete command, as you would expect, deletes the item with the DeleteItemAsync() method. The final thing to note from our views is that I am using the MessagingCenter to communicate between the TaskDetail and TaskList views. If I change the item in the TaskDetail list, then I want to update the list in the TaskList view. Note that all the code we have added to the solution thus far is in the common TaskList project. Nothing is required for this simple example in a platform specific project. That isn't normal, as we shall see in later chapters.","title":"Building the UI for the App"},{"location":"chapter1/firstapp_pc/#building-the-client-for-universal-windows","text":"I tend to start by building the Universal Windows mobile client. I'm using Visual Studio, after all, and I don't need to use any emulator. To build the clients: Right-Click the TaskList.UWP (Universal Windows) project, then select Set as StartUp Project . Right-Click the TaskList.UWP (Universal Windows) project again, then select Build . Once the build is complete, Right-Click the TaskList.UWP (Universal Windows) project again, then select Deploy . Click the Local Machine button in your command bar to run the application. Ignore the warning APPX0108 produced during the build. It warns that your certificate has expired (you don't have one yet). You can still run the application because you have turned Developer mode on in the Windows Settings. Here are the three screen screens we generated on Windows: There are some problems with the UWP version. Most notably, the \"pull-to-refresh\" gesture does not exist, so we will need to set up an alternate gesture. This could be as easy as adding a refresh button right next to the Add New Item button. In addition, there is no indication of network activity - this manifests as a significant delay between the TaskList page appearing and the data appearing in the list. Aside from this, I did do some styling work to ensure that the final version looked like my mock-ups (with the exception of the UI form of the switch, which is platform dependent). If you want to see what I did to correct this, check out the final version of the Chapter 1 sample on GitHub. Building for On-Premise If you want to run your backend using on-premise resources (for example, Azure Stack or a local IIS service), your UWP application will need the Private Networks capability. You can add this in the Package.appxmanifest file. Choose the Capabilities tab and add the required capability. If you need to build the project, ensure you redeploy the project after building. It's a step that is easy to miss and can cause some consternation as you change the code and it doesn't seem to have an effect on the application. To aid you in this: Select Build -> Configuration Manager... Set the Active solution platform to Any CPU . Uncheck all the boxes under Build and Deploy . Check the boxes for TodoList.UWP under to Build and Deploy . Click Close . When you click the Local Machine button to start the application, Visual Studio will automatically build and deploy your app.","title":"Building the Client for Universal Windows"},{"location":"chapter1/firstapp_pc/#building-the-client-for-android","text":"Prior to building the Android version of the application, we need to make two additional changes. Go to your Android project and open the MainActivity.cs file. In the OnCreate method we need to add an initalizer for our Mobile Apps SDK: protected override void OnCreate(Bundle bundle) { TabLayoutResource = Resource.Layout.Tabbar; ToolbarResource = Resource.Layout.Toolbar; base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); LoadApplication(new App()); } We also need to set the minimum API version. Azure Mobile Apps only works with API level 19 or above and the provided template set the minimum SDK version to API level 15. Open the Properties\\AndroidManifest.xml file in the TaskList.Android project and update the uses-sdk element: <?xml version=\"1.0\" encoding=\"utf-8\"?> <manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"> <uses-sdk android:minSdkVersion=\"19\" /> <application android:label=\"TodoList.Android\"></application> </manifest> Once that is done: Right-Click the TaskList.Droid project, then select Set as StartUp Project . Right-Click the TaskList.Droid project again, then select Build . Updating Xamarin.Android Support Packages If you use NuGet to update the referenced packages, ensure all the Xamarin Android support packages are the same version. Your build will fail if this is not the case. You may see build failures for a variety of reasons. Generally, a search on the Xamarin Forums will turn up the root cause of the failure. Xamarin Android builds seem to have more errors than either Windows or iOS, but they are easily correctable issues with the environment. In Visual Studio 2017, we use the Google Emulator for Android to test our Android application. Four devices will be defined for you by default: Select the VisualStudio_android-23_x86_phone emulator. Disable Hyper-V before using the Google Emulator The Google Android Emulator is incompatible with Hyper-V. If you see deployment errors, check the output window. You may see a message asking you to disable Hyper-V. Open up a PowerShell prompt with \"Run as Administrator\" and type bcdedit /set hypervisorlaunchtype off , then restart your computer. When the computer is restarted, Hyper-V will not be running. If everything is working, you should see the Google Android Emulator display your mobile client: Note that the task list view is a \"light\" style and the rest of the app is a \"dark\" style. This is because the default styling on an Android device is dark. We are using the default styling on two of the pages and specifying colors on the list page. Fortunately, Xamarin Forms allows for platform-specific styling . The final sample has platform-specific styling for the list page.","title":"Building the Client for Android"},{"location":"chapter1/firstapp_pc/#building-the-client-for-ios","text":"Finally, we get to the iOS platform. You will need to ensure your Mac is turned on and accessible via ssh, that it has XCode installed (and you have run XCode once so that you can accept the license agreement), and it has Visual Studio for Mac installed. Use Visual Studio Mobile Center for Mac Builds If you don't have a Mac handy, you can use Visual Studio Mobile Center to build your iOS version. You will need a signing certificate for this as you will obviously not have access to the iOS Simulator that is available on a Mac. When you created the projects, you were asked to link Visual Studio to your mac. This linkage is used for building the project. In essence, the entire project is sent to the Mac and the build tools that are supplied with Visual Studio for Mac are used to build the project. Right-click the TaskList.iOS project and select Set as StartUp Project . Right-click the TaskList.iOS project and select Build . You knew it was not going to be that easy, right? Here are the errors that I received when building for the first time: The error about No valid iOS code signing keys found in keychain is because we selected (by default) a package to deploy onto an iPhone and have not signed up for an Apple Developer Account and linked it to our Mac development environment. To fix this, select the \"iPhoneSimulator\" Configuration: Once you have selected the iPhoneSimulator configuration, re-build the mobile application. It should work this time. Run the simulator using the normal Visual Studio method. Visual Studio 2017 will connect to the Mac to run the simulator but display the simulator on your PC. Before long, you should see the following: You need a platform initialization call on most platforms for offline sync, and you always need a platform initializer for iOS. In TaskList.Droid, this should be added in MainActivity.cs and in TaskList.iOS, it's AppDelegate.cs . In each of these files, there is a line that initializes the Xamarin Forms system. // Android Version global::Xamarin.Forms.Forms.Init(this, bundle); // iOS Version global::Xamarin.Forms.Forms.Init(); Immediately before this line, you should add the initializer for Azure Mobile Apps. It's important that the Azure Mobile Apps subsystem is initialized prior to Xamarin Forms being initialized and any UI code being called. Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); This initializer is not required on the UWP project. Once this is done and you have re-built the project, you can see the fruits of your labor. The final product screens look like this:","title":"Building the Client for iOS"},{"location":"chapter1/firstapp_pc/#some-final-thoughts","text":"If you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations. There are a lot of places where things can go wrong. You are really integrating the build systems across Visual Studio, Android, Xamarin, and XCode. Fortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work. If you would rather develop code on your mac, the next sectiom is for you - it gets into the nitty gritty of developing the exact same app, but using Visual Studio for Mac instead. The following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order. If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.","title":"Some Final Thoughts"},{"location":"chapter2/authconcepts/","text":"Authentication Concepts \u00b6 One of the very first things you want to do when developing a mobile app is to provide users with a unique experience. For our example task list application, this could be as simple as providing a task list for the user who is logged in. In more complex applications, this is the gateway to role-based access controls, group rules, and sharing with your friends. In all these cases, properly identifying the user using the phone is the starting point. Authentication provides a process by which the user that is using the mobile device can be identified securely. This is generally done by entering a username and password. However, modern systems can also provide multi-factor authentication , send you a text message to a registered device, or use your fingerprint as the password. The OAuth Process \u00b6 In just about every single mobile application, a process called OAuth is used to properly identify a user to the mobile backend. OAuth is not an authentication mechanism in its own right. It is used to route the authentication request to the right place and to verify that the authentication took place. There are three actors in the OAuth protocol: The Client is the application attempting to get access to the resource. The Resource is the mobile backend that the client is attempting to access. The Identity Provider (or IdP) is the service that is responsible for authenticating the client. At the end of the process, a cryptographically signed token is minted. This token is added to every request made by the client to the resource to securely identify the user. Server Side vs. Client Side \u00b6 There are two types of authentication flow: Server-flow and Client-flow. They are so named because of who controls the flow of the actual authentication. Server-flow is named because the authentication flow is managed by the Azure App Service (the server) through a webview-based work flow. It is generally used in two cases: You want a simple placeholder for authentication in your mobile app while you are developing other code. You are developing a traditional web app. Tip If you are developing a single-page application (SPA), then client-flow is the more appropriate model for authentication. The SPA looks more like a mobile client than a traditional web app. In particular, you will be redirected away from your single page and returned to the app at a specific entry point, removing any context from the app. In the case of server-flow: The client brings up a web view and asks for the login page from the resource. The resource redirects the client to the identity provider. The identity provider does the authentication before redirecting the client back to the resource (with an identity provider token). The resource validates the identity provider token with the identity provider. Finally, the resource mints a new resource token that it returns to the client. Client-flow authentication uses an IdP provided SDK to integrate a more native feel to the authentication flow. The actual flow happens on the client, communicating only with the identity provider. It is used is most cases: You want a more integrated experience for your users. You want the most native feel to your authentication flow. You are developing a single-page web application. A client-flow feels similar to the server-flow, but using a native SDK instead of a web view. The client uses the identity provider SDK to communicate with the identity provider. The identity provider authenticates the user, returning an identity provider token. The client presents the identity provider token to the resource. The resource validates the identity provider token with the identity provider. Finally, the resource mints a new resource token that it returns to the client. For example, if you use the Facebook SDK for authentication, your app will seamlessly switch over into the Facebook app and ask you to authorize your client application before switching you back to your client application. You should use the identity provider SDK when developing an app that will be released on the app store. The identity providers will advise you to use their SDK and it provides the best experience for your end users. Info The Azure App Service Authentication / Authorization service works with any App Service, including web apps and API apps. It's not just for Mobile Apps. Authentication Providers \u00b6 Azure Mobile Apps supports five identity providers natively: Azure Active Directory Facebook Google Microsoft (MSA) Twitter Info Azure App Service Authentication / Authorization maintains a token store in the XDrive (which is the drive that is shared among all instances of the backend within the same App Service Plan). The token store is located at D:\\\\home\\\\data\\\\.auth\\\\tokens on the backend. The tokens are encrypted and stored in a per-user encrypted file. In addition, you can set up client-flow custom authentication that allows you to mint a ZUMO token to your specifications for any provider using a client-flow. For example, you could use authentication providers like Azure AD B2C , LinkedIn or GitHub , a third-party authentication provider like Auth0 , or you could set up an identity table in your database so that you can check username and password without an identity provider.","title":"Concepts"},{"location":"chapter2/authconcepts/#authentication-concepts","text":"One of the very first things you want to do when developing a mobile app is to provide users with a unique experience. For our example task list application, this could be as simple as providing a task list for the user who is logged in. In more complex applications, this is the gateway to role-based access controls, group rules, and sharing with your friends. In all these cases, properly identifying the user using the phone is the starting point. Authentication provides a process by which the user that is using the mobile device can be identified securely. This is generally done by entering a username and password. However, modern systems can also provide multi-factor authentication , send you a text message to a registered device, or use your fingerprint as the password.","title":"Authentication Concepts"},{"location":"chapter2/authconcepts/#the-oauth-process","text":"In just about every single mobile application, a process called OAuth is used to properly identify a user to the mobile backend. OAuth is not an authentication mechanism in its own right. It is used to route the authentication request to the right place and to verify that the authentication took place. There are three actors in the OAuth protocol: The Client is the application attempting to get access to the resource. The Resource is the mobile backend that the client is attempting to access. The Identity Provider (or IdP) is the service that is responsible for authenticating the client. At the end of the process, a cryptographically signed token is minted. This token is added to every request made by the client to the resource to securely identify the user.","title":"The OAuth Process"},{"location":"chapter2/authconcepts/#server-side-vs-client-side","text":"There are two types of authentication flow: Server-flow and Client-flow. They are so named because of who controls the flow of the actual authentication. Server-flow is named because the authentication flow is managed by the Azure App Service (the server) through a webview-based work flow. It is generally used in two cases: You want a simple placeholder for authentication in your mobile app while you are developing other code. You are developing a traditional web app. Tip If you are developing a single-page application (SPA), then client-flow is the more appropriate model for authentication. The SPA looks more like a mobile client than a traditional web app. In particular, you will be redirected away from your single page and returned to the app at a specific entry point, removing any context from the app. In the case of server-flow: The client brings up a web view and asks for the login page from the resource. The resource redirects the client to the identity provider. The identity provider does the authentication before redirecting the client back to the resource (with an identity provider token). The resource validates the identity provider token with the identity provider. Finally, the resource mints a new resource token that it returns to the client. Client-flow authentication uses an IdP provided SDK to integrate a more native feel to the authentication flow. The actual flow happens on the client, communicating only with the identity provider. It is used is most cases: You want a more integrated experience for your users. You want the most native feel to your authentication flow. You are developing a single-page web application. A client-flow feels similar to the server-flow, but using a native SDK instead of a web view. The client uses the identity provider SDK to communicate with the identity provider. The identity provider authenticates the user, returning an identity provider token. The client presents the identity provider token to the resource. The resource validates the identity provider token with the identity provider. Finally, the resource mints a new resource token that it returns to the client. For example, if you use the Facebook SDK for authentication, your app will seamlessly switch over into the Facebook app and ask you to authorize your client application before switching you back to your client application. You should use the identity provider SDK when developing an app that will be released on the app store. The identity providers will advise you to use their SDK and it provides the best experience for your end users. Info The Azure App Service Authentication / Authorization service works with any App Service, including web apps and API apps. It's not just for Mobile Apps.","title":"Server Side vs. Client Side"},{"location":"chapter2/authconcepts/#authentication-providers","text":"Azure Mobile Apps supports five identity providers natively: Azure Active Directory Facebook Google Microsoft (MSA) Twitter Info Azure App Service Authentication / Authorization maintains a token store in the XDrive (which is the drive that is shared among all instances of the backend within the same App Service Plan). The token store is located at D:\\\\home\\\\data\\\\.auth\\\\tokens on the backend. The tokens are encrypted and stored in a per-user encrypted file. In addition, you can set up client-flow custom authentication that allows you to mint a ZUMO token to your specifications for any provider using a client-flow. For example, you could use authentication providers like Azure AD B2C , LinkedIn or GitHub , a third-party authentication provider like Auth0 , or you could set up an identity table in your database so that you can check username and password without an identity provider.","title":"Authentication Providers"},{"location":"chapter2/authorization/","text":"Claims and Authorization \u00b6 Now that we have covered all the techniques for authentication, it's time to look at authorization. While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation. Authorization is handled within the server-side project by the [Authorize] attribute. Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not. The Authorize attribute can also check to see if a user is in a list of users or roles. However, there is a problem with this. The user id is not guessable and we have no roles. To see what I mean, run the Backend project locally and set a break point on the GetAllTodoItems() method in the TodoItemController , then run your server and your UWP application. Tip Once you have built and deployed the UWP application, it will appear in your normal Application list. This allows you to run the application and the server at the same time on the same machine. Once you have authenticated, you will be able to set a break point to take a look at this.User.Identity : Note that the Name property is null. This is the property that is used when you want to authorize individual users. Expand the Claims property and then click on Results View : The only claims are the ones in the token, and none of them match the RoleClaimType , so we can't use roles either. Clearly, we are going to have to do something else. Obtaining User Claims \u00b6 At some point you are going to need to deal with something other than the claims that are in the token passed for authentication. Fortunately, the Authentication / Authorization feature has an endpoint for that at /.auth/me : Of course, the /.auth/me endpoint is not of any use if you cannot access it. The most use of this information is gained during authorization on the server and we will cover this use later on. However, there are reasons to pull this information on the client as well. For example, we may want to make the List View title be our name instead of \"Tasks\". Warn You can't use the /.auth/me endpoint when using custom authentication. Since identity provider claims can be anything, they are transferred as a list within a JSON object. Before we can decode the JSON object, we need to define the models. This is done in the shared TaskList project. I've defined this in Models\\AppServiceIdentity.cs . using System.Collections.Generic; using Newtonsoft.Json; namespace TaskList.Models { public class AppServiceIdentity { [JsonProperty(PropertyName = \"id_token\")] public string IdToken { get; set; } [JsonProperty(PropertyName = \"provider_name\")] public string ProviderName { get; set; } [JsonProperty(PropertyName = \"user_id\")] public string UserId { get; set; } [JsonProperty(PropertyName = \"user_claims\")] public List<UserClaim> UserClaims { get; set; } } public class UserClaim { [JsonProperty(PropertyName = \"typ\")] public string Type { get; set; } [JsonProperty(PropertyName = \"val\")] public string Value { get; set; } } } This matches the JSON format from the /.auth/me call we did earlier. This is going to be a part of the ICloudService as follows: using System.Threading.Tasks; using TaskList.Models; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); Task LoginAsync(User user); Task<AppServiceIdentity> GetIdentityAsync(); } } Finally, we need to actually implement the concrete version in AzureCloudService.cs : List<AppServiceIdentity> identities = null; public async Task<AppServiceIdentity> GetIdentityAsync() { if (client.CurrentUser == null || client.CurrentUser?.MobileServiceAuthenticationToken == null) { throw new InvalidOperationException(\"Not Authenticated\"); } if (identities == null) { identities = await client.InvokeApiAsync<List<AppServiceIdentity>>(\"/.auth/me\"); } if (identities.Count > 0) return identities[0]; return null; } Note that there is no reason to instantiate your own HttpClient() . The Azure Mobile Apps SDK has a method for invoking custom API calls (as we shall see later on). However, if you prefix the path with a slash, it will execute a HTTP GET for any API with any authentication that is currently in force. We can leverage this to call the /.auth/me endpoint and decode the response in one line of code. Adjust the ExecuteRefreshCommand() method in the ViewModels\\TaskListViewModel.cs file to take advantage of this: async Task ExecuteRefreshCommand() { if (IsBusy) return; IsBusy = true; try { var identity = await cloudService.GetIdentityAsync(); if (identity != null) { var name = identity.UserClaims.FirstOrDefault(c => c.Type.Equals(\"name\")).Value; Title = $\"Tasks for {name}\"; } var list = await Table.ReadAllItemsAsync(); Items.ReplaceRange(list); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Items Not Loaded\", ex.Message, \"OK\"); } finally { IsBusy = false; } } The return value from the GetIdentityAsync() method is the first identity. Normally, a user would only authenticate once, so this is fairly safe. The number of claims returned depends on the identity provider and could easily number in the hundreds. Even the default configuration for Azure Active Directory returns 18 claims. These are easily handled using LINQ, however. The Type property holds the type. This could be a short (common) name. It could also be a schema name, which looks more like a URI. The only way to know what claims are coming back for sure is to look at the /.auth/me result with something like Postman. Warn If you are using Custom Authentication (e.g. username/password or a third-party token), then the /.auth/me endpoint is not available to you. You can still produce a custom API in your backend to provide this information to your client, but you are responsible for the code - it's custom, after all! Authorization \u00b6 Now that we have covered all the techniques for authentication, it's time to look at authorization. While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation. Authorization is handled within the server-side project by the [Authorize] attribute. Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not. The Authorize attribute can also check to see if a user is in a list of users or roles. However, there is a problem with this. The user id is not guessable and we have no roles. To see what I mean, run the Backend project and set a break point on the GetAllTodoItems() method in the TodoItemController , then run your server and your UWP application. Tip Once you have built and deployed the UWP application, it will appear in your normal Application list. This allows you to run the application and the server at the same time on the same machine. Alternatively, you can attach a Debugger to your Azure App Service within Visual Studio's Cloud Explorer. Once you have authenticated, you will be able to set a break point to take a look at this.User.Identity : Note that the Name property is null. This is the property that is used when you want to authorize individual users. Expand the Claims property and then click on Results View : The only claims are the ones in the token, and none of them match the RoleClaimType , so we can't use roles either. Clearly, we are going to have to do something else. Fortunately, we already know that we can get some information about the identity provider claims from the /.auth/me endpoint. To get the extra information, we need to query the User object: var identity = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); There is one Credentials class for each supported authentication technique - Azure Active Directory, Facebook, Google, Microsoft Account and Twitter. These are in the Microsoft.Azure.Mobile.Server.Authentication namespace. They all follow the same pattern as the model we created for the client - there are Provider, UserId and UserClaims properties. The token and any special information will be automatically decoded for you. For instance, the TenantId is pulled out of the response for Azure AD. Tip You can use the AccessToken property to do Graph API lookups for most providers. Use the Graph SDK provided by the authentication provider for this purpose; either in the mobile backend or the client. Adding Group Claims to the Request \u00b6 There are times when you want to add something else to the token that is returned from Azure AD. The most common requirement is to add group information to the response so you can handle group-based authorization. To add security groups to the Azure AD token: Log into the Azure Portal . Click on Azure Active Directory in your left-hand menu (you may need to search for it). If you need to work in a different directory, click Switch directory to do so. Click App registrations . Click your WEB application. Click Manifest at the top of the registered app blade. You are now placed within a manifest editor. Find the groupMembershipClaims entry and change it from null to \"SecurityGroup\" . When complete, click Save . You can now give the web application additional permissions: Close the Manifest editor. Click Settings . Click Required permissions . Click Windows Azure Active Directory . Click Read directory data under the Delegated Permissions section. Scroll to the bottom, click on Delegated Permissions . Check the box for Read directory data . Click on Save . If you are not an administrator of the domain, then an administrator will need to approve your request. Now that you have configured the application to return groups as part of the claims, you should probably add a couple of groups: Click Azure Active Directory in the left-hand menu to return to the top-level. Click Users and groups . Click All groups . Click + Add . Fill in the information, then click Create . Click on the new group. Make a note of the OBJECT ID . The claims for groups are listed by the Object ID, so you will need this to refer to the group later. It's a good idea to add a couple of groups for testing purposes. If you are using the organization directory, you will need to request the creation of a couple of groups for application roles. The view of the groups will be shown when we get the identity of the user using User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request) : Group Authorization \u00b6 Now that we have group claims in the claims list for the /.auth/me endpoint, we can move forward to do authorization based on these claims. This can be done in a relatively basic manner by implementing a method to check the claims: async Task<bool> IsAuthorizedAsync() { var identity = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); var countofGroups = identity.UserClaims .Where(c => c.Type.Equals(\"groups\") && c.Value.Equals(\"01f214a9-af1f-4bdd-938f-3f16749aef0e\")) .Count(); return (countofGroups > 0); } The UserClaims object is an IEnumerable that contains objects with a Type and a Value. The Type for the group claims is groups . Once we have this knowledge, we can use a LINQ query to obtain a count of the claims that match the conditions we want to test. The Value we use is the Object ID of the group. This is available in the PROPERTIES tab of the group. We can prevent a new record being added by adjusting the PostTodoItem() method: public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { if (!await IsAuthorizedAsync()) { return Unauthorized(); } TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } Unfortunately, most of the table controller methods do not return an IHttpActionResult , so this has limited value. What would be better would be an [Authorize] attribute that tests the claims for us. For instance, we should be able to do the following: [AuthorizeClaims(\"groups\", \"01f214a9-af1f-4bdd-938f-3f16749aef0e\")] public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } The [AuthorizeClaims()] attribute does not exist, so we have to provide it ourselves: using System.Linq; using System.Net; using System.Security.Principal; using System.Threading; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.Filters; using Microsoft.Azure.Mobile.Server.Authentication; namespace Backend.Helpers { public class AuthorizeClaimsAttribute : AuthorizationFilterAttribute { string Type { get; } string Value { get; } public AuthorizeClaimsAttribute(string type, string value) { Type = type; Value = value; } public override async Task OnAuthorizationAsync(HttpActionContext actionContext, CancellationToken cancellationToken) { var request = actionContext.Request; var user = actionContext.RequestContext.Principal; if (user != null) { var identity = await user.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(request); var countOfMatchingClaims = identity.UserClaims .Where(c => c.Type.Equals(Type) && c.Value.Equals(Value)) .Count(); if (countOfMatchingClaims > 0) return; } throw new HttpResponseException(HttpStatusCode.Unauthorized); } } } This is the same type of authorization filter attribute that the officially provided AuthorizeAttribute is based on. However, the AuthorizeAttribute is synchronous. We require an asynchronous version of the attribute, so we cannot use a sub-class of the AuthorizeAttribute. Aside from that note, this uses virtually the same code that we used in the IsAythorizedAsync() method we developped earlier. We can now use this attribute for testing any claim. For example, our claims has the identity provider as a claim. We can use the following: [AuthorizeClaims(\"http://schemas.microsoft.com/identity/claims/identityprovider\", \"live.com\")] Tip If you want to test other claims that are not provided, you can enable the Read Directory Data permission in the Azure Active Directory permissions and do a query against the Azure Active Directory. You should think about caching results or minting a new ZUMO token (just like we did in the custom authentication case) for performance reasons.","title":"Claims and Authorization"},{"location":"chapter2/authorization/#claims-and-authorization","text":"Now that we have covered all the techniques for authentication, it's time to look at authorization. While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation. Authorization is handled within the server-side project by the [Authorize] attribute. Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not. The Authorize attribute can also check to see if a user is in a list of users or roles. However, there is a problem with this. The user id is not guessable and we have no roles. To see what I mean, run the Backend project locally and set a break point on the GetAllTodoItems() method in the TodoItemController , then run your server and your UWP application. Tip Once you have built and deployed the UWP application, it will appear in your normal Application list. This allows you to run the application and the server at the same time on the same machine. Once you have authenticated, you will be able to set a break point to take a look at this.User.Identity : Note that the Name property is null. This is the property that is used when you want to authorize individual users. Expand the Claims property and then click on Results View : The only claims are the ones in the token, and none of them match the RoleClaimType , so we can't use roles either. Clearly, we are going to have to do something else.","title":"Claims and Authorization"},{"location":"chapter2/authorization/#obtaining-user-claims","text":"At some point you are going to need to deal with something other than the claims that are in the token passed for authentication. Fortunately, the Authentication / Authorization feature has an endpoint for that at /.auth/me : Of course, the /.auth/me endpoint is not of any use if you cannot access it. The most use of this information is gained during authorization on the server and we will cover this use later on. However, there are reasons to pull this information on the client as well. For example, we may want to make the List View title be our name instead of \"Tasks\". Warn You can't use the /.auth/me endpoint when using custom authentication. Since identity provider claims can be anything, they are transferred as a list within a JSON object. Before we can decode the JSON object, we need to define the models. This is done in the shared TaskList project. I've defined this in Models\\AppServiceIdentity.cs . using System.Collections.Generic; using Newtonsoft.Json; namespace TaskList.Models { public class AppServiceIdentity { [JsonProperty(PropertyName = \"id_token\")] public string IdToken { get; set; } [JsonProperty(PropertyName = \"provider_name\")] public string ProviderName { get; set; } [JsonProperty(PropertyName = \"user_id\")] public string UserId { get; set; } [JsonProperty(PropertyName = \"user_claims\")] public List<UserClaim> UserClaims { get; set; } } public class UserClaim { [JsonProperty(PropertyName = \"typ\")] public string Type { get; set; } [JsonProperty(PropertyName = \"val\")] public string Value { get; set; } } } This matches the JSON format from the /.auth/me call we did earlier. This is going to be a part of the ICloudService as follows: using System.Threading.Tasks; using TaskList.Models; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); Task LoginAsync(User user); Task<AppServiceIdentity> GetIdentityAsync(); } } Finally, we need to actually implement the concrete version in AzureCloudService.cs : List<AppServiceIdentity> identities = null; public async Task<AppServiceIdentity> GetIdentityAsync() { if (client.CurrentUser == null || client.CurrentUser?.MobileServiceAuthenticationToken == null) { throw new InvalidOperationException(\"Not Authenticated\"); } if (identities == null) { identities = await client.InvokeApiAsync<List<AppServiceIdentity>>(\"/.auth/me\"); } if (identities.Count > 0) return identities[0]; return null; } Note that there is no reason to instantiate your own HttpClient() . The Azure Mobile Apps SDK has a method for invoking custom API calls (as we shall see later on). However, if you prefix the path with a slash, it will execute a HTTP GET for any API with any authentication that is currently in force. We can leverage this to call the /.auth/me endpoint and decode the response in one line of code. Adjust the ExecuteRefreshCommand() method in the ViewModels\\TaskListViewModel.cs file to take advantage of this: async Task ExecuteRefreshCommand() { if (IsBusy) return; IsBusy = true; try { var identity = await cloudService.GetIdentityAsync(); if (identity != null) { var name = identity.UserClaims.FirstOrDefault(c => c.Type.Equals(\"name\")).Value; Title = $\"Tasks for {name}\"; } var list = await Table.ReadAllItemsAsync(); Items.ReplaceRange(list); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Items Not Loaded\", ex.Message, \"OK\"); } finally { IsBusy = false; } } The return value from the GetIdentityAsync() method is the first identity. Normally, a user would only authenticate once, so this is fairly safe. The number of claims returned depends on the identity provider and could easily number in the hundreds. Even the default configuration for Azure Active Directory returns 18 claims. These are easily handled using LINQ, however. The Type property holds the type. This could be a short (common) name. It could also be a schema name, which looks more like a URI. The only way to know what claims are coming back for sure is to look at the /.auth/me result with something like Postman. Warn If you are using Custom Authentication (e.g. username/password or a third-party token), then the /.auth/me endpoint is not available to you. You can still produce a custom API in your backend to provide this information to your client, but you are responsible for the code - it's custom, after all!","title":"Obtaining User Claims"},{"location":"chapter2/authorization/#authorization","text":"Now that we have covered all the techniques for authentication, it's time to look at authorization. While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation. Authorization is handled within the server-side project by the [Authorize] attribute. Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not. The Authorize attribute can also check to see if a user is in a list of users or roles. However, there is a problem with this. The user id is not guessable and we have no roles. To see what I mean, run the Backend project and set a break point on the GetAllTodoItems() method in the TodoItemController , then run your server and your UWP application. Tip Once you have built and deployed the UWP application, it will appear in your normal Application list. This allows you to run the application and the server at the same time on the same machine. Alternatively, you can attach a Debugger to your Azure App Service within Visual Studio's Cloud Explorer. Once you have authenticated, you will be able to set a break point to take a look at this.User.Identity : Note that the Name property is null. This is the property that is used when you want to authorize individual users. Expand the Claims property and then click on Results View : The only claims are the ones in the token, and none of them match the RoleClaimType , so we can't use roles either. Clearly, we are going to have to do something else. Fortunately, we already know that we can get some information about the identity provider claims from the /.auth/me endpoint. To get the extra information, we need to query the User object: var identity = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); There is one Credentials class for each supported authentication technique - Azure Active Directory, Facebook, Google, Microsoft Account and Twitter. These are in the Microsoft.Azure.Mobile.Server.Authentication namespace. They all follow the same pattern as the model we created for the client - there are Provider, UserId and UserClaims properties. The token and any special information will be automatically decoded for you. For instance, the TenantId is pulled out of the response for Azure AD. Tip You can use the AccessToken property to do Graph API lookups for most providers. Use the Graph SDK provided by the authentication provider for this purpose; either in the mobile backend or the client.","title":"Authorization"},{"location":"chapter2/authorization/#adding-group-claims-to-the-request","text":"There are times when you want to add something else to the token that is returned from Azure AD. The most common requirement is to add group information to the response so you can handle group-based authorization. To add security groups to the Azure AD token: Log into the Azure Portal . Click on Azure Active Directory in your left-hand menu (you may need to search for it). If you need to work in a different directory, click Switch directory to do so. Click App registrations . Click your WEB application. Click Manifest at the top of the registered app blade. You are now placed within a manifest editor. Find the groupMembershipClaims entry and change it from null to \"SecurityGroup\" . When complete, click Save . You can now give the web application additional permissions: Close the Manifest editor. Click Settings . Click Required permissions . Click Windows Azure Active Directory . Click Read directory data under the Delegated Permissions section. Scroll to the bottom, click on Delegated Permissions . Check the box for Read directory data . Click on Save . If you are not an administrator of the domain, then an administrator will need to approve your request. Now that you have configured the application to return groups as part of the claims, you should probably add a couple of groups: Click Azure Active Directory in the left-hand menu to return to the top-level. Click Users and groups . Click All groups . Click + Add . Fill in the information, then click Create . Click on the new group. Make a note of the OBJECT ID . The claims for groups are listed by the Object ID, so you will need this to refer to the group later. It's a good idea to add a couple of groups for testing purposes. If you are using the organization directory, you will need to request the creation of a couple of groups for application roles. The view of the groups will be shown when we get the identity of the user using User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request) :","title":"Adding Group Claims to the Request"},{"location":"chapter2/authorization/#group-authorization","text":"Now that we have group claims in the claims list for the /.auth/me endpoint, we can move forward to do authorization based on these claims. This can be done in a relatively basic manner by implementing a method to check the claims: async Task<bool> IsAuthorizedAsync() { var identity = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); var countofGroups = identity.UserClaims .Where(c => c.Type.Equals(\"groups\") && c.Value.Equals(\"01f214a9-af1f-4bdd-938f-3f16749aef0e\")) .Count(); return (countofGroups > 0); } The UserClaims object is an IEnumerable that contains objects with a Type and a Value. The Type for the group claims is groups . Once we have this knowledge, we can use a LINQ query to obtain a count of the claims that match the conditions we want to test. The Value we use is the Object ID of the group. This is available in the PROPERTIES tab of the group. We can prevent a new record being added by adjusting the PostTodoItem() method: public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { if (!await IsAuthorizedAsync()) { return Unauthorized(); } TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } Unfortunately, most of the table controller methods do not return an IHttpActionResult , so this has limited value. What would be better would be an [Authorize] attribute that tests the claims for us. For instance, we should be able to do the following: [AuthorizeClaims(\"groups\", \"01f214a9-af1f-4bdd-938f-3f16749aef0e\")] public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } The [AuthorizeClaims()] attribute does not exist, so we have to provide it ourselves: using System.Linq; using System.Net; using System.Security.Principal; using System.Threading; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.Filters; using Microsoft.Azure.Mobile.Server.Authentication; namespace Backend.Helpers { public class AuthorizeClaimsAttribute : AuthorizationFilterAttribute { string Type { get; } string Value { get; } public AuthorizeClaimsAttribute(string type, string value) { Type = type; Value = value; } public override async Task OnAuthorizationAsync(HttpActionContext actionContext, CancellationToken cancellationToken) { var request = actionContext.Request; var user = actionContext.RequestContext.Principal; if (user != null) { var identity = await user.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(request); var countOfMatchingClaims = identity.UserClaims .Where(c => c.Type.Equals(Type) && c.Value.Equals(Value)) .Count(); if (countOfMatchingClaims > 0) return; } throw new HttpResponseException(HttpStatusCode.Unauthorized); } } } This is the same type of authorization filter attribute that the officially provided AuthorizeAttribute is based on. However, the AuthorizeAttribute is synchronous. We require an asynchronous version of the attribute, so we cannot use a sub-class of the AuthorizeAttribute. Aside from that note, this uses virtually the same code that we used in the IsAythorizedAsync() method we developped earlier. We can now use this attribute for testing any claim. For example, our claims has the identity provider as a claim. We can use the following: [AuthorizeClaims(\"http://schemas.microsoft.com/identity/claims/identityprovider\", \"live.com\")] Tip If you want to test other claims that are not provided, you can enable the Read Directory Data permission in the Azure Active Directory permissions and do a query against the Azure Active Directory. You should think about caching results or minting a new ZUMO token (just like we did in the custom authentication case) for performance reasons.","title":"Group Authorization"},{"location":"chapter2/backend/","text":"Adding Authentication to a Mobile Backend \u00b6 The Azure App Service Authentication / Authorization service integrates seamlessly into an Azure Mobile Apps backend as a piece of middleware that fills in the Identity information for ASP.NET. That means the only thing we have to worry about is authorization. The authentication piece (determining that a user is who they say they are) is already taken care of. Authorization (which is the determination of whether an authenticated user can use a specific API) can happen at either the controller level or an individual operation level. We can add authorization to an entire table controller by adding the [Authorize] attribute to the table controller. For example, here is our table controller from the first chapter with authorization required for all operations: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { [Authorize] public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) => DeleteAsync(id); } } Authorization can also happen on a per-operation basis by adding the [Authorize] attribute to a single method within the table controller. For example, instead of requiring authorization on the entire table, we want a version where reading was possible anonymously but updating the database required authentication: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 [Authorize] public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem [Authorize] public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 [Authorize] public Task DeleteTodoItem(string id) => DeleteAsync(id); } } Note that the [Authorize] attribute can do much more than what is provided here. Underneath there are various parameters that you can adjust to see if the user belongs to a specific group or role. However, the token that is checked to see if the user is authenticated does not pull in any of the other information that is normally needed for such authorization tasks. As a result, the [Authorize] tags is really only checking whether a request requires authentication or not. Configuring an Authentication Provider \u00b6 Configuration of the identity provider is very dependent on the identity provider and whether the client is using a client-flow or server-flow. Choose one of the several options for authentication: Enterprise Authentication covers Azure Active Directory. Social Authentication covers Facebook, Google, Microsoft and Twitter. We can also configure authentication using custom routes. This allows us to use other (non-supported) services or to completely customize our flow (for example, to use an existing identity database). We will cover custom authentication later on.","title":"Authentication in the Backend"},{"location":"chapter2/backend/#adding-authentication-to-a-mobile-backend","text":"The Azure App Service Authentication / Authorization service integrates seamlessly into an Azure Mobile Apps backend as a piece of middleware that fills in the Identity information for ASP.NET. That means the only thing we have to worry about is authorization. The authentication piece (determining that a user is who they say they are) is already taken care of. Authorization (which is the determination of whether an authenticated user can use a specific API) can happen at either the controller level or an individual operation level. We can add authorization to an entire table controller by adding the [Authorize] attribute to the table controller. For example, here is our table controller from the first chapter with authorization required for all operations: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { [Authorize] public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) => DeleteAsync(id); } } Authorization can also happen on a per-operation basis by adding the [Authorize] attribute to a single method within the table controller. For example, instead of requiring authorization on the entire table, we want a version where reading was possible anonymously but updating the database required authentication: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() => Query(); // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) => Lookup(id); // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 [Authorize] public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) => UpdateAsync(id, patch); // POST tables/TodoItem [Authorize] public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 [Authorize] public Task DeleteTodoItem(string id) => DeleteAsync(id); } } Note that the [Authorize] attribute can do much more than what is provided here. Underneath there are various parameters that you can adjust to see if the user belongs to a specific group or role. However, the token that is checked to see if the user is authenticated does not pull in any of the other information that is normally needed for such authorization tasks. As a result, the [Authorize] tags is really only checking whether a request requires authentication or not.","title":"Adding Authentication to a Mobile Backend"},{"location":"chapter2/backend/#configuring-an-authentication-provider","text":"Configuration of the identity provider is very dependent on the identity provider and whether the client is using a client-flow or server-flow. Choose one of the several options for authentication: Enterprise Authentication covers Azure Active Directory. Social Authentication covers Facebook, Google, Microsoft and Twitter. We can also configure authentication using custom routes. This allows us to use other (non-supported) services or to completely customize our flow (for example, to use an existing identity database). We will cover custom authentication later on.","title":"Configuring an Authentication Provider"},{"location":"chapter2/bestpractices/","text":"Best Practices \u00b6 We've covered a lot of ground with authentication and authorization, so I wanted to cover some of the best practices that I generally advise when thinking about this topic. Don't store passwords \u00b6 I can't really advise on which identity provider is best for your mobile application. However, I can clearly say that delegating the security of the identity database to someone who has that as their full time job is an excellent idea. Choosing an identity provider is not easy. Here are my choices: If you need enterprise authentication, use Azure Active Directory . If you need a specific social identity provider (for example, Facebook or Google), use that. If you need multiple social identity providers, Auth0 is an excellent choice. If you need usernames and password, use Azure Active Directory B2C . Storing usernames and passwords in your own database is a bad idea and should be avoided. Avoid using Microsoft Account and Twitter authentication providers. Their implementation has limitations that will be a concern if you ever try to share an authentication provider between two different backends or apps. Use the client SDK provided by the Identity Provider \u00b6 Your first step should be getting an access token from the identity provider itself. You will see the most integrated experience if you use their SDK. For Azure Active Directory , that SDK is ADAL . For Facebook , check out Xamarin.Facebook.iOS or Xamarin.Facebook.Android . For Google , check out Google APIs Core Client Library . For Auth0 , check out the Auth0 Xamarin Component . For Azure Active Directory B2C , use ADAL . Swap the Identity Provider access token for a ZUMO token. \u00b6 Never use the token provided by the identity provider for anything other than requesting access to the resource. Use a short lived token (an hour is the standard) that is minted just for the purpose of providing that access. If you are not using an identity provider that is explicitly supported by Azure App Service, use a custom authentication provider to mint your own token. Enforce Security at the Server \u00b6 There is no easy way to say this. There are bad guys out there, and they are after your data. You should not assume that someone is using your client. It could just be someone with a REST client. Ensure you enforce security on your server. You can do this easily by using the [Authorize] attribute, the [AuthorizeClaims()] attribute we developed in the Authorization section or your own custom authorization attribute. Monitoring the server is just as important as enforcing security. The Azure App Service Authentication service outputs quite a bit of logging about who is logging in (and who is denied), so you can get some good intelligence out of the logs when you mine them properly. Securely Store Tokens \u00b6 The decisions we make are always a trade-off between convenience and security. One such decision is if we should store tokens with the app. On the one hand, it's convenient to allow the app to remember the login and to not ask us to log in again with each app start. On the other hand, that fact opens up a security hole that a determined hacker can exploit. We can have convenience while still having security by utilizing the secure stores that each platform provides to store secrets like the access token. Use https only \u00b6 This should go without saying. Always use https communication. Most security professionals start off their security career with learning \"Security at Rest & Security in Transit\". In practical terms, storing secrets (like tokens) securely and using HTTPS as a transport mechanism satisfies both claims. Don't stop with security there though. HTTPS is just a medium through which secure communications can take place. There are a wide range of protocols and ciphers that can be used to encrypt the traffic. Some are considered less secure and not to be used. Azure App Service provides a default set of protocols and ciphers to support backwards compatibility with older browsers. You can adjust the ciphers in use by your App Service. For information on this, refer to the Azure Documentation . Handle Expiring Tokens \u00b6 Unless you are using an identity provider that doesn't support refresh tokens (like Facebook or Twitter), you should handle refresh tokens by silently calling the refresh action. Tokens are going to expire. This is a fact of the token specifications. You need to deal with expiring tokens and act accordingly. If you do need to use an identity provider that does not support refresh tokens, you are going to have to ask for credentials whenever the token expires. You don't get out of determining the user experience when tokens expire just because you are using Facebook or Twitter. Authenticating Your App \u00b6 My final word on authentication has to do with authenticating your app. I get the same request every week. How do I implement an API key for my app? When I probe a little, I get a few reasons for this request: I want to ensure my app is the only one accessing my backend because the data is important. I don't want my users to log in as it is inconvenient. I want to monetize my app, and I can't do that if anyone can copy it. API keys are used by multi-tenant systems to route requests for data to the appropriate data store. For example, the very popular Parse Server used to have an API key because all clients connected to the same parse.com service. Once the Parse Server was open-sourced, the API key went away. It was no longer needed to route the request. In the same way, the Azure App Service has a unique name - the URL of the service, so it doesn't need an API key to route the information. An API key does not prevent a rogue client from accessing your data. If you did use an API key for security, you can easily get the API key for the app by putting together a \"man in the middle proxy\". One such proxy is Telerik Fiddler . One of its features is \"Security Testing\" which amounts to a man-in-the-middle decryption technique. This works with the Android emulator as well. For iOS, you can use Charles . So, how do you authenticate your app? Step back a moment. What are you monetizing or protecting? It's likely the data within the mobile backend. Protect that data by authenticating your users. If you absolutely must monetize your app, then there are ways to do it, and we will discuss those later in the book.","title":"Best Practices"},{"location":"chapter2/bestpractices/#best-practices","text":"We've covered a lot of ground with authentication and authorization, so I wanted to cover some of the best practices that I generally advise when thinking about this topic.","title":"Best Practices"},{"location":"chapter2/bestpractices/#dont-store-passwords","text":"I can't really advise on which identity provider is best for your mobile application. However, I can clearly say that delegating the security of the identity database to someone who has that as their full time job is an excellent idea. Choosing an identity provider is not easy. Here are my choices: If you need enterprise authentication, use Azure Active Directory . If you need a specific social identity provider (for example, Facebook or Google), use that. If you need multiple social identity providers, Auth0 is an excellent choice. If you need usernames and password, use Azure Active Directory B2C . Storing usernames and passwords in your own database is a bad idea and should be avoided. Avoid using Microsoft Account and Twitter authentication providers. Their implementation has limitations that will be a concern if you ever try to share an authentication provider between two different backends or apps.","title":"Don't store passwords"},{"location":"chapter2/bestpractices/#use-the-client-sdk-provided-by-the-identity-provider","text":"Your first step should be getting an access token from the identity provider itself. You will see the most integrated experience if you use their SDK. For Azure Active Directory , that SDK is ADAL . For Facebook , check out Xamarin.Facebook.iOS or Xamarin.Facebook.Android . For Google , check out Google APIs Core Client Library . For Auth0 , check out the Auth0 Xamarin Component . For Azure Active Directory B2C , use ADAL .","title":"Use the client SDK provided by the Identity Provider"},{"location":"chapter2/bestpractices/#swap-the-identity-provider-access-token-for-a-zumo-token","text":"Never use the token provided by the identity provider for anything other than requesting access to the resource. Use a short lived token (an hour is the standard) that is minted just for the purpose of providing that access. If you are not using an identity provider that is explicitly supported by Azure App Service, use a custom authentication provider to mint your own token.","title":"Swap the Identity Provider access token for a ZUMO token."},{"location":"chapter2/bestpractices/#enforce-security-at-the-server","text":"There is no easy way to say this. There are bad guys out there, and they are after your data. You should not assume that someone is using your client. It could just be someone with a REST client. Ensure you enforce security on your server. You can do this easily by using the [Authorize] attribute, the [AuthorizeClaims()] attribute we developed in the Authorization section or your own custom authorization attribute. Monitoring the server is just as important as enforcing security. The Azure App Service Authentication service outputs quite a bit of logging about who is logging in (and who is denied), so you can get some good intelligence out of the logs when you mine them properly.","title":"Enforce Security at the Server"},{"location":"chapter2/bestpractices/#securely-store-tokens","text":"The decisions we make are always a trade-off between convenience and security. One such decision is if we should store tokens with the app. On the one hand, it's convenient to allow the app to remember the login and to not ask us to log in again with each app start. On the other hand, that fact opens up a security hole that a determined hacker can exploit. We can have convenience while still having security by utilizing the secure stores that each platform provides to store secrets like the access token.","title":"Securely Store Tokens"},{"location":"chapter2/bestpractices/#use-https-only","text":"This should go without saying. Always use https communication. Most security professionals start off their security career with learning \"Security at Rest & Security in Transit\". In practical terms, storing secrets (like tokens) securely and using HTTPS as a transport mechanism satisfies both claims. Don't stop with security there though. HTTPS is just a medium through which secure communications can take place. There are a wide range of protocols and ciphers that can be used to encrypt the traffic. Some are considered less secure and not to be used. Azure App Service provides a default set of protocols and ciphers to support backwards compatibility with older browsers. You can adjust the ciphers in use by your App Service. For information on this, refer to the Azure Documentation .","title":"Use https only"},{"location":"chapter2/bestpractices/#handle-expiring-tokens","text":"Unless you are using an identity provider that doesn't support refresh tokens (like Facebook or Twitter), you should handle refresh tokens by silently calling the refresh action. Tokens are going to expire. This is a fact of the token specifications. You need to deal with expiring tokens and act accordingly. If you do need to use an identity provider that does not support refresh tokens, you are going to have to ask for credentials whenever the token expires. You don't get out of determining the user experience when tokens expire just because you are using Facebook or Twitter.","title":"Handle Expiring Tokens"},{"location":"chapter2/bestpractices/#authenticating-your-app","text":"My final word on authentication has to do with authenticating your app. I get the same request every week. How do I implement an API key for my app? When I probe a little, I get a few reasons for this request: I want to ensure my app is the only one accessing my backend because the data is important. I don't want my users to log in as it is inconvenient. I want to monetize my app, and I can't do that if anyone can copy it. API keys are used by multi-tenant systems to route requests for data to the appropriate data store. For example, the very popular Parse Server used to have an API key because all clients connected to the same parse.com service. Once the Parse Server was open-sourced, the API key went away. It was no longer needed to route the request. In the same way, the Azure App Service has a unique name - the URL of the service, so it doesn't need an API key to route the information. An API key does not prevent a rogue client from accessing your data. If you did use an API key for security, you can easily get the API key for the app by putting together a \"man in the middle proxy\". One such proxy is Telerik Fiddler . One of its features is \"Security Testing\" which amounts to a man-in-the-middle decryption technique. This works with the Android emulator as well. For iOS, you can use Charles . So, how do you authenticate your app? Step back a moment. What are you monetizing or protecting? It's likely the data within the mobile backend. Protect that data by authenticating your users. If you absolutely must monetize your app, then there are ways to do it, and we will discuss those later in the book.","title":"Authenticating Your App"},{"location":"chapter2/custom/","text":"Custom authentication \u00b6 For some situations, the social or enterprise flows are not valid for the mobile client. Perhaps you want the ability to provide a sign-up process with a username and password rather than using a social provider. Perhaps you want to use an alternate provider that is not one of the supported five providers. Whatever the reason, Azure App Service provides the ability to handle all situations. In this section, I will look at three methods for providing a unique set of usernames with no connection to the social or enterprise authentication. Using an Identity Database. \u00b6 Probably the most common request is to use a custom identity database. In general, this is desirable because you already have a database of usernames and password. However, it's probably the least desirable option because of the security concerns that come along with this technique. The news is rife with password leakage for very large organizations. The best way to ensure you do not disclose a users password is to not have it in the first place. Warn I'm not going to cover the sign-up case here. This would be an additional process and would use a regular Web API to insert data into the database after validation (and probably verification via email or text message). The first thing we need to add to our project is a model for the user object. I created the following in the Models folder of the Backend project: using System.ComponentModel.DataAnnotations; namespace Backend.Models { public class User { [Key] public int Id { get; set; } public string Username { get; set; } public string Password { get; set; } } } We also need to modify the MobileServiceContext.cs file so that the database table is included in the Entity Framework context: public class MobileServiceContext : DbContext { private const string connectionStringName = \"Name=MS_TableConnectionString\"; public MobileServiceContext() : base(connectionStringName) { } public DbSet<TodoItem> TodoItems { get; set; } public DbSet<User> Users { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); } } Finally, we probably want to put some seed data into the database when it is first created so that we can test it. Adjust the MobileServiceInitializer in the Startup.MobileApp.cs file: protected override void Seed(MobileServiceContext context) { List<TodoItem> todoItems = new List<TodoItem> { new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"First item\", Complete = false }, new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"Second item\", Complete = false } }; foreach (TodoItem todoItem in todoItems) { context.Set<TodoItem>().Add(todoItem); } List<User> users = new List<User> { new User { Id = 1, Username = \"adrian\", Password = \"supersecret\" } }; foreach (User user in users) { context.Set<User>().Add(user); } base.Seed(context); } Note that we are storing the passwords in plain text. This is most definitely frowned upon. We should be using some sort of encryption. This code is most definitely just for demonstration purposes. Continuing the code on the backend, we need to handle the request to authenticate from the client. We will use a custom API controller for this; it is located in Controllers\\CustomAuthController.cs : using System; using System.IdentityModel.Tokens; using System.Linq; using System.Security.Claims; using System.Web.Http; using Backend.Models; using Microsoft.Azure.Mobile.Server.Login; using Newtonsoft.Json; namespace Backend.Controllers { [Route(\".auth/login/custom\")] public class CustomAuthController : ApiController { private MobileServiceContext db; private string signingKey, audience, issuer; public CustomAuthController() { db = new MobileServiceContext(); signingKey = Environment.GetEnvironmentVariable(\"WEBSITE_AUTH_SIGNING_KEY\"); var website = Environment.GetEnvironmentVariable(\"WEBSITE_HOSTNAME\"); audience = $\"https://{website}/\"; issuer = $\"https://{website}/\"; } [HttpPost] public IHttpActionResult Post([FromBody] User body) { if (body == null || body.Username == null || body.Password == null || body.Username.Length == 0 || body.Password.Length == 0) { return BadRequest(); ; } if (!IsValidUser(body)) { return Unauthorized(); } var claims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, body.Username) }; JwtSecurityToken token = AppServiceLoginHandler.CreateToken( claims, signingKey, audience, issuer, TimeSpan.FromDays(30)); return Ok(new LoginResult() { AuthenticationToken = token.RawData, User = new LoginResultUser { UserId = body.Username } }); } protected override void Dispose(bool disposing) { if (disposing) { db.Dispose(); } base.Dispose(disposing); } private bool IsValidUser(User user) { return db.Users.Count(u => u.Username.Equals(user.Username) && u.Password.Equals(user.Password)) > 0; } } public class LoginResult { [JsonProperty(PropertyName = \"authenticationToken\")] public string AuthenticationToken { get; set; } [JsonProperty(PropertyName = \"user\")] public LoginResultUser User { get; set; } } public class LoginResultUser { [JsonProperty(PropertyName = \"userId\")] public string UserId { get; set; } } } There is a lot going on here: The constructor reads the signing key and other information that we need for constructing the JWT. Note that the signing key is only available if you have the Authentication / Authorization is turned on. The LoginResult and LoginResultUser provide the response to the client, when serialized by the JSON serializer. The Post() method is where the work happens. It verifies that you have a valid object, then checks that the username and password match something in the user database. It then constructs the JWT and returns the required JSON object. The IsValidUser() method actually validates the username and password provided in the request with the users in the database. This version is very simplistic. I expect your version to at least include encryption of the password. Warn You must turn on Authentication / Authorization in your App Service. Set the Action to take when request is not authenticated to Allow Request (no action) and do not configure any of the supported authentication providers. You can add additional claims in the token that is passed back to the client by adding additional rows to the claims object. For example: var claims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, body.Username), new Claim(\"foo\", \"Value for Foo\") }; For example, you could do a custom authentication that includes group information, permissions structures, or additional information about the user from the directory. Claim names are normally three letters and the value is always a string. It is normal to create a class (just like the JwtRegisteredClaimNames ) with the strings in it that can be shared between the client and server projects: public static class LocalClaimNames { public string MainUser => \"mus\" }; The only claim that must be present is the \"sub\" claim (referenced here by JwtRegisteredClaimNames.Sub claim type). The token, when encoded, must fit in a HTTP header. For Windows systems based on IIS, the maximum size of a header is 16Kb. For Linux systems based on Apache, the maximum size of a header is 8Kb. The server will return 413 Entity Too Large if the header is too long. The token is also transmitted with every single request so you should make efforts to reduce the size of the token. It is better to make two requests initially (one request for the token followed by an authenticated request for the extra information) than to include the extra information in the token. Next, we need to wire the custom authentication controller so that it appears in the same place as all the other authenticators. We are going to access it via the /.auth/login/custom endpoint. The normal ASP.NET methods can be applied for this. In this project, we can enable attribute routing : public static void ConfigureMobileApp(IAppBuilder app) { HttpConfiguration config = new HttpConfiguration(); new MobileAppConfiguration() .AddTablesWithEntityFramework() .ApplyTo(config); // Map routes by attribute config.MapHttpAttributeRoutes(); // Use Entity Framework Code First to create database tables based on your DbContext Database.SetInitializer(new MobileServiceInitializer()); MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings(); if (string.IsNullOrEmpty(settings.HostName)) { app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions { SigningKey = ConfigurationManager.AppSettings[\"SigningKey\"], ValidAudiences = new[] { ConfigurationManager.AppSettings[\"ValidAudience\"] }, ValidIssuers = new[] { ConfigurationManager.AppSettings[\"ValidIssuer\"] }, TokenHandler = config.GetAppServiceTokenHandler() }); } app.UseWebApi(config); } At this point, we can deploy the backend to the App Service and send a suitably formed POST request to the backend. I use [Postman][19] for this purpose. The request: A successful POST will return the token and user ID in the response: Any other request (such as no body or a wrong username or password) should produce the right response. If the body is correct, but the information is wrong, then a 401 Unauthorized response should be produced. If the body is invalid, then 400 Bad Request should be produced. Info The format of the response is exactly the same as the token response we saw earlier when we were discussing the contents of a JWT. We can now turn our attention to the mobile client. Custom Authentication is always implemented using a client-flow mechanism. To implement this, we are going to adjust the entry page so that the username and password fields are displayed. The gathered username and password will then be passed to a new ICloudService LoginAsync() method. All of the UI work is done in the shared project. To start, we need a copy of the User.cs model from the backend project. Unlike Data Transfer Objects, this model is the same: namespace TaskList.Models { public class User { public string Username { get; set; } public string Password { get; set; } } } The abstraction we use for the cloud service needs to be adjusted so that we can pass the user object into the login method. This is the Abstractions\\ICloudService.cs interface: using System.Threading.Tasks; using TaskList.Models; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); Task LoginAsync(User user); } } I am adding a new version of the LoginAsync() method. The concrete version of this method no longer has to go through the dependency service since I can use shared code. Here is the definition of our new LoginAsync() method in Services\\AzureCloudService.cs : public Task LoginAsync(User user) { return client.LoginAsync(\"custom\", JObject.FromObject(user)); } Finally, we need to update the view-model ViewModels\\EntryPageViewModel.cs so that we can store the username and password in the model. We will also update the call to the LoginAsync() method of the cloud service so it calls our new method: using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class EntryPageViewModel : BaseViewModel { public EntryPageViewModel() { Title = \"Task List\"; User = new Models.User { Username = \"\", Password = \"\" }; } Command loginCmd; public Command LoginCommand => loginCmd ?? (loginCmd = new Command(async () => await ExecuteLoginCommand())); public Models.User User { get; set; } async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(User); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[ExecuteLoginCommand] Error = {ex.Message}\"); } finally { IsBusy = false; } } } } There are three new pieces here. Firstly, we have the User property (for holding the username and password in our form). Next, the constructor initializes the user object to an empty object. Finally, the call to LoginAsync() passes the user object to the cloud service. We also need some UI changes. Specifically, we need a couple of fields for the username and password added to the Pages\\EntryPage.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.EntryPage\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Vertical\" VerticalOptions=\"Center\"> <Label Text=\"Username?\" /> <Entry Text=\"{Binding User.Username}\" /> <Label Text=\"Password?\" /> <Entry IsPassword=\"True\" Text=\"{Binding User.Password}\" /> <Button BackgroundColor=\"Teal\" BorderRadius=\"10\" Command=\"{Binding LoginCommand}\" Text=\"Enter The App\" TextColor=\"White\" /> </StackLayout> </ContentPage.Content> </ContentPage> There is lots to complain about in this demonstration (including lack of encryption, storage of passwords, and a generally bad UI). However, it serves to demonstrate the salient points for using a (perhaps pre-existing) identity database for authentication of the users. Using Azure Active Directory B2C \u00b6 Custom authentication allows you to really customize the process, but I like to reduce the amount of code I write by using services or libraries. The whole sign-in and sign-up process is ripe for this sort of reduction. The code needed for building the sign-in / sign-up process is boiler-plate code. It also introduces problems that I have to deal with going forward. I have to store passwords and profile information, which introduces a security concern. I have to scale the database and ensure my app scales with it as my app gets popular. Finally, I am being fairly inflexible and causing potential privacy concerns with my users. There are a couple of services that I can use to get around these concerns. The first is an Azure service: Azure Active Directory B2C . The B2C stands for Business to Consumer. It is a mechanism by which you can add a sign-in and sign-up flow to your application. The user can enter a username or password, or, at your option, add on support for one or more social providers. In addition, there is support for branding the sign-in process, doing email verification of sign-ups and automatic password resets via email. The Azure AD B2C sign-in / sign-up process is primarily a server-flow process, so we will be able to add support in our app with just one line of code. The Minimal Setup of Azure AD B2C \u00b6 Azure AD is managed from the Classic Azure Portal , so start by logging in using your Azure Subscription credentials. Click the big + NEW button in the bottom left of the screen. Select App Services -> Active Directory -> Directory -> Custom Create . Choose a name for the tenant, then choose a unique domain name (which will appear in the onmicrosoft.com domain) and country. Ensure you check the This is a B2C directory. Click the tick to create the directory. As noted, this process will take a couple of minutes to complete. This creates a new tenant for you to manage. If you go back to your Azure Portal and click your name (top right corner), you will note that there is a new DIRECTORY entry for your B2C tenant. This is where you will be managing your B2C tenant. It's a good idea to pin the B2C settings blade to your dashboard or navigation pane so you can access it faster. To do this: Log in to the Azure Portal . Switch to your B2C tenant by clicking on your name, then selecting the new tenant in the drop-down. (The portal may ask you to re-confirm your ID and password) Click More services> in the left-hand navigation bar. Search for B2C . Click the empty star next to Azure AD B2C . This will make Azure AD B2C appear in your left hand navigation bar. To place it on the dashboard, click on Azure AD B2C in the left hand navigation bar, then click the pin at the top of the AZURE AD B2C SETTINGS blade. We also need to link the B2C tenant to an Azure subscription that can be billed. If you see an orange banner across the top of the Azure AD BC Settings, then click it to find the simple 3-step process to link the service. Once that is done, return to the the B2C tenant. Warn The process for creating a B2C tenant may change over time. If you find these instructions don't work, consult the official documentation on docs.microsoft.com . The next job is to create an application registration within the B2C tenant: Open the Azure AD B2C from your dashboard or the left hand navigation. In the Settings blade, click Applications . Click + ADD to add a new application. In the New application blade: Enter a unique name for the application. Click Yes under Include web app / web API . In the Reply URL, enter https://yoursite.azurewebsites.net/.auth/login/aad/callback . Click OK . There is no spinner or deployment here. After approximately 5-10 seconds, the application registration will appear in the list. Click the application registration to see the Application ID : We will also need an App Key. Click Keys . Click + Generate Key . Click Save . The new App Key will be generated and the display updated. Copy the key that has been generated before you leave this blade as it cannot be re-displayed. The next time you enter this blade, the secret will be obscured with no way of displaying it. We will need the Application ID and App Key later on. The next step is to create a Sign-in/Sign-up policy. We'll create a policy for signing up with an email address and email confirmation, then signing in with that email address. Close all the blades out to the Settings blade for the B2C tenant, then: In the Settings blade, click Sign-up or sign-in policies . Click the + Add button. Give the policy a name, like emailPolicy . Click Identity providers : Click Email signup / Local Account (a tick will appear next to the row). Click OK . Click Sign-up attributes : Click Email Address and any other fields you want to gather. Click OK . Click Application claims : Click Email Addresses and any other fields you want to provide to the application. Click OK Click Create on the Add policy blade. Click the policy you just created. It will be named something like B2C_1_emailPolicy . Make a note of the Metadata Endpoint for this policy . Now that your B2C tenant is configured, you can switch back to your original tenant (by clicking on your name in the top-right corner and selecting the default directory). To configure the App Service Authentication / Authorization . Open up the Settings blade, then Authentication / Authorization . Ensure the authentication service is turned on. Click on Azure Active Directory . This time, we are going to select the Advanced option. The Client ID is the application ID of your B2C application registration, and the Issuer Url is the Metadata Endpoint for your sign-up policy: Click OK to configure the authentication server flow, the Save to save the settings. As before, you can test your server flow by pointing your browser to https://yoursite.azurewebsites.net/.auth/login/aad : If you have done everything right, you should be able to register an account, get the email verification code, and finally log in to get the happy login page. All that is left to do is to configure your app for Azure Active Directory Server Flow. We did that earlier when discussing the Enterprise Authentication flow for the mobile client. Drawbacks of Azure Active Directory B2C \u00b6 Azure AD B2C is great for storing your users passwords and doing the sign-up and sign-in process for you. There are a couple of reasons why you wouldn't want to use Azure Active Directory B2C. The most obvious one is that this is built on Azure Active Directory. That means you won't be able to, for example, integrate the Facebook, Google and Twitter identity providers by utilizing their client libraries. You also do not get access to the underlying identity provider token, so you are restricted from accessing the Graph API for the individual providers. Finally, since the AAD B2C identity provider is configured with the AAD provider, you can't use both a B2C provider and a regular AAD provider. If you just want a sign-up / sign-in flow, then AAD B2C is probably the best way to go. If, however, your plans include integration with other social identity providers, you should consider using the identity providers directly or via separate configuration with the Azure App Service Authentication / Authorization. Finally, you cannot use a \"client-flow\" for Azure Active Directory B2C when using it in combination with Azure Mobile Apps. The Azure Mobile Apps will only accept a token from the ADAL library (as we described in the Active Directory section), and Azure Active Directory B2C requires authentication with MSAL (a newer library). We can happily work with server-flow. Using Third Party Tokens \u00b6 The final method of authenticating a user we are going to look at is a process by which you use a third party authentication token. For example, you may want to authenticate via GitHub or miiCard or using an authentication provider like Auth0 to get some single sign-in capabilities. Authentication with third party tokens works remarkably similar to the custom authentication case. Instead of a username and password, you pass in the token from the other provider. To look at this in example form, we are going to implement Auth0 as a provider. Your first stop should be the Auth0 web site to sign up for a developer account. Once you have done that: Click the + NEW CLIENT button in the Dashboard . Give your app a name, then click Native and then CREATE . Click the Xamarin icon to get the Xamarin Quickstart. Click Settings . Enter the callback URL in the Allowed Callback URLs . The callback URL will be something like https://_youraccount_.auth0.com/mobile and will be listed in the Quickstart page. Scroll down to the bottom of the page and click SAVE CHANGES . Make a note of the Client ID of the application. You will need it later. Click Connections . Turn on any connections that you want to use. For this example, ensure you turn on the Username-Password-Authentication and a couple of social providers. Now that the Auth0 service is configured, we can turn our attention to the mobile client. The Xamarin.Auth0Client is a component, so right-click the Components node of a platform project and select Get More Components... . In the dialog, find the Auth0 SDK , then click Add to App . For our iOS application, we are going to integrate Auth0 into the Services\\iOSLoginProvider.cs : public async Task LoginAsync(MobileServiceClient client) { // Client Flow var accessToken = await LoginAuth0Async(); var zumoPayload = new JObject(); zumoPayload[\"access_token\"] = accessToken; await client.LoginAsync(\"auth0\", zumoPayload); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; public async Task<string> LoginAuth0Async() { var auth0 = new Auth0.SDK.Auth0Client( \"shellmonger.auth0.com\", \"lmFp5jXnwPpD9lQIYwgwwPmFeofuLpYq\"); var user = await auth0.LoginAsync(RootView, scope: \"openid email\"); return user.IdToken; } The parameters for the constructor to the Auth0Client are your Auth0 domain and client ID. You can retrieve these from the Auth0 management page for your app. Note that I am requesting the email address. This will become a part of my ZUMO token when I create it. Switching our attention to our Backend project, we need a new custom authentication controller. This is located in Controllers\\Auth0Controller.cs : using System; using System.Diagnostics; using System.IdentityModel.Tokens; using System.Linq; using System.Security.Claims; using System.Web.Http; using Backend.Models; using Microsoft.Azure.Mobile.Server.Login; namespace Backend.Controllers { [Route(\".auth/login/auth0\")] public class Auth0Controller : ApiController { private JwtSecurityTokenHandler tokenHandler; private string clientID, domain; private string signingKey, audience, issuer; public Auth0Controller() { // Information for the incoming Auth0 Token domain = Environment.GetEnvironmentVariable(\"AUTH0_DOMAIN\"); clientID = Environment.GetEnvironmentVariable(\"AUTH0_CLIENTID\"); // Information for the outgoing ZUMO Token signingKey = Environment.GetEnvironmentVariable(\"WEBSITE_AUTH_SIGNING_KEY\"); var website = Environment.GetEnvironmentVariable(\"WEBSITE_HOSTNAME\"); audience = $\"https://{website}/\"; issuer = $\"https://{website}/\"; // Token Handler tokenHandler = new JwtSecurityTokenHandler(); } [HttpPost] public IHttpActionResult Post([FromBody] Auth0User body) { if (body == null || body.access_token == null || body.access_token.Length == 0) { return BadRequest(); } try { var token = (JwtSecurityToken)tokenHandler.ReadToken(body.access_token); if (!IsValidUser(token)) { return Unauthorized(); } var subject = token.Claims.FirstOrDefault(c => c.Type.Equals(\"sub\"))?.Value; var email = token.Claims.FirstOrDefault(c => c.Type.Equals(\"email\"))?.Value; if (subject == null || email == null) { return BadRequest(); } var claims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, subject), new Claim(JwtRegisteredClaimNames.Email, email) }; JwtSecurityToken zumoToken = AppServiceLoginHandler.CreateToken( claims, signingKey, audience, issuer, TimeSpan.FromDays(30)); return Ok(new LoginResult() { AuthenticationToken = zumoToken.RawData, User = new LoginResultUser { UserId = email } }); } catch (Exception ex) { Debug.WriteLine($\"Auth0 JWT Exception = {ex.Message}\"); throw ex; } } private bool IsValidUser(JwtSecurityToken token) { if (token == null) return false; var audience = token.Audiences.FirstOrDefault(); if (!audience.Equals(clientID)) return false; if (!token.Issuer.Equals($\"https://{domain}/\")) return false; if (token.ValidTo.AddMinutes(5) < DateTime.Now) return false; return true; } } public class Auth0User { public string access_token { get; set; } } } Note that we are reading two new environment variables. In the Azure App Service, you can read Application Settings by reading the environment variable of the same name. We need to set the AUTH0_CLIENTID to the Client ID of our Auth0 application, and the AUTH0_DOMAIN to the domain of our account. Both of these values need to match the settings in the client. These are not \"secure items\". If using the client secret (to validate the token), then that would be considered secure and should only appear on the server side. The validation is that the token passed in is valid (i.e. it has the right audience, issuer and expiry times). In addition, you should check the validity of the token signature. You can do this by acquiring the token secret and using tokenHandler.ValidateToken() instead of tokenHandler.ReadToken() . My new token lasts for 30 days. The ZUMO token that is generated in custom authentication does not have to be the same length as the original token. You can make it last for as long as you like.","title":"Custom Authentication"},{"location":"chapter2/custom/#custom-authentication","text":"For some situations, the social or enterprise flows are not valid for the mobile client. Perhaps you want the ability to provide a sign-up process with a username and password rather than using a social provider. Perhaps you want to use an alternate provider that is not one of the supported five providers. Whatever the reason, Azure App Service provides the ability to handle all situations. In this section, I will look at three methods for providing a unique set of usernames with no connection to the social or enterprise authentication.","title":"Custom authentication"},{"location":"chapter2/custom/#using-an-identity-database","text":"Probably the most common request is to use a custom identity database. In general, this is desirable because you already have a database of usernames and password. However, it's probably the least desirable option because of the security concerns that come along with this technique. The news is rife with password leakage for very large organizations. The best way to ensure you do not disclose a users password is to not have it in the first place. Warn I'm not going to cover the sign-up case here. This would be an additional process and would use a regular Web API to insert data into the database after validation (and probably verification via email or text message). The first thing we need to add to our project is a model for the user object. I created the following in the Models folder of the Backend project: using System.ComponentModel.DataAnnotations; namespace Backend.Models { public class User { [Key] public int Id { get; set; } public string Username { get; set; } public string Password { get; set; } } } We also need to modify the MobileServiceContext.cs file so that the database table is included in the Entity Framework context: public class MobileServiceContext : DbContext { private const string connectionStringName = \"Name=MS_TableConnectionString\"; public MobileServiceContext() : base(connectionStringName) { } public DbSet<TodoItem> TodoItems { get; set; } public DbSet<User> Users { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); } } Finally, we probably want to put some seed data into the database when it is first created so that we can test it. Adjust the MobileServiceInitializer in the Startup.MobileApp.cs file: protected override void Seed(MobileServiceContext context) { List<TodoItem> todoItems = new List<TodoItem> { new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"First item\", Complete = false }, new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"Second item\", Complete = false } }; foreach (TodoItem todoItem in todoItems) { context.Set<TodoItem>().Add(todoItem); } List<User> users = new List<User> { new User { Id = 1, Username = \"adrian\", Password = \"supersecret\" } }; foreach (User user in users) { context.Set<User>().Add(user); } base.Seed(context); } Note that we are storing the passwords in plain text. This is most definitely frowned upon. We should be using some sort of encryption. This code is most definitely just for demonstration purposes. Continuing the code on the backend, we need to handle the request to authenticate from the client. We will use a custom API controller for this; it is located in Controllers\\CustomAuthController.cs : using System; using System.IdentityModel.Tokens; using System.Linq; using System.Security.Claims; using System.Web.Http; using Backend.Models; using Microsoft.Azure.Mobile.Server.Login; using Newtonsoft.Json; namespace Backend.Controllers { [Route(\".auth/login/custom\")] public class CustomAuthController : ApiController { private MobileServiceContext db; private string signingKey, audience, issuer; public CustomAuthController() { db = new MobileServiceContext(); signingKey = Environment.GetEnvironmentVariable(\"WEBSITE_AUTH_SIGNING_KEY\"); var website = Environment.GetEnvironmentVariable(\"WEBSITE_HOSTNAME\"); audience = $\"https://{website}/\"; issuer = $\"https://{website}/\"; } [HttpPost] public IHttpActionResult Post([FromBody] User body) { if (body == null || body.Username == null || body.Password == null || body.Username.Length == 0 || body.Password.Length == 0) { return BadRequest(); ; } if (!IsValidUser(body)) { return Unauthorized(); } var claims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, body.Username) }; JwtSecurityToken token = AppServiceLoginHandler.CreateToken( claims, signingKey, audience, issuer, TimeSpan.FromDays(30)); return Ok(new LoginResult() { AuthenticationToken = token.RawData, User = new LoginResultUser { UserId = body.Username } }); } protected override void Dispose(bool disposing) { if (disposing) { db.Dispose(); } base.Dispose(disposing); } private bool IsValidUser(User user) { return db.Users.Count(u => u.Username.Equals(user.Username) && u.Password.Equals(user.Password)) > 0; } } public class LoginResult { [JsonProperty(PropertyName = \"authenticationToken\")] public string AuthenticationToken { get; set; } [JsonProperty(PropertyName = \"user\")] public LoginResultUser User { get; set; } } public class LoginResultUser { [JsonProperty(PropertyName = \"userId\")] public string UserId { get; set; } } } There is a lot going on here: The constructor reads the signing key and other information that we need for constructing the JWT. Note that the signing key is only available if you have the Authentication / Authorization is turned on. The LoginResult and LoginResultUser provide the response to the client, when serialized by the JSON serializer. The Post() method is where the work happens. It verifies that you have a valid object, then checks that the username and password match something in the user database. It then constructs the JWT and returns the required JSON object. The IsValidUser() method actually validates the username and password provided in the request with the users in the database. This version is very simplistic. I expect your version to at least include encryption of the password. Warn You must turn on Authentication / Authorization in your App Service. Set the Action to take when request is not authenticated to Allow Request (no action) and do not configure any of the supported authentication providers. You can add additional claims in the token that is passed back to the client by adding additional rows to the claims object. For example: var claims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, body.Username), new Claim(\"foo\", \"Value for Foo\") }; For example, you could do a custom authentication that includes group information, permissions structures, or additional information about the user from the directory. Claim names are normally three letters and the value is always a string. It is normal to create a class (just like the JwtRegisteredClaimNames ) with the strings in it that can be shared between the client and server projects: public static class LocalClaimNames { public string MainUser => \"mus\" }; The only claim that must be present is the \"sub\" claim (referenced here by JwtRegisteredClaimNames.Sub claim type). The token, when encoded, must fit in a HTTP header. For Windows systems based on IIS, the maximum size of a header is 16Kb. For Linux systems based on Apache, the maximum size of a header is 8Kb. The server will return 413 Entity Too Large if the header is too long. The token is also transmitted with every single request so you should make efforts to reduce the size of the token. It is better to make two requests initially (one request for the token followed by an authenticated request for the extra information) than to include the extra information in the token. Next, we need to wire the custom authentication controller so that it appears in the same place as all the other authenticators. We are going to access it via the /.auth/login/custom endpoint. The normal ASP.NET methods can be applied for this. In this project, we can enable attribute routing : public static void ConfigureMobileApp(IAppBuilder app) { HttpConfiguration config = new HttpConfiguration(); new MobileAppConfiguration() .AddTablesWithEntityFramework() .ApplyTo(config); // Map routes by attribute config.MapHttpAttributeRoutes(); // Use Entity Framework Code First to create database tables based on your DbContext Database.SetInitializer(new MobileServiceInitializer()); MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings(); if (string.IsNullOrEmpty(settings.HostName)) { app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions { SigningKey = ConfigurationManager.AppSettings[\"SigningKey\"], ValidAudiences = new[] { ConfigurationManager.AppSettings[\"ValidAudience\"] }, ValidIssuers = new[] { ConfigurationManager.AppSettings[\"ValidIssuer\"] }, TokenHandler = config.GetAppServiceTokenHandler() }); } app.UseWebApi(config); } At this point, we can deploy the backend to the App Service and send a suitably formed POST request to the backend. I use [Postman][19] for this purpose. The request: A successful POST will return the token and user ID in the response: Any other request (such as no body or a wrong username or password) should produce the right response. If the body is correct, but the information is wrong, then a 401 Unauthorized response should be produced. If the body is invalid, then 400 Bad Request should be produced. Info The format of the response is exactly the same as the token response we saw earlier when we were discussing the contents of a JWT. We can now turn our attention to the mobile client. Custom Authentication is always implemented using a client-flow mechanism. To implement this, we are going to adjust the entry page so that the username and password fields are displayed. The gathered username and password will then be passed to a new ICloudService LoginAsync() method. All of the UI work is done in the shared project. To start, we need a copy of the User.cs model from the backend project. Unlike Data Transfer Objects, this model is the same: namespace TaskList.Models { public class User { public string Username { get; set; } public string Password { get; set; } } } The abstraction we use for the cloud service needs to be adjusted so that we can pass the user object into the login method. This is the Abstractions\\ICloudService.cs interface: using System.Threading.Tasks; using TaskList.Models; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); Task LoginAsync(User user); } } I am adding a new version of the LoginAsync() method. The concrete version of this method no longer has to go through the dependency service since I can use shared code. Here is the definition of our new LoginAsync() method in Services\\AzureCloudService.cs : public Task LoginAsync(User user) { return client.LoginAsync(\"custom\", JObject.FromObject(user)); } Finally, we need to update the view-model ViewModels\\EntryPageViewModel.cs so that we can store the username and password in the model. We will also update the call to the LoginAsync() method of the cloud service so it calls our new method: using System; using System.Diagnostics; using System.Threading.Tasks; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.Models; using Xamarin.Forms; namespace TaskList.ViewModels { public class EntryPageViewModel : BaseViewModel { public EntryPageViewModel() { Title = \"Task List\"; User = new Models.User { Username = \"\", Password = \"\" }; } Command loginCmd; public Command LoginCommand => loginCmd ?? (loginCmd = new Command(async () => await ExecuteLoginCommand())); public Models.User User { get; set; } async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(User); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[ExecuteLoginCommand] Error = {ex.Message}\"); } finally { IsBusy = false; } } } } There are three new pieces here. Firstly, we have the User property (for holding the username and password in our form). Next, the constructor initializes the user object to an empty object. Finally, the call to LoginAsync() passes the user object to the cloud service. We also need some UI changes. Specifically, we need a couple of fields for the username and password added to the Pages\\EntryPage.xaml file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.EntryPage\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Vertical\" VerticalOptions=\"Center\"> <Label Text=\"Username?\" /> <Entry Text=\"{Binding User.Username}\" /> <Label Text=\"Password?\" /> <Entry IsPassword=\"True\" Text=\"{Binding User.Password}\" /> <Button BackgroundColor=\"Teal\" BorderRadius=\"10\" Command=\"{Binding LoginCommand}\" Text=\"Enter The App\" TextColor=\"White\" /> </StackLayout> </ContentPage.Content> </ContentPage> There is lots to complain about in this demonstration (including lack of encryption, storage of passwords, and a generally bad UI). However, it serves to demonstrate the salient points for using a (perhaps pre-existing) identity database for authentication of the users.","title":"Using an Identity Database."},{"location":"chapter2/custom/#using-azure-active-directory-b2c","text":"Custom authentication allows you to really customize the process, but I like to reduce the amount of code I write by using services or libraries. The whole sign-in and sign-up process is ripe for this sort of reduction. The code needed for building the sign-in / sign-up process is boiler-plate code. It also introduces problems that I have to deal with going forward. I have to store passwords and profile information, which introduces a security concern. I have to scale the database and ensure my app scales with it as my app gets popular. Finally, I am being fairly inflexible and causing potential privacy concerns with my users. There are a couple of services that I can use to get around these concerns. The first is an Azure service: Azure Active Directory B2C . The B2C stands for Business to Consumer. It is a mechanism by which you can add a sign-in and sign-up flow to your application. The user can enter a username or password, or, at your option, add on support for one or more social providers. In addition, there is support for branding the sign-in process, doing email verification of sign-ups and automatic password resets via email. The Azure AD B2C sign-in / sign-up process is primarily a server-flow process, so we will be able to add support in our app with just one line of code.","title":"Using Azure Active Directory B2C"},{"location":"chapter2/custom/#the-minimal-setup-of-azure-ad-b2c","text":"Azure AD is managed from the Classic Azure Portal , so start by logging in using your Azure Subscription credentials. Click the big + NEW button in the bottom left of the screen. Select App Services -> Active Directory -> Directory -> Custom Create . Choose a name for the tenant, then choose a unique domain name (which will appear in the onmicrosoft.com domain) and country. Ensure you check the This is a B2C directory. Click the tick to create the directory. As noted, this process will take a couple of minutes to complete. This creates a new tenant for you to manage. If you go back to your Azure Portal and click your name (top right corner), you will note that there is a new DIRECTORY entry for your B2C tenant. This is where you will be managing your B2C tenant. It's a good idea to pin the B2C settings blade to your dashboard or navigation pane so you can access it faster. To do this: Log in to the Azure Portal . Switch to your B2C tenant by clicking on your name, then selecting the new tenant in the drop-down. (The portal may ask you to re-confirm your ID and password) Click More services> in the left-hand navigation bar. Search for B2C . Click the empty star next to Azure AD B2C . This will make Azure AD B2C appear in your left hand navigation bar. To place it on the dashboard, click on Azure AD B2C in the left hand navigation bar, then click the pin at the top of the AZURE AD B2C SETTINGS blade. We also need to link the B2C tenant to an Azure subscription that can be billed. If you see an orange banner across the top of the Azure AD BC Settings, then click it to find the simple 3-step process to link the service. Once that is done, return to the the B2C tenant. Warn The process for creating a B2C tenant may change over time. If you find these instructions don't work, consult the official documentation on docs.microsoft.com . The next job is to create an application registration within the B2C tenant: Open the Azure AD B2C from your dashboard or the left hand navigation. In the Settings blade, click Applications . Click + ADD to add a new application. In the New application blade: Enter a unique name for the application. Click Yes under Include web app / web API . In the Reply URL, enter https://yoursite.azurewebsites.net/.auth/login/aad/callback . Click OK . There is no spinner or deployment here. After approximately 5-10 seconds, the application registration will appear in the list. Click the application registration to see the Application ID : We will also need an App Key. Click Keys . Click + Generate Key . Click Save . The new App Key will be generated and the display updated. Copy the key that has been generated before you leave this blade as it cannot be re-displayed. The next time you enter this blade, the secret will be obscured with no way of displaying it. We will need the Application ID and App Key later on. The next step is to create a Sign-in/Sign-up policy. We'll create a policy for signing up with an email address and email confirmation, then signing in with that email address. Close all the blades out to the Settings blade for the B2C tenant, then: In the Settings blade, click Sign-up or sign-in policies . Click the + Add button. Give the policy a name, like emailPolicy . Click Identity providers : Click Email signup / Local Account (a tick will appear next to the row). Click OK . Click Sign-up attributes : Click Email Address and any other fields you want to gather. Click OK . Click Application claims : Click Email Addresses and any other fields you want to provide to the application. Click OK Click Create on the Add policy blade. Click the policy you just created. It will be named something like B2C_1_emailPolicy . Make a note of the Metadata Endpoint for this policy . Now that your B2C tenant is configured, you can switch back to your original tenant (by clicking on your name in the top-right corner and selecting the default directory). To configure the App Service Authentication / Authorization . Open up the Settings blade, then Authentication / Authorization . Ensure the authentication service is turned on. Click on Azure Active Directory . This time, we are going to select the Advanced option. The Client ID is the application ID of your B2C application registration, and the Issuer Url is the Metadata Endpoint for your sign-up policy: Click OK to configure the authentication server flow, the Save to save the settings. As before, you can test your server flow by pointing your browser to https://yoursite.azurewebsites.net/.auth/login/aad : If you have done everything right, you should be able to register an account, get the email verification code, and finally log in to get the happy login page. All that is left to do is to configure your app for Azure Active Directory Server Flow. We did that earlier when discussing the Enterprise Authentication flow for the mobile client.","title":"The Minimal Setup of Azure AD B2C"},{"location":"chapter2/custom/#drawbacks-of-azure-active-directory-b2c","text":"Azure AD B2C is great for storing your users passwords and doing the sign-up and sign-in process for you. There are a couple of reasons why you wouldn't want to use Azure Active Directory B2C. The most obvious one is that this is built on Azure Active Directory. That means you won't be able to, for example, integrate the Facebook, Google and Twitter identity providers by utilizing their client libraries. You also do not get access to the underlying identity provider token, so you are restricted from accessing the Graph API for the individual providers. Finally, since the AAD B2C identity provider is configured with the AAD provider, you can't use both a B2C provider and a regular AAD provider. If you just want a sign-up / sign-in flow, then AAD B2C is probably the best way to go. If, however, your plans include integration with other social identity providers, you should consider using the identity providers directly or via separate configuration with the Azure App Service Authentication / Authorization. Finally, you cannot use a \"client-flow\" for Azure Active Directory B2C when using it in combination with Azure Mobile Apps. The Azure Mobile Apps will only accept a token from the ADAL library (as we described in the Active Directory section), and Azure Active Directory B2C requires authentication with MSAL (a newer library). We can happily work with server-flow.","title":"Drawbacks of Azure Active Directory B2C"},{"location":"chapter2/custom/#using-third-party-tokens","text":"The final method of authenticating a user we are going to look at is a process by which you use a third party authentication token. For example, you may want to authenticate via GitHub or miiCard or using an authentication provider like Auth0 to get some single sign-in capabilities. Authentication with third party tokens works remarkably similar to the custom authentication case. Instead of a username and password, you pass in the token from the other provider. To look at this in example form, we are going to implement Auth0 as a provider. Your first stop should be the Auth0 web site to sign up for a developer account. Once you have done that: Click the + NEW CLIENT button in the Dashboard . Give your app a name, then click Native and then CREATE . Click the Xamarin icon to get the Xamarin Quickstart. Click Settings . Enter the callback URL in the Allowed Callback URLs . The callback URL will be something like https://_youraccount_.auth0.com/mobile and will be listed in the Quickstart page. Scroll down to the bottom of the page and click SAVE CHANGES . Make a note of the Client ID of the application. You will need it later. Click Connections . Turn on any connections that you want to use. For this example, ensure you turn on the Username-Password-Authentication and a couple of social providers. Now that the Auth0 service is configured, we can turn our attention to the mobile client. The Xamarin.Auth0Client is a component, so right-click the Components node of a platform project and select Get More Components... . In the dialog, find the Auth0 SDK , then click Add to App . For our iOS application, we are going to integrate Auth0 into the Services\\iOSLoginProvider.cs : public async Task LoginAsync(MobileServiceClient client) { // Client Flow var accessToken = await LoginAuth0Async(); var zumoPayload = new JObject(); zumoPayload[\"access_token\"] = accessToken; await client.LoginAsync(\"auth0\", zumoPayload); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; public async Task<string> LoginAuth0Async() { var auth0 = new Auth0.SDK.Auth0Client( \"shellmonger.auth0.com\", \"lmFp5jXnwPpD9lQIYwgwwPmFeofuLpYq\"); var user = await auth0.LoginAsync(RootView, scope: \"openid email\"); return user.IdToken; } The parameters for the constructor to the Auth0Client are your Auth0 domain and client ID. You can retrieve these from the Auth0 management page for your app. Note that I am requesting the email address. This will become a part of my ZUMO token when I create it. Switching our attention to our Backend project, we need a new custom authentication controller. This is located in Controllers\\Auth0Controller.cs : using System; using System.Diagnostics; using System.IdentityModel.Tokens; using System.Linq; using System.Security.Claims; using System.Web.Http; using Backend.Models; using Microsoft.Azure.Mobile.Server.Login; namespace Backend.Controllers { [Route(\".auth/login/auth0\")] public class Auth0Controller : ApiController { private JwtSecurityTokenHandler tokenHandler; private string clientID, domain; private string signingKey, audience, issuer; public Auth0Controller() { // Information for the incoming Auth0 Token domain = Environment.GetEnvironmentVariable(\"AUTH0_DOMAIN\"); clientID = Environment.GetEnvironmentVariable(\"AUTH0_CLIENTID\"); // Information for the outgoing ZUMO Token signingKey = Environment.GetEnvironmentVariable(\"WEBSITE_AUTH_SIGNING_KEY\"); var website = Environment.GetEnvironmentVariable(\"WEBSITE_HOSTNAME\"); audience = $\"https://{website}/\"; issuer = $\"https://{website}/\"; // Token Handler tokenHandler = new JwtSecurityTokenHandler(); } [HttpPost] public IHttpActionResult Post([FromBody] Auth0User body) { if (body == null || body.access_token == null || body.access_token.Length == 0) { return BadRequest(); } try { var token = (JwtSecurityToken)tokenHandler.ReadToken(body.access_token); if (!IsValidUser(token)) { return Unauthorized(); } var subject = token.Claims.FirstOrDefault(c => c.Type.Equals(\"sub\"))?.Value; var email = token.Claims.FirstOrDefault(c => c.Type.Equals(\"email\"))?.Value; if (subject == null || email == null) { return BadRequest(); } var claims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, subject), new Claim(JwtRegisteredClaimNames.Email, email) }; JwtSecurityToken zumoToken = AppServiceLoginHandler.CreateToken( claims, signingKey, audience, issuer, TimeSpan.FromDays(30)); return Ok(new LoginResult() { AuthenticationToken = zumoToken.RawData, User = new LoginResultUser { UserId = email } }); } catch (Exception ex) { Debug.WriteLine($\"Auth0 JWT Exception = {ex.Message}\"); throw ex; } } private bool IsValidUser(JwtSecurityToken token) { if (token == null) return false; var audience = token.Audiences.FirstOrDefault(); if (!audience.Equals(clientID)) return false; if (!token.Issuer.Equals($\"https://{domain}/\")) return false; if (token.ValidTo.AddMinutes(5) < DateTime.Now) return false; return true; } } public class Auth0User { public string access_token { get; set; } } } Note that we are reading two new environment variables. In the Azure App Service, you can read Application Settings by reading the environment variable of the same name. We need to set the AUTH0_CLIENTID to the Client ID of our Auth0 application, and the AUTH0_DOMAIN to the domain of our account. Both of these values need to match the settings in the client. These are not \"secure items\". If using the client secret (to validate the token), then that would be considered secure and should only appear on the server side. The validation is that the token passed in is valid (i.e. it has the right audience, issuer and expiry times). In addition, you should check the validity of the token signature. You can do this by acquiring the token secret and using tokenHandler.ValidateToken() instead of tokenHandler.ReadToken() . My new token lasts for 30 days. The ZUMO token that is generated in custom authentication does not have to be the same length as the original token. You can make it last for as long as you like.","title":"Using Third Party Tokens"},{"location":"chapter2/debugging/","text":"What is in a JWT \u00b6 At this point you will have the \"Authentication Success\" screen - perhaps several times. If you bring up the Developer Tools for your browser, you can take a look at the token that is being minted for the authentication session. Take a look at the URL on the \"successful authentication\" page. The authentication token is clearly marked (after you strip away the URL encoding). You can use a URL Decoder / Encoder - just cut and paste the entire URL into the box and click on Decode . Note that the token is actually a JSON object. You can now easily extract the authenticationToken field from the JSON object. Technically, the authentication token is a JSON Web Token . This is a mechanism for transferring claims between two systems securely. The JWT is a cryptographically signed JSON object. You can decode the JWT using the jwt.io tool . Cut and paste the authentication token into the Encoded box and it will be decoded. Note that the contents of the JWT are revealed even without knowing the secret. The client secret is generated (or entered as part of a configuration) at the identity provider and is used to cryptographically sign the token. You will need the client secret to verify the signature of the token. The client secret is copied from the identity provider to the resource (in this case, the App Service). We can see other items within the token. The Issuer is the place that issued the token. This is generally a URI. The Audience is an identifier for who the token is for. In this case, we have a token minted by the Azure App Service for use accessing that same App Service, so the issuer and audience are both the URI. If you look at an Auth0 token, you will see that the issuer is the Auth0 domain and the audience is the Client ID of the Auth0 tenant. Each token will also have a number of claims. The most common claim is the Subject of the token. This is generally a security ID, but could be any unique user ID. Info Azure App Service sets the subject to a stable SID. The stable SID is unique to the identity provider that is used for the authentication and guaranteed not to change, even if the user changes their email address or username on the underlying identity provider. The JWT can include any data from the identity provider and there are some identity providers that place just about everything about the user in the JWT. App Service keeps the amount of data small because the client will be sending the JWT with every request. Imagine adding a few kilobytes to every single request that the client makes. The bandwidth usage will add up quickly, and your app will be known as a bandwidth hog. However, there are some fields that are pretty universal. Your JWT should always have the following fields: sub = Subject (the identifier for the token) exp = Expiry (when the token expires) nbf = Not Before (the earliest point in time the token is valid) iss = Issuer (the site that issued the token) aud = Audience (who is the token for) The timestamps (exp and nbf) are all UNIX timestamps (i.e. the number of seconds since January 1, 1970). App Service adds to this: stable_sid = Security Id of the user idp = the IdP that was used in the authentication request ver = the Version of the token App Service will be able to validate any token provided to it when presented in an X-ZUMO-AUTH header. If you are using Azure Active Directory, you can also use the more standard Bearer Authorization header. If the token does not match, then the X-ZUMO-AUTH header will be stripped from the request before the request is passed to your site. Testing Authentication without a Client \u00b6 Testing authentication against your App Service without a client requires a REST client. I use Postman , which is based on Google Chrome. If you use Firefox, you might want to take a look at RESTClient . Telerik also distributes a web debugging proxy called Fiddler that can do API testing. To test the server, we will need a token. We can get one by testing authentication configuration by pointing the browser to /.auth/login/aad . The return URL will contain a token in the query string and as a secure cookie . Tip You can test any of the supported identity providers by replacing aad with the authentication provider name: facebook , google , microsoftaccount and twitter are possibilities here. We can then do a request to /tables/todoitem to try and obtain the list of current tasks. We will need to add two headers: ZUMO-API-VERSION should contain a value of 2.0.0 . X-ZUMO-AUTH should contain the token you received. My first request shows authentication failing: Go through one of the authentication flows and copy the authentication token. In Postman, add a new header called X-ZUMO-AUTH and paste the authentication token in. Note that we have tested all this without touching the client. Separating the backend operations from the client operations means we can be sure of where the inevitable bug that creeps in is located. We have verified that we can do each authentication flow on the server side and that the server is properly rejecting unauthenticated requests, plus it is properly returning data when authenticated requests are issued. Developing Locally \u00b6 One would normally be able to run the ASP.NET backend locally and get full functionality without authentication. However, authentication puts a stop to that because the redirect URLs, secrets and other authentication configuration settings only work with a known endpoint. To alleviate that, Azure Mobile Apps allows you to run a local server while using an authentication endpoint in Azure App Service. When the authentication transaction takes place, it is taking place against the Azure App Service. When it is not doing the OAuth transaction, however, it is operating against a local server. Setting this up requires a little bit of local machine configuration and a change to the configuration of your client. Update your Local Development Environment \u00b6 The first step in this process is to make your local IIS development environment look more like the Azure App Service, particularly in reference to the authentication settings. This means setting up a few app settings that should be pulled from your App Service. Log on to the Azure Portal . Select your App Service from the App Services list. Click on Tools , then Advanced Tools , then Go . Kudu (now known as Advanced Tools in the Azure portal menu) is the backend debug console for Azure App Service and there is a lot you can do here. Of note in this instance is that you can gain access to the keys and audience for your App Service. Click on Environment in the top banner. Click on Environment variables . Scroll down to the environment variables starting with WEBSITE_AUTH . Make a note of the WEBSITE_AUTH_SIGNING_KEY and WEBSITE_HOSTNAME values. WEBSITE_AUTH_ALLOWED_AUDIENCES Setting The WEBSITE_AUTH_ALLOWED_AUDIENCES setting may be seen. This is set only in the case of Azure Active Directory and may not be present (or not valid) if you configure other providers. Add the following to your project Web.config <appSettings> section: <appSettings> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> <add key=\"SigningKey\" value=\"{Your WEBSITE_AUTH_SIGNING_KEY}\"/> <add key=\"ValidAudience\" value=\"https://{Your WEBSITE_HOSTNAME}/\"/> <add key=\"ValidIssuer\" value=\"https://{Your WEBSITE_HOSTNAME}/\"/> </appSettings> Tip Both the ValidAudience and ValidIssuer will have a slash on the end and be a https URL. The last three keys are the keys you will need to add. Make sure you do not have a HostName key as this is how the startup file determines if you are running locally or remote. Talking of which, edit your App_Start\\Startup.MobileApp.cs file to include the following: public static void ConfigureMobileApp(IAppBuilder app) { HttpConfiguration config = new HttpConfiguration(); new MobileAppConfiguration() .AddTablesWithEntityFramework() .ApplyTo(config); // Use Entity Framework Code First to create database tables based on your DbContext Database.SetInitializer(new MobileServiceInitializer()); MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings(); if (string.IsNullOrEmpty(settings.HostName)) { app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions { SigningKey = ConfigurationManager.AppSettings[\"SigningKey\"], ValidAudiences = new[] { ConfigurationManager.AppSettings[\"ValidAudience\"] }, ValidIssuers = new[] { ConfigurationManager.AppSettings[\"ValidIssuer\"] }, TokenHandler = config.GetAppServiceTokenHandler() }); } app.UseWebApi(config); } The UseAppServiceAuthentication() method sets up authentication checking. This section is not required when running within App Service. If you are running the server locally, you should either set up a local SQL Server instance and put the connection string into the Web.config file, or open the firewall on your SQL Azure database so that your local development environment can connect to it, then place the connection string in the Web.config . You can get the connection string of the SQL Azure instance by looking at the Connection Strings in the Application properties of your App Service. Update your Mobile Client \u00b6 For this demonstration, I have updated the TaskList.UWP application so that it is using the server-flow authentication for Azure Active Directory. This means updating the LoginAsync() method in the Services\\UWPLoginProvider.cs file to be the following: public async Task LoginAsync(MobileServiceClient client) { // Server-Flow Version await client.LoginAsync(\"aad\"); } This is because the default local IIS instance is IIS Express. IIS Express only listens for local connections. If you run a client from another device (for example, the Android emulator on a Hyper-V service or the iOS simulator on a Mac), then that client would be connecting via a network connection. You can still debug locally, but you need to convert your environment to IIS first. In the TaskList (Portable) project, update the Helpers\\Locations.cs file: namespace TaskList.Helpers { public static class Locations { #if DEBUG public static readonly string AppServiceUrl = \"http://localhost:17568/\"; public static readonly string AlternateLoginHost = \"https://the-book.azurewebsites.net\"; #else public static readonly string AppServiceUrl = \"https://the-book.azurewebsites.net\"; public static readonly string AlternateLoginHost = null; #endif } } The AppServiceUrl is always set to the location of your backend. In this case, I right-clicked on the Backend project and selected Properties then Web . The correct URL for local debugging is listed in the Project URL . The AlternateLoginHost is set to the App Service when locally debugging or null if not. You can specify the DEBUG constant in the Build tab. In the same project, update the Services\\AzureCloudService.cs constructor to the following: public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl); if (Locations.AlternateLoginHost != null) client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost); } Tip It's a good idea to separate the client and server into different solutions. Although it doesn't hurt anything to have them in the same solution (like we have), having the client and server separated allows you to attach a debugger separately - which allows you to debug both sides of the connection at the same time. With these settings, the client will contact the AlternateLoginHost listed for the authentication process and then contact the local server for the rest of the transaction. Run the Local Server \u00b6 Running the local server and the client takes a larger machine. You need to run two instances of Visual Studio: one for the client and one for the server. This is really where you will appreciate multiple monitors (my personal favorite) or the snap action to the sides of the screens. Ensure you have your backend and clients in different solutions if you intend to run both client and server. The debugger in Visual Studio will stop one to run the other when they are in the same solution.","title":"Debugging Authentication"},{"location":"chapter2/debugging/#what-is-in-a-jwt","text":"At this point you will have the \"Authentication Success\" screen - perhaps several times. If you bring up the Developer Tools for your browser, you can take a look at the token that is being minted for the authentication session. Take a look at the URL on the \"successful authentication\" page. The authentication token is clearly marked (after you strip away the URL encoding). You can use a URL Decoder / Encoder - just cut and paste the entire URL into the box and click on Decode . Note that the token is actually a JSON object. You can now easily extract the authenticationToken field from the JSON object. Technically, the authentication token is a JSON Web Token . This is a mechanism for transferring claims between two systems securely. The JWT is a cryptographically signed JSON object. You can decode the JWT using the jwt.io tool . Cut and paste the authentication token into the Encoded box and it will be decoded. Note that the contents of the JWT are revealed even without knowing the secret. The client secret is generated (or entered as part of a configuration) at the identity provider and is used to cryptographically sign the token. You will need the client secret to verify the signature of the token. The client secret is copied from the identity provider to the resource (in this case, the App Service). We can see other items within the token. The Issuer is the place that issued the token. This is generally a URI. The Audience is an identifier for who the token is for. In this case, we have a token minted by the Azure App Service for use accessing that same App Service, so the issuer and audience are both the URI. If you look at an Auth0 token, you will see that the issuer is the Auth0 domain and the audience is the Client ID of the Auth0 tenant. Each token will also have a number of claims. The most common claim is the Subject of the token. This is generally a security ID, but could be any unique user ID. Info Azure App Service sets the subject to a stable SID. The stable SID is unique to the identity provider that is used for the authentication and guaranteed not to change, even if the user changes their email address or username on the underlying identity provider. The JWT can include any data from the identity provider and there are some identity providers that place just about everything about the user in the JWT. App Service keeps the amount of data small because the client will be sending the JWT with every request. Imagine adding a few kilobytes to every single request that the client makes. The bandwidth usage will add up quickly, and your app will be known as a bandwidth hog. However, there are some fields that are pretty universal. Your JWT should always have the following fields: sub = Subject (the identifier for the token) exp = Expiry (when the token expires) nbf = Not Before (the earliest point in time the token is valid) iss = Issuer (the site that issued the token) aud = Audience (who is the token for) The timestamps (exp and nbf) are all UNIX timestamps (i.e. the number of seconds since January 1, 1970). App Service adds to this: stable_sid = Security Id of the user idp = the IdP that was used in the authentication request ver = the Version of the token App Service will be able to validate any token provided to it when presented in an X-ZUMO-AUTH header. If you are using Azure Active Directory, you can also use the more standard Bearer Authorization header. If the token does not match, then the X-ZUMO-AUTH header will be stripped from the request before the request is passed to your site.","title":"What is in a JWT"},{"location":"chapter2/debugging/#testing-authentication-without-a-client","text":"Testing authentication against your App Service without a client requires a REST client. I use Postman , which is based on Google Chrome. If you use Firefox, you might want to take a look at RESTClient . Telerik also distributes a web debugging proxy called Fiddler that can do API testing. To test the server, we will need a token. We can get one by testing authentication configuration by pointing the browser to /.auth/login/aad . The return URL will contain a token in the query string and as a secure cookie . Tip You can test any of the supported identity providers by replacing aad with the authentication provider name: facebook , google , microsoftaccount and twitter are possibilities here. We can then do a request to /tables/todoitem to try and obtain the list of current tasks. We will need to add two headers: ZUMO-API-VERSION should contain a value of 2.0.0 . X-ZUMO-AUTH should contain the token you received. My first request shows authentication failing: Go through one of the authentication flows and copy the authentication token. In Postman, add a new header called X-ZUMO-AUTH and paste the authentication token in. Note that we have tested all this without touching the client. Separating the backend operations from the client operations means we can be sure of where the inevitable bug that creeps in is located. We have verified that we can do each authentication flow on the server side and that the server is properly rejecting unauthenticated requests, plus it is properly returning data when authenticated requests are issued.","title":"Testing Authentication without a Client"},{"location":"chapter2/debugging/#developing-locally","text":"One would normally be able to run the ASP.NET backend locally and get full functionality without authentication. However, authentication puts a stop to that because the redirect URLs, secrets and other authentication configuration settings only work with a known endpoint. To alleviate that, Azure Mobile Apps allows you to run a local server while using an authentication endpoint in Azure App Service. When the authentication transaction takes place, it is taking place against the Azure App Service. When it is not doing the OAuth transaction, however, it is operating against a local server. Setting this up requires a little bit of local machine configuration and a change to the configuration of your client.","title":"Developing Locally"},{"location":"chapter2/debugging/#update-your-local-development-environment","text":"The first step in this process is to make your local IIS development environment look more like the Azure App Service, particularly in reference to the authentication settings. This means setting up a few app settings that should be pulled from your App Service. Log on to the Azure Portal . Select your App Service from the App Services list. Click on Tools , then Advanced Tools , then Go . Kudu (now known as Advanced Tools in the Azure portal menu) is the backend debug console for Azure App Service and there is a lot you can do here. Of note in this instance is that you can gain access to the keys and audience for your App Service. Click on Environment in the top banner. Click on Environment variables . Scroll down to the environment variables starting with WEBSITE_AUTH . Make a note of the WEBSITE_AUTH_SIGNING_KEY and WEBSITE_HOSTNAME values. WEBSITE_AUTH_ALLOWED_AUDIENCES Setting The WEBSITE_AUTH_ALLOWED_AUDIENCES setting may be seen. This is set only in the case of Azure Active Directory and may not be present (or not valid) if you configure other providers. Add the following to your project Web.config <appSettings> section: <appSettings> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> <add key=\"SigningKey\" value=\"{Your WEBSITE_AUTH_SIGNING_KEY}\"/> <add key=\"ValidAudience\" value=\"https://{Your WEBSITE_HOSTNAME}/\"/> <add key=\"ValidIssuer\" value=\"https://{Your WEBSITE_HOSTNAME}/\"/> </appSettings> Tip Both the ValidAudience and ValidIssuer will have a slash on the end and be a https URL. The last three keys are the keys you will need to add. Make sure you do not have a HostName key as this is how the startup file determines if you are running locally or remote. Talking of which, edit your App_Start\\Startup.MobileApp.cs file to include the following: public static void ConfigureMobileApp(IAppBuilder app) { HttpConfiguration config = new HttpConfiguration(); new MobileAppConfiguration() .AddTablesWithEntityFramework() .ApplyTo(config); // Use Entity Framework Code First to create database tables based on your DbContext Database.SetInitializer(new MobileServiceInitializer()); MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings(); if (string.IsNullOrEmpty(settings.HostName)) { app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions { SigningKey = ConfigurationManager.AppSettings[\"SigningKey\"], ValidAudiences = new[] { ConfigurationManager.AppSettings[\"ValidAudience\"] }, ValidIssuers = new[] { ConfigurationManager.AppSettings[\"ValidIssuer\"] }, TokenHandler = config.GetAppServiceTokenHandler() }); } app.UseWebApi(config); } The UseAppServiceAuthentication() method sets up authentication checking. This section is not required when running within App Service. If you are running the server locally, you should either set up a local SQL Server instance and put the connection string into the Web.config file, or open the firewall on your SQL Azure database so that your local development environment can connect to it, then place the connection string in the Web.config . You can get the connection string of the SQL Azure instance by looking at the Connection Strings in the Application properties of your App Service.","title":"Update your Local Development Environment"},{"location":"chapter2/debugging/#update-your-mobile-client","text":"For this demonstration, I have updated the TaskList.UWP application so that it is using the server-flow authentication for Azure Active Directory. This means updating the LoginAsync() method in the Services\\UWPLoginProvider.cs file to be the following: public async Task LoginAsync(MobileServiceClient client) { // Server-Flow Version await client.LoginAsync(\"aad\"); } This is because the default local IIS instance is IIS Express. IIS Express only listens for local connections. If you run a client from another device (for example, the Android emulator on a Hyper-V service or the iOS simulator on a Mac), then that client would be connecting via a network connection. You can still debug locally, but you need to convert your environment to IIS first. In the TaskList (Portable) project, update the Helpers\\Locations.cs file: namespace TaskList.Helpers { public static class Locations { #if DEBUG public static readonly string AppServiceUrl = \"http://localhost:17568/\"; public static readonly string AlternateLoginHost = \"https://the-book.azurewebsites.net\"; #else public static readonly string AppServiceUrl = \"https://the-book.azurewebsites.net\"; public static readonly string AlternateLoginHost = null; #endif } } The AppServiceUrl is always set to the location of your backend. In this case, I right-clicked on the Backend project and selected Properties then Web . The correct URL for local debugging is listed in the Project URL . The AlternateLoginHost is set to the App Service when locally debugging or null if not. You can specify the DEBUG constant in the Build tab. In the same project, update the Services\\AzureCloudService.cs constructor to the following: public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl); if (Locations.AlternateLoginHost != null) client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost); } Tip It's a good idea to separate the client and server into different solutions. Although it doesn't hurt anything to have them in the same solution (like we have), having the client and server separated allows you to attach a debugger separately - which allows you to debug both sides of the connection at the same time. With these settings, the client will contact the AlternateLoginHost listed for the authentication process and then contact the local server for the rest of the transaction.","title":"Update your Mobile Client"},{"location":"chapter2/debugging/#run-the-local-server","text":"Running the local server and the client takes a larger machine. You need to run two instances of Visual Studio: one for the client and one for the server. This is really where you will appreciate multiple monitors (my personal favorite) or the snap action to the sides of the screens. Ensure you have your backend and clients in different solutions if you intend to run both client and server. The debugger in Visual Studio will stop one to run the other when they are in the same solution.","title":"Run the Local Server"},{"location":"chapter2/enterprise/","text":"Enterprise Authentication \u00b6 Enterprise Authentication is handled by Azure Active Directory, which is fairly commonly configured within Azure. Every Azure subscription has a default directory associated with it that you can leverage for this section. In addition, if your organization has an Office 365 subscription, this will likely be tied to an Azure Active Directory domain to allow enterprise sign-in. In either case, you have a directory you can use for providing authentication to your app. In general, you will need to get special permissions to update the directory. If you want to use your organizations corporate directory, then you are likely to have to get your IT department involved to set it up. Azure Active Directory: Server-Flow setup \u00b6 The Azure Active Directory server-flow is perhaps the easiest of all the authentication methods to configure. No matter if you are doing a client flow or server flow, you need to set up the server flow first. Tip We recommend that you implement Client Flow in any non-trivial application. If you are using your default directory and you want to add a couple of test users, you will need to set those up first. Start by logging in to the Azure portal . In the left-hand menu, click More services . Enter Active Directory in the search box, then click Azure Active Directory . (Optional) If you need to manage a different directory to the default directory, click Switch directory and choose a different directory. Click Users and groups , then All users . Click Add . Fill in the information. Ensure you add the user to a group, if necessary. You can also add additional information (like a real name) in the Profile section. Check Show Password and make a note of the new password. Click Create . Repeat for each test user you wish to use. Once done, move onto configuring your App Service for authentication: Click All resources in the left hand menu. Click your App Service or Mobile App. Search for and click Authentication / Authorization (it's under SETTINGS). Change App Service Authentication to On . Ensure the Action to take when request is not authenticated is set to Allow Anonymous requests . Click Azure Active Directory . Click Express . All the information is filled in for you. Click OK , then Save . Info Make sure you create the app service in the right directory / subscription. If you have access to more than one directory, you can choose the right one by selecting it under your account drop-down in the top-right corner. There is also an Advanced track. This is used in client-flow situations and in situations where you have more than one directory. It's also used if you want to use the newer MSAL library for authentication. The Express flow is great for getting started quickly. You can walk through a server-flow authentication to test that you have all the settings correct. Point your browser at https:// yoursite .azurewebsites.net/.auth/login/aad. The browser will take you through an authentication flow before giving you a successful authentication image: If you get an error akin to \"We are having problems signing you in\", use an Incognito or InPrivate browsing window. Adding Authentication to a Mobile Client \u00b6 Now that the backend is completely configured, we can move our attention to the mobile client. We are going to be using the same mobile client that we developed in the first chapter, but we are now going to add authentication to it. Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought of this and provided a facility for running platform specific code called the DependencyService . Info If we run our application right now, clicking on the \"Enter the App\" button will result in an error. We will be able to see the Unauthorized error in the debug window of Visual Studio. Our first step is to define an Abstractions\\ILoginProvider.cs interface within the shared project: using Microsoft.WindowsAzure.MobileServices; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ILoginProvider { Task LoginAsync(MobileServiceClient client); } } Next, we are going to extend our Abstractions\\ICloudService.cs interface so that the main application can call the login routine: using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); } } Our code will call LoginAsync() in the ICloudService , which will get the platform-specific version of the login provider and call LoginAsync() there, but with our defined mobile service client. That is defined in the Services\\AzureCloudService.cs class: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Helpers; using Xamarin.Forms; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl); } public ICloudTable<T> GetTable<T>() where T : TableData => new AzureCloudTable<T>(client); public Task LoginAsync() { var loginProvider = DependencyService.Get<ILoginProvider>(); return loginProvider.LoginAsync(client); } } } The method looks up the platform dependent version of the login provider and executes the login method, passing along the client (which we will need later). Finally, we will want to easily instantiate the cloud provider. In the shared project, add the following to the constructor for App.cs : public App() { ServiceLocator.Instance.Add<ICloudService, AzureCloudService>(); MainPage = new NavigationPage(new Pages.EntryPage()); } The ServiceLocator class is used to manage the singletons in an application. The cloud service object can now be retrieved in any view using the following snippet: ICloudService cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); We will need to do this in the ViewModels/TaskDetailViewModel.cs and ViewModels/TaskKListViewModel.cs classes. Refer to the project in GitHub if you run into issues here. In each platform-specific project, we need to define a concrete implementation of the login provider that uses a web view to hold the actual authentication flow. Here is the droid Services\\DroidLoginProvider.cs (in the TaskList.Droid project): using System.Threading.Tasks; using Android.Content; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Droid.Services; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { Context context; public void Init(Context context) { this.context = context; } public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(context, \"aad\"); } } } Let us take a closer look at this implementation. The LoginAsync() method on the Azure Mobile Apps client object takes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\", \"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service. The clever piece is the Xamarin.Forms.Dependency call at the top - that registers the class as a platform service so we can access it through the Xamarin dependency service. Note that we need an extra initialization routine for Android that must be called prior the login provider being called to pass along the main window of the app (also known as the context). This is done in the MainActivity.cs file after the Xamarin Forms initialization call. In addition, Azure Mobile Apps uses Xamarin.Essentials under the covers, and that requires that you handle the callback. The dependency service is not set up until after the Xamarin Forms library is initialized, so we will not be able to get the login provider reference before that point: protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); ((DroidLoginProvider)DependencyService.Get<ILoginProvider>()).Init(this); LoadApplication(new App()); } protected override void OnResume() { base.OnResume(); Xamarin.Essentials.Platform.OnResume(); } iOS is similar, but does not require the initialization step in the main startup class. However, it does require handling the callback in the AppDelegate.cs: public override bool OpenUrl(UIApplication app, NSUrl url, NSDictionary options) { if (Xamarin.Essentials.Platform.OpenUrl(app, url, options)) { return true; } return base.OpenUrl(app, url, options); } The login provider class is in Services\\iOSLoginProvider.cs (in the TaskList.iOS project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.iOS.Services; using UIKit; [assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))] namespace TaskList.iOS.Services { public class iOSLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(RootView, \"aad\"); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; } } Note that we are using the same pattern here for registering the concrete implementation with the dependency service, so we can get it the same way. Finally, here is the UWP Services\\UWPLoginProvider.cs (in the TaskList.UWP project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.UWP.Services; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(\"aad\"); } } } Now that we have all the platform-specific login routines registered, we can move on to adding the login routine to the UI. We have already got a button on the entry page to enter the app. It makes sense to wire up that button so that it logs us in as well. The Command for the login button is in the ViewModels\\EntryPageViewModel.cs : async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[ExecuteLoginCommand] Error = {ex.Message}\"); } finally { IsBusy = false; } } Info The ServiceLocator class is a basic singleton handler. It is available in the Chapter2 project. It returns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1. When you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window: Going through the authentication process will get you to the task list again. If the authentication process fails, then LoginAsync() will throw an error, which is caught at the ViewModel. Right now, the EntryPageViewModel does nothing more than print a diagnostic message to the debug window of Visual Studio. Azure Active Directory: Client-Flow Setup \u00b6 Configuring Azure Active Directory for client-flow is a three-step process. First, we need to create a WEB application. This represents the resource: in our case, the resource is the Azure Mobile Apps backend. Then we need to create a NATIVE application. This represents the client: in our case, the ADAL (Active Directory Access Library) library will need this information. Finally, we need to give the NATIVE application permission to access the WEB application. It starts with configuring a server-flow to protect the resource. We've already done that above. Then configure a \"Native Application\" and give it permissions to the web application: Log on to the Azure portal . Select Azure Active Directory from the left hand menu. Click App registrations . Note that our existing web application is already there. You will see more applications, depending on what you have set up. Click + Add at the top of the page. Enter a name for the app registration. Select Native as the application type. Enter a valid URI in the Redirect URI. It can be anything, but it has to be valid. Click Create . In the Settings blade, click Redirect URIs . Add a Redirect URI of the form: https://yoursite.azurewebsites.net/.auth/login/done . Click Save . Click Required permissions . Click + Add . Click Select an API . Enter the name of your web application in the search box, and press Enter. Click the name of your web application, then click Select . You will be taken to Select permissions . Click Access your web application Click Select , then Done . So, what did we just do there? We created a new Azure AD app for the native application. We then gave permission for the native application to access resources that are protected by the web application. In our Azure App Service, we configured the service so that the Azure AD web application is used to protect our resources. The net effect is that our native application OR our web application can access the App Service resources that are protected via the [Authorize] attribute. Before continuing, you will need the Application ID and the Redirect URI for the NATIVE application. The Application ID for the native app is available in the Properties section of the Settings blade in the App Registrations blade: You can enter these into the Helpers\\Locations.cs file in the shared project: namespace TaskList.Helpers { public static class Locations { public static readonly string AppServiceUrl = \"https://zumobook-chapter2.azurewebsites.net\"; public static readonly string AadClientId = \"0c3309fe-e392-4ca5-8d54-55f69ae1e0f8\"; public static readonly string AadRedirectUri = \"https://zumobook-chapter2.azurewebsites.net/.auth/login/done\"; public static readonly string AadAuthority = \"https://login.windows.net/photoadrianoutlook.onmicrosoft.com\"; } } The AadClientId and AadRedirectUri must match what was configured in Azure AD for the native app. The other piece of information we need to add is the Azure AD Authority for the directory. This is available in the Domain names blade within the Azure Active Directory blade. Add the Microsoft.IdentityModel.Clients.ActiveDirectory NuGet package using Manage NuGet Packages... to each platform project. This package contains the ADAL library as a portable class library. Now you can add the client flow to each project. Start with the login provider in the TaskList.UWP project, located in the Services\\UWPLoginProvider.cs file: using System; using System.Linq; using System.Threading.Tasks; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.UWP.Services; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { /// <summary> /// Login via ADAL /// </summary> /// <returns>(async) token from the ADAL process</returns> public async Task<string> LoginADALAsync() { Uri returnUri = new Uri(Locations.AadRedirectUri); var authContext = new AuthenticationContext(Locations.AadAuthority); if (authContext.TokenCache.ReadItems().Count() > 0) { authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority); } var authResult = await authContext.AcquireTokenAsync( Locations.AppServiceUrl, /* The resource we want to access */ Locations.AadClientId, /* The Client ID of the Native App */ returnUri, /* The Return URI we configured */ new PlatformParameters(PromptBehavior.Auto, false)); return authResult.AccessToken; } public async Task LoginAsync(MobileServiceClient client) { // Client Flow var accessToken = await LoginADALAsync(); var zumoPayload = new JObject() { [\"access_token\"] = accessToken }; await client.LoginAsync(\"aad\", zumoPayload); // Server-Flow Version // await client.LoginAsync(\"aad\"); } } } The LoginADALAsync() method does the actual client-flow - using the ADAL library to authenticate the user and return the access token. The LoginAsync() method initiates the client-flow. It uses the token it receives from the client-flow to log in to the App Service, by packaging the token into a JSON object. I have placed the client and server flow next to each other so you can compare the two. In the TaskList.Droid project, we need to deal with the Context , as is common with Android libraries. The client flow in Services\\DroidLoginProvider.cs is remarkably similar though: using System; using System.Linq; using System.Threading.Tasks; using Android.App; using Android.Content; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Droid.Services; using TaskList.Helpers; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { Context context; public void Init(Context context) { this.context = context; } /// <summary> /// Login via ADAL /// </summary> /// <returns>(async) token from the ADAL process</returns> public async Task<string> LoginADALAsync() { Uri returnUri = new Uri(Locations.AadRedirectUri); var authContext = new AuthenticationContext(Locations.AadAuthority); if (authContext.TokenCache.ReadItems().Count() > 0) { authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority); } var authResult = await authContext.AcquireTokenAsync( Locations.AppServiceUrl, /* The resource we want to access */ Locations.AadClientId, /* The Client ID of the Native App */ returnUri, /* The Return URI we configured */ new PlatformParameters((Activity)context)); return authResult.AccessToken; } public async Task LoginAsync(MobileServiceClient client) { // Client Flow var accessToken = await LoginADALAsync(); var zumoPayload = new JObject(); zumoPayload[\"access_token\"] = accessToken; await client.LoginAsync(\"aad\", zumoPayload); // Server-Flow Version // await client.LoginAsync(context, \"aad\"); } } } The only real difference between this one and the Universal Windows edition is the PlatformParameters. We need to pass in the context of the MainActivity (which is passed in through the Init() call). However, we must also handle the response from the ADAL library. This is done in MainActivity.cs . Add the following method to the MainActivity class: protected override void OnActivityResult(int requestCode, Result resultCode, Intent data) { base.OnActivityResult(requestCode, resultCode, data); AuthenticationAgentContinuationHelper.SetAuthenticationAgentContinuationEventArgs(requestCode, resultCode, data); } Finally, the iOS version also requires access to the root view, so its PlatformParameters are also slightly different. Here is Services\\iOSLoginProvider.cs : using System; using System.Linq; using System.Threading.Tasks; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.iOS.Services; using UIKit; [assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))] namespace TaskList.iOS.Services { public class iOSLoginProvider : ILoginProvider { /// <summary> /// Login via ADAL /// </summary> /// <returns>(async) token from the ADAL process</returns> public async Task<string> LoginADALAsync(UIViewController view) { Uri returnUri = new Uri(Locations.AadRedirectUri); var authContext = new AuthenticationContext(Locations.AadAuthority); if (authContext.TokenCache.ReadItems().Count() > 0) { authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority); } var authResult = await authContext.AcquireTokenAsync( Locations.AppServiceUrl, /* The resource we want to access */ Locations.AadClientId, /* The Client ID of the Native App */ returnUri, /* The Return URI we configured */ new PlatformParameters(view)); return authResult.AccessToken; } public async Task LoginAsync(MobileServiceClient client) { var rootView = UIApplication.SharedApplication.KeyWindow.RootViewController; // Client Flow var accessToken = await LoginADALAsync(rootView); var zumoPayload = new JObject(); zumoPayload[\"access_token\"] = accessToken; await client.LoginAsync(\"aad\", zumoPayload); // Server Flow //await client.LoginAsync(rootView, \"aad\"); } } } Note that we can balance the needs of each platform by using the dependency service. The code that is unique to the platform is minimized and stored with the platform. If you aren't interested in social authentication (Facebook, Google, Microsoft or Twitter authentication providers), you can skip the Social Authentication section .","title":"Enterprise Authentication"},{"location":"chapter2/enterprise/#enterprise-authentication","text":"Enterprise Authentication is handled by Azure Active Directory, which is fairly commonly configured within Azure. Every Azure subscription has a default directory associated with it that you can leverage for this section. In addition, if your organization has an Office 365 subscription, this will likely be tied to an Azure Active Directory domain to allow enterprise sign-in. In either case, you have a directory you can use for providing authentication to your app. In general, you will need to get special permissions to update the directory. If you want to use your organizations corporate directory, then you are likely to have to get your IT department involved to set it up.","title":"Enterprise Authentication"},{"location":"chapter2/enterprise/#azure-active-directory-server-flow-setup","text":"The Azure Active Directory server-flow is perhaps the easiest of all the authentication methods to configure. No matter if you are doing a client flow or server flow, you need to set up the server flow first. Tip We recommend that you implement Client Flow in any non-trivial application. If you are using your default directory and you want to add a couple of test users, you will need to set those up first. Start by logging in to the Azure portal . In the left-hand menu, click More services . Enter Active Directory in the search box, then click Azure Active Directory . (Optional) If you need to manage a different directory to the default directory, click Switch directory and choose a different directory. Click Users and groups , then All users . Click Add . Fill in the information. Ensure you add the user to a group, if necessary. You can also add additional information (like a real name) in the Profile section. Check Show Password and make a note of the new password. Click Create . Repeat for each test user you wish to use. Once done, move onto configuring your App Service for authentication: Click All resources in the left hand menu. Click your App Service or Mobile App. Search for and click Authentication / Authorization (it's under SETTINGS). Change App Service Authentication to On . Ensure the Action to take when request is not authenticated is set to Allow Anonymous requests . Click Azure Active Directory . Click Express . All the information is filled in for you. Click OK , then Save . Info Make sure you create the app service in the right directory / subscription. If you have access to more than one directory, you can choose the right one by selecting it under your account drop-down in the top-right corner. There is also an Advanced track. This is used in client-flow situations and in situations where you have more than one directory. It's also used if you want to use the newer MSAL library for authentication. The Express flow is great for getting started quickly. You can walk through a server-flow authentication to test that you have all the settings correct. Point your browser at https:// yoursite .azurewebsites.net/.auth/login/aad. The browser will take you through an authentication flow before giving you a successful authentication image: If you get an error akin to \"We are having problems signing you in\", use an Incognito or InPrivate browsing window.","title":"Azure Active Directory: Server-Flow setup"},{"location":"chapter2/enterprise/#adding-authentication-to-a-mobile-client","text":"Now that the backend is completely configured, we can move our attention to the mobile client. We are going to be using the same mobile client that we developed in the first chapter, but we are now going to add authentication to it. Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought of this and provided a facility for running platform specific code called the DependencyService . Info If we run our application right now, clicking on the \"Enter the App\" button will result in an error. We will be able to see the Unauthorized error in the debug window of Visual Studio. Our first step is to define an Abstractions\\ILoginProvider.cs interface within the shared project: using Microsoft.WindowsAzure.MobileServices; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ILoginProvider { Task LoginAsync(MobileServiceClient client); } } Next, we are going to extend our Abstractions\\ICloudService.cs interface so that the main application can call the login routine: using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); } } Our code will call LoginAsync() in the ICloudService , which will get the platform-specific version of the login provider and call LoginAsync() there, but with our defined mobile service client. That is defined in the Services\\AzureCloudService.cs class: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Helpers; using Xamarin.Forms; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl); } public ICloudTable<T> GetTable<T>() where T : TableData => new AzureCloudTable<T>(client); public Task LoginAsync() { var loginProvider = DependencyService.Get<ILoginProvider>(); return loginProvider.LoginAsync(client); } } } The method looks up the platform dependent version of the login provider and executes the login method, passing along the client (which we will need later). Finally, we will want to easily instantiate the cloud provider. In the shared project, add the following to the constructor for App.cs : public App() { ServiceLocator.Instance.Add<ICloudService, AzureCloudService>(); MainPage = new NavigationPage(new Pages.EntryPage()); } The ServiceLocator class is used to manage the singletons in an application. The cloud service object can now be retrieved in any view using the following snippet: ICloudService cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); We will need to do this in the ViewModels/TaskDetailViewModel.cs and ViewModels/TaskKListViewModel.cs classes. Refer to the project in GitHub if you run into issues here. In each platform-specific project, we need to define a concrete implementation of the login provider that uses a web view to hold the actual authentication flow. Here is the droid Services\\DroidLoginProvider.cs (in the TaskList.Droid project): using System.Threading.Tasks; using Android.Content; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Droid.Services; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { Context context; public void Init(Context context) { this.context = context; } public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(context, \"aad\"); } } } Let us take a closer look at this implementation. The LoginAsync() method on the Azure Mobile Apps client object takes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\", \"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service. The clever piece is the Xamarin.Forms.Dependency call at the top - that registers the class as a platform service so we can access it through the Xamarin dependency service. Note that we need an extra initialization routine for Android that must be called prior the login provider being called to pass along the main window of the app (also known as the context). This is done in the MainActivity.cs file after the Xamarin Forms initialization call. In addition, Azure Mobile Apps uses Xamarin.Essentials under the covers, and that requires that you handle the callback. The dependency service is not set up until after the Xamarin Forms library is initialized, so we will not be able to get the login provider reference before that point: protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); ((DroidLoginProvider)DependencyService.Get<ILoginProvider>()).Init(this); LoadApplication(new App()); } protected override void OnResume() { base.OnResume(); Xamarin.Essentials.Platform.OnResume(); } iOS is similar, but does not require the initialization step in the main startup class. However, it does require handling the callback in the AppDelegate.cs: public override bool OpenUrl(UIApplication app, NSUrl url, NSDictionary options) { if (Xamarin.Essentials.Platform.OpenUrl(app, url, options)) { return true; } return base.OpenUrl(app, url, options); } The login provider class is in Services\\iOSLoginProvider.cs (in the TaskList.iOS project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.iOS.Services; using UIKit; [assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))] namespace TaskList.iOS.Services { public class iOSLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(RootView, \"aad\"); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; } } Note that we are using the same pattern here for registering the concrete implementation with the dependency service, so we can get it the same way. Finally, here is the UWP Services\\UWPLoginProvider.cs (in the TaskList.UWP project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.UWP.Services; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(\"aad\"); } } } Now that we have all the platform-specific login routines registered, we can move on to adding the login routine to the UI. We have already got a button on the entry page to enter the app. It makes sense to wire up that button so that it logs us in as well. The Command for the login button is in the ViewModels\\EntryPageViewModel.cs : async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[ExecuteLoginCommand] Error = {ex.Message}\"); } finally { IsBusy = false; } } Info The ServiceLocator class is a basic singleton handler. It is available in the Chapter2 project. It returns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1. When you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window: Going through the authentication process will get you to the task list again. If the authentication process fails, then LoginAsync() will throw an error, which is caught at the ViewModel. Right now, the EntryPageViewModel does nothing more than print a diagnostic message to the debug window of Visual Studio.","title":"Adding Authentication to a Mobile Client"},{"location":"chapter2/enterprise/#azure-active-directory-client-flow-setup","text":"Configuring Azure Active Directory for client-flow is a three-step process. First, we need to create a WEB application. This represents the resource: in our case, the resource is the Azure Mobile Apps backend. Then we need to create a NATIVE application. This represents the client: in our case, the ADAL (Active Directory Access Library) library will need this information. Finally, we need to give the NATIVE application permission to access the WEB application. It starts with configuring a server-flow to protect the resource. We've already done that above. Then configure a \"Native Application\" and give it permissions to the web application: Log on to the Azure portal . Select Azure Active Directory from the left hand menu. Click App registrations . Note that our existing web application is already there. You will see more applications, depending on what you have set up. Click + Add at the top of the page. Enter a name for the app registration. Select Native as the application type. Enter a valid URI in the Redirect URI. It can be anything, but it has to be valid. Click Create . In the Settings blade, click Redirect URIs . Add a Redirect URI of the form: https://yoursite.azurewebsites.net/.auth/login/done . Click Save . Click Required permissions . Click + Add . Click Select an API . Enter the name of your web application in the search box, and press Enter. Click the name of your web application, then click Select . You will be taken to Select permissions . Click Access your web application Click Select , then Done . So, what did we just do there? We created a new Azure AD app for the native application. We then gave permission for the native application to access resources that are protected by the web application. In our Azure App Service, we configured the service so that the Azure AD web application is used to protect our resources. The net effect is that our native application OR our web application can access the App Service resources that are protected via the [Authorize] attribute. Before continuing, you will need the Application ID and the Redirect URI for the NATIVE application. The Application ID for the native app is available in the Properties section of the Settings blade in the App Registrations blade: You can enter these into the Helpers\\Locations.cs file in the shared project: namespace TaskList.Helpers { public static class Locations { public static readonly string AppServiceUrl = \"https://zumobook-chapter2.azurewebsites.net\"; public static readonly string AadClientId = \"0c3309fe-e392-4ca5-8d54-55f69ae1e0f8\"; public static readonly string AadRedirectUri = \"https://zumobook-chapter2.azurewebsites.net/.auth/login/done\"; public static readonly string AadAuthority = \"https://login.windows.net/photoadrianoutlook.onmicrosoft.com\"; } } The AadClientId and AadRedirectUri must match what was configured in Azure AD for the native app. The other piece of information we need to add is the Azure AD Authority for the directory. This is available in the Domain names blade within the Azure Active Directory blade. Add the Microsoft.IdentityModel.Clients.ActiveDirectory NuGet package using Manage NuGet Packages... to each platform project. This package contains the ADAL library as a portable class library. Now you can add the client flow to each project. Start with the login provider in the TaskList.UWP project, located in the Services\\UWPLoginProvider.cs file: using System; using System.Linq; using System.Threading.Tasks; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.UWP.Services; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { /// <summary> /// Login via ADAL /// </summary> /// <returns>(async) token from the ADAL process</returns> public async Task<string> LoginADALAsync() { Uri returnUri = new Uri(Locations.AadRedirectUri); var authContext = new AuthenticationContext(Locations.AadAuthority); if (authContext.TokenCache.ReadItems().Count() > 0) { authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority); } var authResult = await authContext.AcquireTokenAsync( Locations.AppServiceUrl, /* The resource we want to access */ Locations.AadClientId, /* The Client ID of the Native App */ returnUri, /* The Return URI we configured */ new PlatformParameters(PromptBehavior.Auto, false)); return authResult.AccessToken; } public async Task LoginAsync(MobileServiceClient client) { // Client Flow var accessToken = await LoginADALAsync(); var zumoPayload = new JObject() { [\"access_token\"] = accessToken }; await client.LoginAsync(\"aad\", zumoPayload); // Server-Flow Version // await client.LoginAsync(\"aad\"); } } } The LoginADALAsync() method does the actual client-flow - using the ADAL library to authenticate the user and return the access token. The LoginAsync() method initiates the client-flow. It uses the token it receives from the client-flow to log in to the App Service, by packaging the token into a JSON object. I have placed the client and server flow next to each other so you can compare the two. In the TaskList.Droid project, we need to deal with the Context , as is common with Android libraries. The client flow in Services\\DroidLoginProvider.cs is remarkably similar though: using System; using System.Linq; using System.Threading.Tasks; using Android.App; using Android.Content; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Droid.Services; using TaskList.Helpers; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { Context context; public void Init(Context context) { this.context = context; } /// <summary> /// Login via ADAL /// </summary> /// <returns>(async) token from the ADAL process</returns> public async Task<string> LoginADALAsync() { Uri returnUri = new Uri(Locations.AadRedirectUri); var authContext = new AuthenticationContext(Locations.AadAuthority); if (authContext.TokenCache.ReadItems().Count() > 0) { authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority); } var authResult = await authContext.AcquireTokenAsync( Locations.AppServiceUrl, /* The resource we want to access */ Locations.AadClientId, /* The Client ID of the Native App */ returnUri, /* The Return URI we configured */ new PlatformParameters((Activity)context)); return authResult.AccessToken; } public async Task LoginAsync(MobileServiceClient client) { // Client Flow var accessToken = await LoginADALAsync(); var zumoPayload = new JObject(); zumoPayload[\"access_token\"] = accessToken; await client.LoginAsync(\"aad\", zumoPayload); // Server-Flow Version // await client.LoginAsync(context, \"aad\"); } } } The only real difference between this one and the Universal Windows edition is the PlatformParameters. We need to pass in the context of the MainActivity (which is passed in through the Init() call). However, we must also handle the response from the ADAL library. This is done in MainActivity.cs . Add the following method to the MainActivity class: protected override void OnActivityResult(int requestCode, Result resultCode, Intent data) { base.OnActivityResult(requestCode, resultCode, data); AuthenticationAgentContinuationHelper.SetAuthenticationAgentContinuationEventArgs(requestCode, resultCode, data); } Finally, the iOS version also requires access to the root view, so its PlatformParameters are also slightly different. Here is Services\\iOSLoginProvider.cs : using System; using System.Linq; using System.Threading.Tasks; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.iOS.Services; using UIKit; [assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))] namespace TaskList.iOS.Services { public class iOSLoginProvider : ILoginProvider { /// <summary> /// Login via ADAL /// </summary> /// <returns>(async) token from the ADAL process</returns> public async Task<string> LoginADALAsync(UIViewController view) { Uri returnUri = new Uri(Locations.AadRedirectUri); var authContext = new AuthenticationContext(Locations.AadAuthority); if (authContext.TokenCache.ReadItems().Count() > 0) { authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority); } var authResult = await authContext.AcquireTokenAsync( Locations.AppServiceUrl, /* The resource we want to access */ Locations.AadClientId, /* The Client ID of the Native App */ returnUri, /* The Return URI we configured */ new PlatformParameters(view)); return authResult.AccessToken; } public async Task LoginAsync(MobileServiceClient client) { var rootView = UIApplication.SharedApplication.KeyWindow.RootViewController; // Client Flow var accessToken = await LoginADALAsync(rootView); var zumoPayload = new JObject(); zumoPayload[\"access_token\"] = accessToken; await client.LoginAsync(\"aad\", zumoPayload); // Server Flow //await client.LoginAsync(rootView, \"aad\"); } } } Note that we can balance the needs of each platform by using the dependency service. The code that is unique to the platform is minimized and stored with the platform. If you aren't interested in social authentication (Facebook, Google, Microsoft or Twitter authentication providers), you can skip the Social Authentication section .","title":"Azure Active Directory: Client-Flow Setup"},{"location":"chapter2/realworld/","text":"Caching Tokens \u00b6 You will notice that we have to log in with every start of the application. The token that is generated has a lifetime that is provided and controlled by the identity provider. Some providers have a relatively short lifetime. For example, Azure Active Directory tokens have a lifetime of 1 hour. Others are incredibly long. Facebook has an expiry time of 60 days. Irrespective of the lifespan of the token, we will want to store it securely and re-use it when we can. Xamarin has provided a nice component, Xamarin.Auth , that provides such as secure store in a cross-platform manner. It starts with an account store: // For iOS: var accountStore = AccountStore.Create(); // For Android: var accountStore = AccountStore.Create(Context); We can then store the token with the following: accountStore.Save(account, \"descriptor\"); The descriptor is a string that allows us to find the token again. The account (which is an Account object) is uniquely identified by a key composed of the account's Username property and the descriptor. The Account class is provided with Xamarin.Auth. Storage is backed by the Keychain on iOS and the KeyStore on Android. To get the token back, we use the following: var accounts = accountStore.FindAccountsForService(\"descriptor\"); When we receive the token back from the key store, we will want to check the expiry time to ensure the token has not expired. As a result, there is a little bit more code to caching code than one would expect. Let's start with the Android version in TaskList.Droid . As with all the other login code, we are adjusting the LoginAsync() method in Services\\DroidLoginProvider.cs : using System; using System.Linq; using System.Text; using System.Threading.Tasks; using Android.App; using Android.Content; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Droid.Services; using TaskList.Helpers; using Xamarin.Auth; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { public Context RootView { get; private set; } public AccountStore AccountStore { get; private set; } public void Init(Context context) { RootView = context; AccountStore = AccountStore.Create(context); } public async Task LoginAsync(MobileServiceClient client) { // Check if the token is available within the key store var accounts = AccountStore.FindAccountsForService(\"tasklist\"); if (accounts != null) { foreach (var acct in accounts) { string token; if (acct.Properties.TryGetValue(\"token\", out token)) { if (!IsTokenExpired(token)) { client.CurrentUser = new MobileServiceUser(acct.Username); client.CurrentUser.MobileServiceAuthenticationToken = token; return; } } } } // Server Flow await client.LoginAsync(RootView, \"aad\"); // Store the new token within the store var account = new Account(client.CurrentUser.UserId); account.Properties.Add(\"token\", client.CurrentUser.MobileServiceAuthenticationToken); AccountStore.Save(account, \"tasklist\"); } bool IsTokenExpired(string token) { // Get just the JWT part of the token (without the signature). var jwt = token.Split(new Char[] { '.' })[1]; // Undo the URL encoding. jwt = jwt.Replace('-', '+').Replace('_', '/'); switch (jwt.Length % 4) { case 0: break; case 2: jwt += \"==\"; break; case 3: jwt += \"=\"; break; default: throw new ArgumentException(\"The token is not a valid Base64 string.\"); } // Convert to a JSON String var bytes = Convert.FromBase64String(jwt); string jsonString = UTF8Encoding.UTF8.GetString(bytes, 0, bytes.Length); // Parse as JSON object and get the exp field value, // which is the expiration date as a JavaScript primative date. JObject jsonObj = JObject.Parse(jsonString); var exp = Convert.ToDouble(jsonObj[\"exp\"].ToString()); // Calculate the expiration by adding the exp value (in seconds) to the // base date of 1/1/1970. DateTime minTime = new DateTime(1970, 1, 1, 0, 0, 0, 0, DateTimeKind.Utc); var expire = minTime.AddSeconds(exp); return (expire < DateTime.UtcNow); } } } There are three new pieces to this code. The first piece is to check to see if there is an existing token in the KeyStore. If there is, we check the expiry time and then set up the Azure Mobile Apps client with the username and token from the KeyStore. If there isn't, we do the normal authentication process. If the authentication process is successful, we reach the second piece, which is to store the token within the KeyStore. If there is an existing entry, it will be overwritten. Finally, there is a method called IsTokenExpired() whose only job is to check to see if a token is expired or not. This same code can be used in the Services/iOSLoginProvider.cs . The only difference is in the AccountStore.Create() call (as discussed earlier). Update Entitlements for iOS 10 You may notice that you are not able to use AccountStore.Save() in the iOS 10 Simulator. A change to the iOS entitlements has caused this change. You must add keychain access to your Entitlements.plist file, and use the Entitlements.plist file as a custom entitlements list. Visual Studio for the PC doesn't provide a lot of assistance with the entitlements. However, Visual Studio for Mac has a great editor for the entitlement, so this is one time I'd suggest going over to the Mac to do something. Make sure you have created an Apple Developer account and created a provisioning profile. These are pre-requisites to using the Keychain. Right-click the TaskList.iOS project to open the options pane and select Options Select the iOS Bundle Signing menu option. Select iPhoneSimulator for the Platform. Click the ... button next to Custom Entitlements . Select the Entitlements.plist file, then click Open . Save the properties (I used Ctrl-S for this) Find and open the Entitlements.plist file in the TaskList.iOS project. In the Keychain sction, check the box next to Enable Keychain Access Groups . This may require additional setup and linking to a provisioning profile. Save the file and re-build your project. Xamarin.Auth only support iOS and Android. We need to turn to an alternate library for token caching on Universal Windows. The standard library has a package called PasswordVault that can be used identically to the KeyStore and Keychain libraries. Here is the Universal Windows version of the same code in Services\\UWPLoginProvider.cs : using System; using System.Linq; using System.Text; using System.Threading.Tasks; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.UWP.Services; using Windows.Security.Credentials; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public PasswordVault PasswordVault { get; private set; } public UWPLoginProvider() { PasswordVault = new PasswordVault(); } public async Task LoginAsync(MobileServiceClient client) { // Check if the token is available within the password vault var acct = PasswordVault.FindAllByResource(\"tasklist\").FirstOrDefault(); if (acct != null) { var token = PasswordVault.Retrieve(\"tasklist\", acct.UserName).Password; if (token != null && token.Length > 0 && !IsTokenExpired(token)) { client.CurrentUser = new MobileServiceUser(acct.UserName); client.CurrentUser.MobileServiceAuthenticationToken = token; return; } } // Server-Flow Version await client.LoginAsync(\"aad\"); // Store the token in the password vault PasswordVault.Add(new PasswordCredential(\"tasklist\", client.CurrentUser.UserId, client.CurrentUser.MobileServiceAuthenticationToken)); } bool IsTokenExpired(string token) { /* Copy code from DroidLoginProvider */ } } } The PasswordVault replaces the KeyStore (Android) and Keychain (iOS), but the concepts are the same. All three mechanisms provide the basic functionality of storing client secrets securely. Refresh Tokens \u00b6 The token cache checks the token to see if it is expired and prompts the user if the token is no longer valid. Since the life of a token is inevitably short (maybe one hour), this will still mean that the user is prompted for new credentials most of the time. In addition, we have an issue when the app is running for a long time. What happens if the user leaves the app running for 2 hours? The token we received at the start of the session will be invalid halfway through the session and we will have to restart the app in order to continue. Both of these situations are undesirable from the point of view of the user. Access tokens eventually expire and we need to explicitly deal with this situation. The first part of the solution is to request a Refresh Token . This is something the identity provider issues when the scope of the request includes an offline scope. Only certain identity providers include the ability to request refresh tokens. For server-flow: Google: Append the \"access_type=offline\" to the request. Microsoft Account: Select the wl.offline_access scope in the Azure management portal. Azure AD: Configure Azure AD to support access to the Graph API. Facebook and Twitter do not provider refresh tokens. Once you have the refresh tokens, you can simply call the refresh API in the Azure Mobile Apps SDK to refresh the token. Info Refresh Tokens are one area that require special consideration when using Custom Authentication. Just like with the /.auth/me endpoint, you are on your own when it comes to handling token expiry for custom authentication. Configuring Refresh Tokens \u00b6 You can add the additional information to a Google request with the following code snippet: client.LoginAsync(\"google\", new Dictionary<string, string> { { \"access_type\", \"offline\" } }); Azure Active Directory is perhaps the trickiest to configure. Log on to the Azure portal . Navigate to your Azure Active Directory. Click App registrations , then your WEB application Click Keys . Enter a friendly name. Pick 2 years in the Expiry duration Click Save . The key will be generated for you. Copy the key (you will need it below). Go to App Services , then your App Service. Click Resource explorer in the menu, then Go . In the Resource Explorer, expand config and select authsettings . Click on Edit . Set the clientSecret to the key you copied from above. Set the additionalLoginParams to [\"response_type=code id_token\"] . Click the Read/Write toggle button at the top of the page. Click the PUT button. The next time the user logs into our web app side, there will be a one-time prompt to consent to graph API access. Once granted, the App Service Authentication / Authorization service will start requesting and receiving refresh tokens. Once you go through this process and re-authenticate, you will be able to see the refresh token in the output of the /.auth/me endpoint: Refresh tokens have a different expiry time to the identity token. The refresh token theoretically lives forever, but there are \"non-use expiry\" times. This varies by identity provider. Google: 6 months Microsoft Account: 24 hours Azure Active Directory: 90 days In addition, there may be other reasons why a token can be invalidated. For instance, Google provides 25 refresh tokens per user. If the user requests more than the limit, the oldest token is invalidated. You should refer to the OAuth documentation for the identity provider. Using Refresh Tokens \u00b6 The Azure Mobile Apps Client SDK has a built in method for refreshing tokens for you. It assumes that you are using a supported identity provider (Azure Active Directory, Google or Microsoft Account), and have configured the identity provider to generate the refresh token. To refresh a token, use: client.RefreshUserAsync(); Tip If you get the error \"You do not have permission to view this directory or page\" when accessing the refresh endpoint, there are no refresh tokens for your user in the token store. This could be because the user has yet to re-authenticate (causing a new refresh token to be generated), the provider is not set up to generate refresh tokens or the provider does not support refresh tokens. We can easily add this to the login process in the platform-specific provider. Rather than provide the same logic over and over, we can extend the ILoginProvider to do the base operations for us then implement the logic once in the AzureCloudService . The Abstractions\\ILoginProvider.cs interface now looks like this: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; namespace TaskList.Abstractions { public interface ILoginProvider { MobileServiceUser RetrieveTokenFromSecureStore(); void StoreTokenInSecureStore(MobileServiceUser user); Task<MobileServiceUser> LoginAsync(MobileServiceClient client); } } Since the RefreshUserAsync() method is purely contained within the Azure Mobile Apps Client SDK and requires no changes between platforms, we don't need a special platform-specific version. Each method of the interface is one of the primitives we have already discussed. For example, the Android version in Services\\DroidLoginProvider.cs now looks like this: [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { #region ILoginProvider Interface public MobileServiceUser RetrieveTokenFromSecureStore() { var accounts = AccountStore.FindAccountsForService(\"tasklist\"); if (accounts != null) { foreach (var acct in accounts) { string token; if (acct.Properties.TryGetValue(\"token\", out token)) { return new MobileServiceUser(acct.Username) { MobileServiceAuthenticationToken = token }; } } } return null; } public void StoreTokenInSecureStore(MobileServiceUser user) { var account = new Account(user.UserId); account.Properties.Add(\"token\", user.MobileServiceAuthenticationToken); AccountStore.Save(account, \"tasklist\"); } public async Task<MobileServiceUser> LoginAsync(MobileServiceClient client) { // Server Flow return await client.LoginAsync(RootView, \"aad\"); } #endregion public Context RootView { get; private set; } public AccountStore AccountStore { get; private set; } public void Init(Context context) { RootView = context; AccountStore = AccountStore.Create(context); } } } The iOS version is practically the same because we are using the common Xamarin.Auth portable library. The difference is in the methods outside of the ILoginProvider interface: public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; public AccountStore AccountStore { get; private set; } public iOSLoginProvider() { AccountStore = AccountStore.Create(); } Finally, the Universal Windows version (in Services\\UWPLoginProvider.cs ) is significantly different in the secure store implementation: [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public PasswordVault PasswordVault { get; private set; } public UWPLoginProvider() { PasswordVault = new PasswordVault(); } #region ILoginProvider Interface public MobileServiceUser RetrieveTokenFromSecureStore() { try { // Check if the token is available within the password vault var acct = PasswordVault.FindAllByResource(\"tasklist\").FirstOrDefault(); if (acct != null) { var token = PasswordVault.Retrieve(\"tasklist\", acct.UserName).Password; if (token != null && token.Length > 0) { return new MobileServiceUser(acct.UserName) { MobileServiceAuthenticationToken = token }; } } } catch (Exception ex) { Debug.WriteLine($\"Error retrieving existing token: {ex.Message}\"); } return null; } public void StoreTokenInSecureStore(MobileServiceUser user) { PasswordVault.Add(new PasswordCredential(\"tasklist\", user.UserId, user.MobileServiceAuthenticationToken)); } public async Task<MobileServiceUser> LoginAsync(MobileServiceClient client) { // Server-Flow Version return await client.LoginAsync(\"aad\"); } #endregion } } We can swap out the server-flow Azure Active Directory login method with any of the client-flow, server-flow or custom flows that we have been discussing thus far across all three platform-specific implementations. The common flow handles all the logic for us. This is the LoginAsync() method in the Services\\AzureCloudService.cs class: public async Task<MobileServiceUser> LoginAsync() { var loginProvider = DependencyService.Get<ILoginProvider>(); client.CurrentUser = loginProvider.RetrieveTokenFromSecureStore(); if (client.CurrentUser != null) { // User has previously been authenticated - try to Refresh the token try { var refreshed = await client.RefreshUserAsync(); if (refreshed != null) { loginProvider.StoreTokenInSecureStore(refreshed); return refreshed; } } catch (Exception refreshException) { Debug.WriteLine($\"Could not refresh token: {refreshException.Message}\"); } } if (client.CurrentUser != null && !IsTokenExpired(client.CurrentUser.MobileServiceAuthenticationToken)) { // User has previously been authenticated, no refresh is required return client.CurrentUser; } // We need to ask for credentials at this point await loginProvider.LoginAsync(client); if (client.CurrentUser != null) { // We were able to successfully log in loginProvider.StoreTokenInSecureStore(client.CurrentUser); } return client.CurrentUser; } For full disclosure, I've also moved the IsTokenExpired() method from the platform-specific code to the shared project, and updated the ICloudService.cs to match the new signature of LoginAsync() . The process follows the best practices: Check for a stored token - if one exists, try to refresh it. If the token (that potentially just got refreshed) is not expired, continue using it. If not, ask the user for credentials. If we get a valid token back, store it in the secure store for next time. There is another place that we must consider refresh tokens. During a HTTP request to our mobile backend, it is possible that the token has expired since our last request. The request will return a 401 Unauthorized response in this case. We need to trap that and perform a login request. The login request will either refresh the token or prompt the user for new credentials. We can then continue with the request as before. The Azure Mobile Apps SDK contains a mechanism for hooking into the HTTP workflow using a DelegatingHandler . A delegating handler is a base type for a HTTP handler that allows us to process the request and response from the HTTP client object before (and after) it finally get processed. It's used for adding additional headers to the request or logging the request and response, for example. We are going to use it to validate the response and re-submit the request (after login) if the request comes back as a 401 Unauthorized. We start with the adjustment to the Services\\AzureCloudService.cs constructor: public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler()); if (Locations.AlternateLoginHost != null) client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost); } The AuthenticationDelegatingHandler() is the new piece here. This is the delegating handler that we are going to implement to handle the re-try logic. I've placed the code in Helpers\\AuthenticationDelegatingHandler.cs : using System.Collections.Generic; using System.IO; using System.Net; using System.Net.Http; using System.Threading; using System.Threading.Tasks; using TaskList.Abstractions; namespace TaskList.Helpers { class AuthenticationDelegatingHandler : DelegatingHandler { protected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) { // Clone the request, in case we need to re-issue it var clone = await CloneHttpRequestMessageAsync(request); // Now do the request var response = await base.SendAsync(request, cancellationToken); if (response.StatusCode == HttpStatusCode.Unauthorized) { // The request resulted in a 401 Unauthorized. We need to do a LoginAsync, // which will do the Refresh if appropriate, or ask for credentials if not. var user = await ServiceLocator.Instance.Resolve<ICloudService>().LoginAsync(); // Now, retry the request with the cloned request. The only thing we have // to do is replace the X-ZUMO-AUTH header with the new auth token. clone.Headers.Remove(\"X-ZUMO-AUTH\"); clone.Headers.Add(\"X-ZUMO-AUTH\", user.MobileServiceAuthenticationToken); response = await base.SendAsync(clone, cancellationToken); } return response; } /// <summary> /// Clone a HttpRequestMessage /// Credit: http://stackoverflow.com/questions/25044166/how-to-clone-a-httprequestmessage-when-the-original-request-has-content /// </summary> /// <param name=\"req\">The request</param> /// <returns>A copy of the request</returns> public static async Task<HttpRequestMessage> CloneHttpRequestMessageAsync(HttpRequestMessage req) { HttpRequestMessage clone = new HttpRequestMessage(req.Method, req.RequestUri); // Copy the request's content (via a MemoryStream) into the cloned object var ms = new MemoryStream(); if (req.Content != null) { await req.Content.CopyToAsync(ms).ConfigureAwait(false); ms.Position = 0; clone.Content = new StreamContent(ms); // Copy the content headers if (req.Content.Headers != null) foreach (var h in req.Content.Headers) clone.Content.Headers.Add(h.Key, h.Value); } clone.Version = req.Version; foreach (KeyValuePair<string, object> prop in req.Properties) clone.Properties.Add(prop); foreach (KeyValuePair<string, IEnumerable<string>> header in req.Headers) clone.Headers.TryAddWithoutValidation(header.Key, header.Value); return clone; } } } There is no in-built method for cloning a HttpRequestMessage object. Fortunately Stack Overflow provided an answer that seems to work. Running this code will now pass every single non-login request through the delegating handler. If we get an Unauthorized at any point, the login flow (which includes an implicit refresh token) will be triggered. Info There are two HTTPClient objects created inside of the MobileServiceClient object. One is for all the non-login flows and it supports the delegating handlers. However there is another one for login flows. The one for login flows does not support delegating handlers. This means you don't have to worry about cyclical references within the delegating handler (where a login flow triggers another login flow). Logging out \u00b6 There is a dirty little secret within the Azure Mobile Apps Client SDK. Calling LogoutAsync() does not actually invalidate the token you are using. It simply removes it from the MobileServiceClient context. Don't believe me? Here is the code : /// <summary> /// Log a user out. /// </summary> public Task LogoutAsync() { this.CurrentUser = null; return Task.FromResult(0); } When you actually think about it, this makes sense. You can get logged in via five different supported identity providers via a web-flow. In this case, you are logging your browser out of the identity provider. Do you really want to log out of Facebook when you log out of your app? So, how do you log out? You should: Call the identity provider logout method (if appropriate). Many identity providers don't provide this. Invalidate the token on the mobile backend. Remove the token from the local secure cache store. Finally, call the LogoutAsync() method on the MobileServiceClient . Invalidating the token on the mobile backend. \u00b6 Calling the /.auth/logout endpoint on the Azure App Service mobile backend will remove the entry on the token store. However, it does not (currently) invalidate the token. The token, if submitted, will still authorize the user. The refresh token is stored in the token store. The user submitting the token will be unable to refresh the token. Once the ZUMO token has expired (which happens an hour after it was created), the logout is complete. We need to do a HTTP client call for this purpose: // Invalidate the token on the mobile backend var authUri = new Uri($\"{client.MobileAppUri}/.auth/logout\"); using (var httpClient = new HttpClient()) { httpClient.DefaultRequestHeaders.Add(\"X-ZUMO-AUTH\", client.CurrentUser.MobileServiceAuthenticationToken); await httpClient.GetAsync(authUri); } Removing the token from the local secure cache store. \u00b6 For this part of the process, We can add a new method to the ILoginProvider.cs interface: void RemoveTokenFromSecureStore(); For Android and iOS, the concrete implementation looks like this: public void RemoveTokenFromSecureStore() { var accounts = AccountStore.FindAccountsForService(\"tasklist\"); if (accounts != null) { foreach (var acct in accounts) { AccountStore.Delete(acct, \"tasklist\"); } } } For Universal Windows, the concrete implementation is a bit different: public void RemoveTokenFromSecureStore() { try { // Check if the token is available within the password vault var acct = PasswordVault.FindAllByResource(\"tasklist\").FirstOrDefault(); if (acct != null) { PasswordVault.Remove(acct); } } catch (Exception ex) { Debug.WriteLine($\"Error retrieving existing token: {ex.Message}\"); } } Implementing a LogoutAsync() method. \u00b6 I've added the following to the ICloudService interface: Task LogoutAsync(); This has a concrete implementation in Services\\AzureCloudService.cs : public async Task LogoutAsync() { if (client.CurrentUser == null || client.CurrentUser.MobileServiceAuthenticationToken == null) return; // Log out of the identity provider (if required) // Invalidate the token on the mobile backend var authUri = new Uri($\"{client.MobileAppUri}/.auth/logout\"); using (var httpClient = new HttpClient()) { httpClient.DefaultRequestHeaders.Add(\"X-ZUMO-AUTH\", client.CurrentUser.MobileServiceAuthenticationToken); await httpClient.GetAsync(authUri); } // Remove the token from the cache DependencyService.Get<ILoginProvider>().RemoveTokenFromSecureStore(); // Remove the token from the MobileServiceClient await client.LogoutAsync(); } This does three of the four providers. If your identity provider supports an app-level logout, then you should call that where indicated. This is probably going to be platform-specific code, so you will want to add a method to the ILoginProvider.cs interface and add a concrete implementation to each platform project. I've also added a logout button to my Pages\\TaskList.xaml ( view code ) and added the event handler for the logout button to the ViewModels\\EntryPageViewModel.cs ( view code ).","title":"Tokens in Real Apps"},{"location":"chapter2/realworld/#caching-tokens","text":"You will notice that we have to log in with every start of the application. The token that is generated has a lifetime that is provided and controlled by the identity provider. Some providers have a relatively short lifetime. For example, Azure Active Directory tokens have a lifetime of 1 hour. Others are incredibly long. Facebook has an expiry time of 60 days. Irrespective of the lifespan of the token, we will want to store it securely and re-use it when we can. Xamarin has provided a nice component, Xamarin.Auth , that provides such as secure store in a cross-platform manner. It starts with an account store: // For iOS: var accountStore = AccountStore.Create(); // For Android: var accountStore = AccountStore.Create(Context); We can then store the token with the following: accountStore.Save(account, \"descriptor\"); The descriptor is a string that allows us to find the token again. The account (which is an Account object) is uniquely identified by a key composed of the account's Username property and the descriptor. The Account class is provided with Xamarin.Auth. Storage is backed by the Keychain on iOS and the KeyStore on Android. To get the token back, we use the following: var accounts = accountStore.FindAccountsForService(\"descriptor\"); When we receive the token back from the key store, we will want to check the expiry time to ensure the token has not expired. As a result, there is a little bit more code to caching code than one would expect. Let's start with the Android version in TaskList.Droid . As with all the other login code, we are adjusting the LoginAsync() method in Services\\DroidLoginProvider.cs : using System; using System.Linq; using System.Text; using System.Threading.Tasks; using Android.App; using Android.Content; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Droid.Services; using TaskList.Helpers; using Xamarin.Auth; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { public Context RootView { get; private set; } public AccountStore AccountStore { get; private set; } public void Init(Context context) { RootView = context; AccountStore = AccountStore.Create(context); } public async Task LoginAsync(MobileServiceClient client) { // Check if the token is available within the key store var accounts = AccountStore.FindAccountsForService(\"tasklist\"); if (accounts != null) { foreach (var acct in accounts) { string token; if (acct.Properties.TryGetValue(\"token\", out token)) { if (!IsTokenExpired(token)) { client.CurrentUser = new MobileServiceUser(acct.Username); client.CurrentUser.MobileServiceAuthenticationToken = token; return; } } } } // Server Flow await client.LoginAsync(RootView, \"aad\"); // Store the new token within the store var account = new Account(client.CurrentUser.UserId); account.Properties.Add(\"token\", client.CurrentUser.MobileServiceAuthenticationToken); AccountStore.Save(account, \"tasklist\"); } bool IsTokenExpired(string token) { // Get just the JWT part of the token (without the signature). var jwt = token.Split(new Char[] { '.' })[1]; // Undo the URL encoding. jwt = jwt.Replace('-', '+').Replace('_', '/'); switch (jwt.Length % 4) { case 0: break; case 2: jwt += \"==\"; break; case 3: jwt += \"=\"; break; default: throw new ArgumentException(\"The token is not a valid Base64 string.\"); } // Convert to a JSON String var bytes = Convert.FromBase64String(jwt); string jsonString = UTF8Encoding.UTF8.GetString(bytes, 0, bytes.Length); // Parse as JSON object and get the exp field value, // which is the expiration date as a JavaScript primative date. JObject jsonObj = JObject.Parse(jsonString); var exp = Convert.ToDouble(jsonObj[\"exp\"].ToString()); // Calculate the expiration by adding the exp value (in seconds) to the // base date of 1/1/1970. DateTime minTime = new DateTime(1970, 1, 1, 0, 0, 0, 0, DateTimeKind.Utc); var expire = minTime.AddSeconds(exp); return (expire < DateTime.UtcNow); } } } There are three new pieces to this code. The first piece is to check to see if there is an existing token in the KeyStore. If there is, we check the expiry time and then set up the Azure Mobile Apps client with the username and token from the KeyStore. If there isn't, we do the normal authentication process. If the authentication process is successful, we reach the second piece, which is to store the token within the KeyStore. If there is an existing entry, it will be overwritten. Finally, there is a method called IsTokenExpired() whose only job is to check to see if a token is expired or not. This same code can be used in the Services/iOSLoginProvider.cs . The only difference is in the AccountStore.Create() call (as discussed earlier). Update Entitlements for iOS 10 You may notice that you are not able to use AccountStore.Save() in the iOS 10 Simulator. A change to the iOS entitlements has caused this change. You must add keychain access to your Entitlements.plist file, and use the Entitlements.plist file as a custom entitlements list. Visual Studio for the PC doesn't provide a lot of assistance with the entitlements. However, Visual Studio for Mac has a great editor for the entitlement, so this is one time I'd suggest going over to the Mac to do something. Make sure you have created an Apple Developer account and created a provisioning profile. These are pre-requisites to using the Keychain. Right-click the TaskList.iOS project to open the options pane and select Options Select the iOS Bundle Signing menu option. Select iPhoneSimulator for the Platform. Click the ... button next to Custom Entitlements . Select the Entitlements.plist file, then click Open . Save the properties (I used Ctrl-S for this) Find and open the Entitlements.plist file in the TaskList.iOS project. In the Keychain sction, check the box next to Enable Keychain Access Groups . This may require additional setup and linking to a provisioning profile. Save the file and re-build your project. Xamarin.Auth only support iOS and Android. We need to turn to an alternate library for token caching on Universal Windows. The standard library has a package called PasswordVault that can be used identically to the KeyStore and Keychain libraries. Here is the Universal Windows version of the same code in Services\\UWPLoginProvider.cs : using System; using System.Linq; using System.Text; using System.Threading.Tasks; using Microsoft.IdentityModel.Clients.ActiveDirectory; using Microsoft.WindowsAzure.MobileServices; using Newtonsoft.Json.Linq; using TaskList.Abstractions; using TaskList.Helpers; using TaskList.UWP.Services; using Windows.Security.Credentials; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public PasswordVault PasswordVault { get; private set; } public UWPLoginProvider() { PasswordVault = new PasswordVault(); } public async Task LoginAsync(MobileServiceClient client) { // Check if the token is available within the password vault var acct = PasswordVault.FindAllByResource(\"tasklist\").FirstOrDefault(); if (acct != null) { var token = PasswordVault.Retrieve(\"tasklist\", acct.UserName).Password; if (token != null && token.Length > 0 && !IsTokenExpired(token)) { client.CurrentUser = new MobileServiceUser(acct.UserName); client.CurrentUser.MobileServiceAuthenticationToken = token; return; } } // Server-Flow Version await client.LoginAsync(\"aad\"); // Store the token in the password vault PasswordVault.Add(new PasswordCredential(\"tasklist\", client.CurrentUser.UserId, client.CurrentUser.MobileServiceAuthenticationToken)); } bool IsTokenExpired(string token) { /* Copy code from DroidLoginProvider */ } } } The PasswordVault replaces the KeyStore (Android) and Keychain (iOS), but the concepts are the same. All three mechanisms provide the basic functionality of storing client secrets securely.","title":"Caching Tokens"},{"location":"chapter2/realworld/#refresh-tokens","text":"The token cache checks the token to see if it is expired and prompts the user if the token is no longer valid. Since the life of a token is inevitably short (maybe one hour), this will still mean that the user is prompted for new credentials most of the time. In addition, we have an issue when the app is running for a long time. What happens if the user leaves the app running for 2 hours? The token we received at the start of the session will be invalid halfway through the session and we will have to restart the app in order to continue. Both of these situations are undesirable from the point of view of the user. Access tokens eventually expire and we need to explicitly deal with this situation. The first part of the solution is to request a Refresh Token . This is something the identity provider issues when the scope of the request includes an offline scope. Only certain identity providers include the ability to request refresh tokens. For server-flow: Google: Append the \"access_type=offline\" to the request. Microsoft Account: Select the wl.offline_access scope in the Azure management portal. Azure AD: Configure Azure AD to support access to the Graph API. Facebook and Twitter do not provider refresh tokens. Once you have the refresh tokens, you can simply call the refresh API in the Azure Mobile Apps SDK to refresh the token. Info Refresh Tokens are one area that require special consideration when using Custom Authentication. Just like with the /.auth/me endpoint, you are on your own when it comes to handling token expiry for custom authentication.","title":"Refresh Tokens"},{"location":"chapter2/realworld/#configuring-refresh-tokens","text":"You can add the additional information to a Google request with the following code snippet: client.LoginAsync(\"google\", new Dictionary<string, string> { { \"access_type\", \"offline\" } }); Azure Active Directory is perhaps the trickiest to configure. Log on to the Azure portal . Navigate to your Azure Active Directory. Click App registrations , then your WEB application Click Keys . Enter a friendly name. Pick 2 years in the Expiry duration Click Save . The key will be generated for you. Copy the key (you will need it below). Go to App Services , then your App Service. Click Resource explorer in the menu, then Go . In the Resource Explorer, expand config and select authsettings . Click on Edit . Set the clientSecret to the key you copied from above. Set the additionalLoginParams to [\"response_type=code id_token\"] . Click the Read/Write toggle button at the top of the page. Click the PUT button. The next time the user logs into our web app side, there will be a one-time prompt to consent to graph API access. Once granted, the App Service Authentication / Authorization service will start requesting and receiving refresh tokens. Once you go through this process and re-authenticate, you will be able to see the refresh token in the output of the /.auth/me endpoint: Refresh tokens have a different expiry time to the identity token. The refresh token theoretically lives forever, but there are \"non-use expiry\" times. This varies by identity provider. Google: 6 months Microsoft Account: 24 hours Azure Active Directory: 90 days In addition, there may be other reasons why a token can be invalidated. For instance, Google provides 25 refresh tokens per user. If the user requests more than the limit, the oldest token is invalidated. You should refer to the OAuth documentation for the identity provider.","title":"Configuring Refresh Tokens"},{"location":"chapter2/realworld/#using-refresh-tokens","text":"The Azure Mobile Apps Client SDK has a built in method for refreshing tokens for you. It assumes that you are using a supported identity provider (Azure Active Directory, Google or Microsoft Account), and have configured the identity provider to generate the refresh token. To refresh a token, use: client.RefreshUserAsync(); Tip If you get the error \"You do not have permission to view this directory or page\" when accessing the refresh endpoint, there are no refresh tokens for your user in the token store. This could be because the user has yet to re-authenticate (causing a new refresh token to be generated), the provider is not set up to generate refresh tokens or the provider does not support refresh tokens. We can easily add this to the login process in the platform-specific provider. Rather than provide the same logic over and over, we can extend the ILoginProvider to do the base operations for us then implement the logic once in the AzureCloudService . The Abstractions\\ILoginProvider.cs interface now looks like this: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; namespace TaskList.Abstractions { public interface ILoginProvider { MobileServiceUser RetrieveTokenFromSecureStore(); void StoreTokenInSecureStore(MobileServiceUser user); Task<MobileServiceUser> LoginAsync(MobileServiceClient client); } } Since the RefreshUserAsync() method is purely contained within the Azure Mobile Apps Client SDK and requires no changes between platforms, we don't need a special platform-specific version. Each method of the interface is one of the primitives we have already discussed. For example, the Android version in Services\\DroidLoginProvider.cs now looks like this: [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { #region ILoginProvider Interface public MobileServiceUser RetrieveTokenFromSecureStore() { var accounts = AccountStore.FindAccountsForService(\"tasklist\"); if (accounts != null) { foreach (var acct in accounts) { string token; if (acct.Properties.TryGetValue(\"token\", out token)) { return new MobileServiceUser(acct.Username) { MobileServiceAuthenticationToken = token }; } } } return null; } public void StoreTokenInSecureStore(MobileServiceUser user) { var account = new Account(user.UserId); account.Properties.Add(\"token\", user.MobileServiceAuthenticationToken); AccountStore.Save(account, \"tasklist\"); } public async Task<MobileServiceUser> LoginAsync(MobileServiceClient client) { // Server Flow return await client.LoginAsync(RootView, \"aad\"); } #endregion public Context RootView { get; private set; } public AccountStore AccountStore { get; private set; } public void Init(Context context) { RootView = context; AccountStore = AccountStore.Create(context); } } } The iOS version is practically the same because we are using the common Xamarin.Auth portable library. The difference is in the methods outside of the ILoginProvider interface: public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; public AccountStore AccountStore { get; private set; } public iOSLoginProvider() { AccountStore = AccountStore.Create(); } Finally, the Universal Windows version (in Services\\UWPLoginProvider.cs ) is significantly different in the secure store implementation: [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public PasswordVault PasswordVault { get; private set; } public UWPLoginProvider() { PasswordVault = new PasswordVault(); } #region ILoginProvider Interface public MobileServiceUser RetrieveTokenFromSecureStore() { try { // Check if the token is available within the password vault var acct = PasswordVault.FindAllByResource(\"tasklist\").FirstOrDefault(); if (acct != null) { var token = PasswordVault.Retrieve(\"tasklist\", acct.UserName).Password; if (token != null && token.Length > 0) { return new MobileServiceUser(acct.UserName) { MobileServiceAuthenticationToken = token }; } } } catch (Exception ex) { Debug.WriteLine($\"Error retrieving existing token: {ex.Message}\"); } return null; } public void StoreTokenInSecureStore(MobileServiceUser user) { PasswordVault.Add(new PasswordCredential(\"tasklist\", user.UserId, user.MobileServiceAuthenticationToken)); } public async Task<MobileServiceUser> LoginAsync(MobileServiceClient client) { // Server-Flow Version return await client.LoginAsync(\"aad\"); } #endregion } } We can swap out the server-flow Azure Active Directory login method with any of the client-flow, server-flow or custom flows that we have been discussing thus far across all three platform-specific implementations. The common flow handles all the logic for us. This is the LoginAsync() method in the Services\\AzureCloudService.cs class: public async Task<MobileServiceUser> LoginAsync() { var loginProvider = DependencyService.Get<ILoginProvider>(); client.CurrentUser = loginProvider.RetrieveTokenFromSecureStore(); if (client.CurrentUser != null) { // User has previously been authenticated - try to Refresh the token try { var refreshed = await client.RefreshUserAsync(); if (refreshed != null) { loginProvider.StoreTokenInSecureStore(refreshed); return refreshed; } } catch (Exception refreshException) { Debug.WriteLine($\"Could not refresh token: {refreshException.Message}\"); } } if (client.CurrentUser != null && !IsTokenExpired(client.CurrentUser.MobileServiceAuthenticationToken)) { // User has previously been authenticated, no refresh is required return client.CurrentUser; } // We need to ask for credentials at this point await loginProvider.LoginAsync(client); if (client.CurrentUser != null) { // We were able to successfully log in loginProvider.StoreTokenInSecureStore(client.CurrentUser); } return client.CurrentUser; } For full disclosure, I've also moved the IsTokenExpired() method from the platform-specific code to the shared project, and updated the ICloudService.cs to match the new signature of LoginAsync() . The process follows the best practices: Check for a stored token - if one exists, try to refresh it. If the token (that potentially just got refreshed) is not expired, continue using it. If not, ask the user for credentials. If we get a valid token back, store it in the secure store for next time. There is another place that we must consider refresh tokens. During a HTTP request to our mobile backend, it is possible that the token has expired since our last request. The request will return a 401 Unauthorized response in this case. We need to trap that and perform a login request. The login request will either refresh the token or prompt the user for new credentials. We can then continue with the request as before. The Azure Mobile Apps SDK contains a mechanism for hooking into the HTTP workflow using a DelegatingHandler . A delegating handler is a base type for a HTTP handler that allows us to process the request and response from the HTTP client object before (and after) it finally get processed. It's used for adding additional headers to the request or logging the request and response, for example. We are going to use it to validate the response and re-submit the request (after login) if the request comes back as a 401 Unauthorized. We start with the adjustment to the Services\\AzureCloudService.cs constructor: public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler()); if (Locations.AlternateLoginHost != null) client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost); } The AuthenticationDelegatingHandler() is the new piece here. This is the delegating handler that we are going to implement to handle the re-try logic. I've placed the code in Helpers\\AuthenticationDelegatingHandler.cs : using System.Collections.Generic; using System.IO; using System.Net; using System.Net.Http; using System.Threading; using System.Threading.Tasks; using TaskList.Abstractions; namespace TaskList.Helpers { class AuthenticationDelegatingHandler : DelegatingHandler { protected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) { // Clone the request, in case we need to re-issue it var clone = await CloneHttpRequestMessageAsync(request); // Now do the request var response = await base.SendAsync(request, cancellationToken); if (response.StatusCode == HttpStatusCode.Unauthorized) { // The request resulted in a 401 Unauthorized. We need to do a LoginAsync, // which will do the Refresh if appropriate, or ask for credentials if not. var user = await ServiceLocator.Instance.Resolve<ICloudService>().LoginAsync(); // Now, retry the request with the cloned request. The only thing we have // to do is replace the X-ZUMO-AUTH header with the new auth token. clone.Headers.Remove(\"X-ZUMO-AUTH\"); clone.Headers.Add(\"X-ZUMO-AUTH\", user.MobileServiceAuthenticationToken); response = await base.SendAsync(clone, cancellationToken); } return response; } /// <summary> /// Clone a HttpRequestMessage /// Credit: http://stackoverflow.com/questions/25044166/how-to-clone-a-httprequestmessage-when-the-original-request-has-content /// </summary> /// <param name=\"req\">The request</param> /// <returns>A copy of the request</returns> public static async Task<HttpRequestMessage> CloneHttpRequestMessageAsync(HttpRequestMessage req) { HttpRequestMessage clone = new HttpRequestMessage(req.Method, req.RequestUri); // Copy the request's content (via a MemoryStream) into the cloned object var ms = new MemoryStream(); if (req.Content != null) { await req.Content.CopyToAsync(ms).ConfigureAwait(false); ms.Position = 0; clone.Content = new StreamContent(ms); // Copy the content headers if (req.Content.Headers != null) foreach (var h in req.Content.Headers) clone.Content.Headers.Add(h.Key, h.Value); } clone.Version = req.Version; foreach (KeyValuePair<string, object> prop in req.Properties) clone.Properties.Add(prop); foreach (KeyValuePair<string, IEnumerable<string>> header in req.Headers) clone.Headers.TryAddWithoutValidation(header.Key, header.Value); return clone; } } } There is no in-built method for cloning a HttpRequestMessage object. Fortunately Stack Overflow provided an answer that seems to work. Running this code will now pass every single non-login request through the delegating handler. If we get an Unauthorized at any point, the login flow (which includes an implicit refresh token) will be triggered. Info There are two HTTPClient objects created inside of the MobileServiceClient object. One is for all the non-login flows and it supports the delegating handlers. However there is another one for login flows. The one for login flows does not support delegating handlers. This means you don't have to worry about cyclical references within the delegating handler (where a login flow triggers another login flow).","title":"Using Refresh Tokens"},{"location":"chapter2/realworld/#logging-out","text":"There is a dirty little secret within the Azure Mobile Apps Client SDK. Calling LogoutAsync() does not actually invalidate the token you are using. It simply removes it from the MobileServiceClient context. Don't believe me? Here is the code : /// <summary> /// Log a user out. /// </summary> public Task LogoutAsync() { this.CurrentUser = null; return Task.FromResult(0); } When you actually think about it, this makes sense. You can get logged in via five different supported identity providers via a web-flow. In this case, you are logging your browser out of the identity provider. Do you really want to log out of Facebook when you log out of your app? So, how do you log out? You should: Call the identity provider logout method (if appropriate). Many identity providers don't provide this. Invalidate the token on the mobile backend. Remove the token from the local secure cache store. Finally, call the LogoutAsync() method on the MobileServiceClient .","title":"Logging out"},{"location":"chapter2/realworld/#invalidating-the-token-on-the-mobile-backend","text":"Calling the /.auth/logout endpoint on the Azure App Service mobile backend will remove the entry on the token store. However, it does not (currently) invalidate the token. The token, if submitted, will still authorize the user. The refresh token is stored in the token store. The user submitting the token will be unable to refresh the token. Once the ZUMO token has expired (which happens an hour after it was created), the logout is complete. We need to do a HTTP client call for this purpose: // Invalidate the token on the mobile backend var authUri = new Uri($\"{client.MobileAppUri}/.auth/logout\"); using (var httpClient = new HttpClient()) { httpClient.DefaultRequestHeaders.Add(\"X-ZUMO-AUTH\", client.CurrentUser.MobileServiceAuthenticationToken); await httpClient.GetAsync(authUri); }","title":"Invalidating the token on the mobile backend."},{"location":"chapter2/realworld/#removing-the-token-from-the-local-secure-cache-store","text":"For this part of the process, We can add a new method to the ILoginProvider.cs interface: void RemoveTokenFromSecureStore(); For Android and iOS, the concrete implementation looks like this: public void RemoveTokenFromSecureStore() { var accounts = AccountStore.FindAccountsForService(\"tasklist\"); if (accounts != null) { foreach (var acct in accounts) { AccountStore.Delete(acct, \"tasklist\"); } } } For Universal Windows, the concrete implementation is a bit different: public void RemoveTokenFromSecureStore() { try { // Check if the token is available within the password vault var acct = PasswordVault.FindAllByResource(\"tasklist\").FirstOrDefault(); if (acct != null) { PasswordVault.Remove(acct); } } catch (Exception ex) { Debug.WriteLine($\"Error retrieving existing token: {ex.Message}\"); } }","title":"Removing the token from the local secure cache store."},{"location":"chapter2/realworld/#implementing-a-logoutasync-method","text":"I've added the following to the ICloudService interface: Task LogoutAsync(); This has a concrete implementation in Services\\AzureCloudService.cs : public async Task LogoutAsync() { if (client.CurrentUser == null || client.CurrentUser.MobileServiceAuthenticationToken == null) return; // Log out of the identity provider (if required) // Invalidate the token on the mobile backend var authUri = new Uri($\"{client.MobileAppUri}/.auth/logout\"); using (var httpClient = new HttpClient()) { httpClient.DefaultRequestHeaders.Add(\"X-ZUMO-AUTH\", client.CurrentUser.MobileServiceAuthenticationToken); await httpClient.GetAsync(authUri); } // Remove the token from the cache DependencyService.Get<ILoginProvider>().RemoveTokenFromSecureStore(); // Remove the token from the MobileServiceClient await client.LogoutAsync(); } This does three of the four providers. If your identity provider supports an app-level logout, then you should call that where indicated. This is probably going to be platform-specific code, so you will want to add a method to the ILoginProvider.cs interface and add a concrete implementation to each platform project. I've also added a logout button to my Pages\\TaskList.xaml ( view code ) and added the event handler for the logout button to the ViewModels\\EntryPageViewModel.cs ( view code ).","title":"Implementing a LogoutAsync() method."},{"location":"chapter2/social/","text":"Social Authentication \u00b6 Azure App Service provides built-in support for Facebook and Google. Microsoft and Twitter are also accepted, but Microsoft authentication is handled within the context of Azure Active Directory (and is an option when creating the app registration), and Twitter uses the older (and insecure) OAuth v1 protocol. It should not be used. Irrespective of whether you intend to use server-flow or client-flow, you will need to configure the Azure App Service Authentication / Authorization service. The method is pretty similar in each case: Obtain a Developer Account for the provider. Create a new application, obtaining a Client ID and Secret. Turn on Azure App Service Authentication. Enter the Client ID and Secret into the specific provider setup. Save the configuration. Before you start any of this, create a new Azure Mobile Apps as we described in Chapter 1 . If you want a site to deploy for the configuration, the Backend project in the Chapter2 solution is pre-configured for authorization. You just need to deploy it to Azure App Service. Facebook Configuration \u00b6 I am going to assume you have a Facebook account already. If you do not have a Facebook account, go to Facebook and sign up. All your friends are likely there already! Now log in to the Facebook Developers web site. Create a new Facebook application: Note : Facebook updates the look and feel of their developer site on a regular basis. As a result, the screen shots I have provided here may be different. If in doubt, follow the bullet descriptions to find your way. Info If you are not already registered, Click the drop-down in the top-right corner and Register as a Developer before continuing. Click the My Apps link in the top right corner of the screen. Click Create a New App . Fill in the form: If required, verify your account according to the instructions. This usually involves typing a CAPTCHA text, adding a credit card number and/or verifying your mobile phone number. Click the Get Started button next to Facebook Login . Click Settings under Facebook Login in the left-hand menu. Enter your application URL + /.auth/login/facebook/callback in the Valid OAuth redirect URIs . Click Save Changes . Click the Settings (under Dashboard ) in the left-hand menu. Click the Show button next to the App Secret Now that you have the App ID and App Secret , you can continue configuration of your app within the Azure Portal . Open up your App Service by clicking on All Resources or App Services followed by the name of your app service. In the Settings blade, Click Authentication / Authorization which is under Features . Turn App Service Authentication to On . In the Action to take when request is not authenticated , select Allow Anonymous requests (no action) . Danger It is very tempting to choose Log in with Facebook . However, you need to avoid this. Selecting this option will mean that all requests need to be authenticated and you will not get the information about the identity on the back end. Selecting Allow Anonymous requests means your app is in charge of what gets authenticated and what does not require authentication. Click Facebook (which should show Not Configured ). Cut and Paste the App ID and App Secret into the boxes provided. Select public_profile and email for Scopes. Warn If you request anything but public_profile, user_friends, and email, your app will need further review by Facebook, which will take time. This process is not worth it for test apps like this one. Click OK (at the bottom of the blade) to close the Facebook configuration blade. Click Save (at the top of the blade) to save your Authentication changes. You can test your authentication process by browsing to https:// yoursite .azurewebsites.net/.auth/login/facebook; this is the same endpoint that the Azure Mobile Apps Client SDK calls when it is time to integrate authentication into the mobile client. If you are not logged in to facebook already, you will be prompted for your facebook credentials first. Finally, here is your happy page - the page that signifies you have done everything right: Warn Every single OAuth provider will ask you what sort of information you want to have access to. These \"claims\" translate into permissions. The more permissions you request, the less likely the user is going to accept them. Be a good net citizen and only request the information you are actually going to use. Google Configuration \u00b6 It should be no shock that you need a Google Account to get started. If you do not have one already (or you want a different account for your development activities), create a new account now. Then log in to the Google Developer Portal . Click the Create Project link at the top: Enter a nice name, then Click Create . The screen will show the progress and eventually the project will be listed in the All Projects list. It takes about 30 seconds to create a project. Once you have your Google project, the API Manager Library will show up. If Google decides to change the end place, Click the hamburger menu (in the top-left corner), select API Manager , followed by Library to see all the Google APIs you can enable: There is no \"Google Login\" that can guide you here. The API you need to add is called Google+ and is listed under the Social APIs . Click Google+ API , then click Enable at the top of the screen. Adding an API to a Google app doesn't mean it is ready to use. In our case, we need to configure the login process. Google has, of course, set this up so that it's easy to access a Google service, but difficult to use the authentication for other purposes. Configure Google login as follows: Click Credentials in the left-hand menu. Select the OAuth consent screen tab: Fill in the form and Click Save . 3. Create Credentials button. This pops up a drop-down menu. You want the OAuth Client ID . Select Web application . Enter the URL of your App Service in the Authorized JavaScript origins box. Ensure you use the https version of the URL. Enter https://_yoursite_.azurewebsites.net/.auth/login/google/callback in the Authorized redirect URIs box. Click Create . Google will display your client ID and client secret. You can also view the Client ID and Client Secret from the interface by clicking on the Credentials link on the left-hand menu. The process from here is practically the same as Facebook. Open your App Service within the Azure Portal, Click All Settings , then Authentication / Authorization and finally Google (assuming you have already turned on the authentication service). Copy and paste the Client ID and Client Secret into the boxes provided. Click OK (at the bottom) followed by Save (at the top of the page). Info You can define multiple providers at the same time. The code in the client determines what authentication mechanism is used. You can test this just like Facebook. Go to https:// yoursite /.auth/login/google with your browser. You should see something like the following: Confirming here should get us to the same happy screen we achieved with Facebook. If you happen to mis-type the Authorized redirect URI, Google will inform you that the URI is wrong. I inevitably swap http for https. When this happens, it is an easy fix, but you have to wait a few minutes before the authentication system updates itself. Warn Google has changed the security semantics for its authentication service. You must use the v3.1.0 of the Azure Mobile Apps Client SDK for Server Flow authentication with Google to work. Adding Authentication to a Mobile Client \u00b6 Now that the backend is completely configured, we can move our attention to the mobile client. We are going to be using the same mobile client that we developed in the first chapter, but we are now going to add authentication to it. Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought of this and provided a facility for running platform specific code called the DependencyService . Info If you have already implemented authentication during the Enterprise Authentication section, this code is the same. You just have to alter the provider name. If we run our application right now, clicking on the \"Enter the App\" button will result in an error. You will be able to see the Unauthorized error in the debug window of Visual Studio. Our first step is to define an Abstractions\\ILoginProvider.cs interface within the shared project: using Microsoft.WindowsAzure.MobileServices; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ILoginProvider { Task LoginAsync(MobileServiceClient client); } } Next, we extend our Abstractions\\ICloudService.cs interface so that the main application can call the login routine: using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); } } Our code will call LoginAsync() in the ICloudService , which will get the platform-specific version of the login provider and call LoginAsync() there, but with our defined mobile service client. That is defined in the Services\\AzureCloudService.cs class: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Helpers; using Xamarin.Forms; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl); } public ICloudTable<T> GetTable<T>() where T : TableData => new AzureCloudTable<T>(client); public Task LoginAsync() { var loginProvider = DependencyService.Get<ILoginProvider>(); return loginProvider.LoginAsync(client); } } } The method looks up the platform dependent version of the login provider and executes the login method, passing along the client (which we will need later). In each platform-specific project, we are going to define a concrete implementation of the login provider that uses a web view to hold the actual authentication flow. Here is the droid Services\\DroidLoginProvider.cs (in the TaskList.Droid project): using System.Threading.Tasks; using Android.Content; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Droid.Services; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { Context context; public void Init(Context context) { this.context = context; } public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(context, \"facebook\"); } } } Tip Replace \"facebook\" with \"google\", \"microsoftaccount\" or \"twitter\", depending on your identity provider. Let us take a closer look at this implementation. The LoginAsync() method on the Azure Mobile Apps client object takes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\", \"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service. The clever piece is the Xamarin.Forms.Dependency call at the top - that registers the class as a platform service so we can access it through the Xamarin dependency service. Note that we need an extra initialization routine for Android that must be called prior the login provider being called to pass along the main window of the app (also known as the context). This is done in the MainActivity.cs file after the Xamarin Forms initialization call. The dependency service is not set up until after the Xamarin Forms library is initialized, so we will not be able to get the login provider reference before that point: protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); ((DroidLoginProvider)DependencyService.Get<ILoginProvider>()).Init(this); LoadApplication(new App()); } iOS is similar, but does not require the initialization step in the main startup class. The login provider class is in Services\\iOSLoginProvider.cs (in the TaskList.iOS project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.iOS.Services; using UIKit; [assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))] namespace TaskList.iOS.Services { public class iOSLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(RootView, \"facebook\"); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; } } Note that we are using the same pattern here for registering the concrete implementation with the dependency service, so we can get it the same way. Finally, here is the UWP Services\\UWPLoginProvider.cs (in the TaskList.UWP project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.UWP.Services; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(\"facebook\"); } } } Now that we have all the platform-specific login routines registered, we can move on to adding the login routine to the UI. We have already got a button on the entry page to enter the app. It makes sense to wire up that button so that it logs us in as well. The Command for the login button is in the ViewModels\\EntryPageViewModel.cs : async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[ExecuteLoginCommand] Error = {ex.Message}\"); } finally { IsBusy = false; } } Info The ServiceLocator class is my basic singleton handler. It is available in the Chapter2 project. It returns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1. When you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window: Going through the authentication process will get you to the task list again. If the authentication process fails, then LoginAsync() will throw an error, which is caught at the ViewModel. Right now, the EntryPageViewModel does nothing more than print a diagnostic message to the debug window of Visual Studio. Integrating with multiple providers Each provider has a slightly different server flow. In the [Chapter2] project, I use this to call the right provider endpoint. When the user clicks on the Facebook logo, for instance, it logs in with Facebook. It's up to you as to how this UI is presented. Client-Flow for Social Providers \u00b6 In each of the social providers, the identity provider SDK (provided by Facebook, Google, Microsoft or Twitter) will need to be integrated. In general, these SDKs are provided for a native platform (Objective-C or Swift for iOS, Java for Android), use callbacks or delegates (as is common practice in native libraries) and are thus more complicated to integrate with your mobile client than those that have a C#/.NET SDK delivered on NuGet. Warn Testing Client Flow for social providers requires that the social app be installed on the device. You cannot install other apps on the iOS simulator and there may be restrictions on the Android Emulator. This means that you generally need to test client flow for social providers on an actual device. The reward for doing so are a more integrated experience on mobile devices. For example, if you integrate the Google Play Services SDK in an Android app, the app will seamlessly authenticate itself with the connected Google account in the background, avoiding the need for repeatedly authenticating the client. It may ask for a fingerprint instead if the app is not trusted. If you integrate the Facebook SDK, then the app will automatically switch to the Facebook app and ask you to approve the authentication request there instead of authenticating the user through a web view. Both of these provide a more integrated experience for the end user, so this work is well worth pursuing. As an example, here is the client flow for Facebook. I've implemented this using the Xamarin.Facebook.iOS library, which can be downloaded and installed into the iOS project from NuGet. The Services\\iOSLoginProvider.cs contains the following: #region Facebook Client Flow private TaskCompletionSource<string> fbtcs; public async Task<string> LoginFacebookAsync() { fbtcs = new TaskCompletionSource<string>(); var loginManager = new LoginManager(); loginManager.LogInWithReadPermissions(new[] { \"public_profile\" }, RootView, LoginTokenHandler); return await fbtcs.Task; } private void LoginTokenHandler(LoginManagerLoginResult loginResult, NSError error) { if (loginResult.Token != null) { fbtcs.TrySetResult(loginResult.Token.TokenString); } else { fbtcs.TrySetException(new Exception(\"Facebook Client Flow Login Failed\")); } } #endregion Note the use of a TaskCompletionSource<>() here. This is used often to convert callback APIs into awaitable APIs. We set off the async call with the callback, then await on the completion (which is signified by the TaskCompletionSource ). When the callback is called, it sets the value of the TaskCompletionSource (or causes an exception) and that causes the task to complete. The LoginAsync() method can now be updated like this: public async Task LoginAsync(MobileServiceClient client) { var accessToken = await LoginFacebookAsync(); var zumoPayload = new JObject() { [\"access_token\"] = accessToken }; await client.LoginAsync(\"facebook\", zumoPayload); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; Finally, you need to configure your Facebook settings within the Info.plist file. Right-click the Info.plist file and select Open with... . Select the XML editor. Add the following to the file within the <dict> element (right before the closing </dict> ): <key>FacebookAppID</key> <string>YOUR-APP-ID</string> <key>LSApplicationQueriesSchemes</key> <array> <string>fbauth2</string> </array> <key>CFBundleURLTypes</key> <array> <dict> <key>CFBundleURLSchemes</key> <array> <string>fbYOUR-APP-ID</string> </array> </dict> </array> Replace YOUR-APP-ID with the Facebook App ID from the Facebook developers console. With this version, clicking on the login button will seamlessly switch into the Facebook application and ask the user to confirm the request, before switching back authenticated. Note that it's likely that the Facebook SDK will not work within a simulator as it requires the Facebook app to be installed. There are a number of pre-built Xamarin libraries for handling provider authentication. For iOS, we have already shown Azure Active Directory and Facebook. Android has its own version of the Facebook SDK . Use the Google .NET API to access Google accounts. It's already cross-platform. In general, using the library from the provider itself is preferable to using one from a third party.","title":"Social Authentication"},{"location":"chapter2/social/#social-authentication","text":"Azure App Service provides built-in support for Facebook and Google. Microsoft and Twitter are also accepted, but Microsoft authentication is handled within the context of Azure Active Directory (and is an option when creating the app registration), and Twitter uses the older (and insecure) OAuth v1 protocol. It should not be used. Irrespective of whether you intend to use server-flow or client-flow, you will need to configure the Azure App Service Authentication / Authorization service. The method is pretty similar in each case: Obtain a Developer Account for the provider. Create a new application, obtaining a Client ID and Secret. Turn on Azure App Service Authentication. Enter the Client ID and Secret into the specific provider setup. Save the configuration. Before you start any of this, create a new Azure Mobile Apps as we described in Chapter 1 . If you want a site to deploy for the configuration, the Backend project in the Chapter2 solution is pre-configured for authorization. You just need to deploy it to Azure App Service.","title":"Social Authentication"},{"location":"chapter2/social/#facebook-configuration","text":"I am going to assume you have a Facebook account already. If you do not have a Facebook account, go to Facebook and sign up. All your friends are likely there already! Now log in to the Facebook Developers web site. Create a new Facebook application: Note : Facebook updates the look and feel of their developer site on a regular basis. As a result, the screen shots I have provided here may be different. If in doubt, follow the bullet descriptions to find your way. Info If you are not already registered, Click the drop-down in the top-right corner and Register as a Developer before continuing. Click the My Apps link in the top right corner of the screen. Click Create a New App . Fill in the form: If required, verify your account according to the instructions. This usually involves typing a CAPTCHA text, adding a credit card number and/or verifying your mobile phone number. Click the Get Started button next to Facebook Login . Click Settings under Facebook Login in the left-hand menu. Enter your application URL + /.auth/login/facebook/callback in the Valid OAuth redirect URIs . Click Save Changes . Click the Settings (under Dashboard ) in the left-hand menu. Click the Show button next to the App Secret Now that you have the App ID and App Secret , you can continue configuration of your app within the Azure Portal . Open up your App Service by clicking on All Resources or App Services followed by the name of your app service. In the Settings blade, Click Authentication / Authorization which is under Features . Turn App Service Authentication to On . In the Action to take when request is not authenticated , select Allow Anonymous requests (no action) . Danger It is very tempting to choose Log in with Facebook . However, you need to avoid this. Selecting this option will mean that all requests need to be authenticated and you will not get the information about the identity on the back end. Selecting Allow Anonymous requests means your app is in charge of what gets authenticated and what does not require authentication. Click Facebook (which should show Not Configured ). Cut and Paste the App ID and App Secret into the boxes provided. Select public_profile and email for Scopes. Warn If you request anything but public_profile, user_friends, and email, your app will need further review by Facebook, which will take time. This process is not worth it for test apps like this one. Click OK (at the bottom of the blade) to close the Facebook configuration blade. Click Save (at the top of the blade) to save your Authentication changes. You can test your authentication process by browsing to https:// yoursite .azurewebsites.net/.auth/login/facebook; this is the same endpoint that the Azure Mobile Apps Client SDK calls when it is time to integrate authentication into the mobile client. If you are not logged in to facebook already, you will be prompted for your facebook credentials first. Finally, here is your happy page - the page that signifies you have done everything right: Warn Every single OAuth provider will ask you what sort of information you want to have access to. These \"claims\" translate into permissions. The more permissions you request, the less likely the user is going to accept them. Be a good net citizen and only request the information you are actually going to use.","title":"Facebook Configuration"},{"location":"chapter2/social/#google-configuration","text":"It should be no shock that you need a Google Account to get started. If you do not have one already (or you want a different account for your development activities), create a new account now. Then log in to the Google Developer Portal . Click the Create Project link at the top: Enter a nice name, then Click Create . The screen will show the progress and eventually the project will be listed in the All Projects list. It takes about 30 seconds to create a project. Once you have your Google project, the API Manager Library will show up. If Google decides to change the end place, Click the hamburger menu (in the top-left corner), select API Manager , followed by Library to see all the Google APIs you can enable: There is no \"Google Login\" that can guide you here. The API you need to add is called Google+ and is listed under the Social APIs . Click Google+ API , then click Enable at the top of the screen. Adding an API to a Google app doesn't mean it is ready to use. In our case, we need to configure the login process. Google has, of course, set this up so that it's easy to access a Google service, but difficult to use the authentication for other purposes. Configure Google login as follows: Click Credentials in the left-hand menu. Select the OAuth consent screen tab: Fill in the form and Click Save . 3. Create Credentials button. This pops up a drop-down menu. You want the OAuth Client ID . Select Web application . Enter the URL of your App Service in the Authorized JavaScript origins box. Ensure you use the https version of the URL. Enter https://_yoursite_.azurewebsites.net/.auth/login/google/callback in the Authorized redirect URIs box. Click Create . Google will display your client ID and client secret. You can also view the Client ID and Client Secret from the interface by clicking on the Credentials link on the left-hand menu. The process from here is practically the same as Facebook. Open your App Service within the Azure Portal, Click All Settings , then Authentication / Authorization and finally Google (assuming you have already turned on the authentication service). Copy and paste the Client ID and Client Secret into the boxes provided. Click OK (at the bottom) followed by Save (at the top of the page). Info You can define multiple providers at the same time. The code in the client determines what authentication mechanism is used. You can test this just like Facebook. Go to https:// yoursite /.auth/login/google with your browser. You should see something like the following: Confirming here should get us to the same happy screen we achieved with Facebook. If you happen to mis-type the Authorized redirect URI, Google will inform you that the URI is wrong. I inevitably swap http for https. When this happens, it is an easy fix, but you have to wait a few minutes before the authentication system updates itself. Warn Google has changed the security semantics for its authentication service. You must use the v3.1.0 of the Azure Mobile Apps Client SDK for Server Flow authentication with Google to work.","title":"Google Configuration"},{"location":"chapter2/social/#adding-authentication-to-a-mobile-client","text":"Now that the backend is completely configured, we can move our attention to the mobile client. We are going to be using the same mobile client that we developed in the first chapter, but we are now going to add authentication to it. Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought of this and provided a facility for running platform specific code called the DependencyService . Info If you have already implemented authentication during the Enterprise Authentication section, this code is the same. You just have to alter the provider name. If we run our application right now, clicking on the \"Enter the App\" button will result in an error. You will be able to see the Unauthorized error in the debug window of Visual Studio. Our first step is to define an Abstractions\\ILoginProvider.cs interface within the shared project: using Microsoft.WindowsAzure.MobileServices; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ILoginProvider { Task LoginAsync(MobileServiceClient client); } } Next, we extend our Abstractions\\ICloudService.cs interface so that the main application can call the login routine: using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task LoginAsync(); } } Our code will call LoginAsync() in the ICloudService , which will get the platform-specific version of the login provider and call LoginAsync() there, but with our defined mobile service client. That is defined in the Services\\AzureCloudService.cs class: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Helpers; using Xamarin.Forms; namespace TaskList.Services { public class AzureCloudService : ICloudService { MobileServiceClient client; public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl); } public ICloudTable<T> GetTable<T>() where T : TableData => new AzureCloudTable<T>(client); public Task LoginAsync() { var loginProvider = DependencyService.Get<ILoginProvider>(); return loginProvider.LoginAsync(client); } } } The method looks up the platform dependent version of the login provider and executes the login method, passing along the client (which we will need later). In each platform-specific project, we are going to define a concrete implementation of the login provider that uses a web view to hold the actual authentication flow. Here is the droid Services\\DroidLoginProvider.cs (in the TaskList.Droid project): using System.Threading.Tasks; using Android.Content; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.Droid.Services; [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))] namespace TaskList.Droid.Services { public class DroidLoginProvider : ILoginProvider { Context context; public void Init(Context context) { this.context = context; } public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(context, \"facebook\"); } } } Tip Replace \"facebook\" with \"google\", \"microsoftaccount\" or \"twitter\", depending on your identity provider. Let us take a closer look at this implementation. The LoginAsync() method on the Azure Mobile Apps client object takes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\", \"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service. The clever piece is the Xamarin.Forms.Dependency call at the top - that registers the class as a platform service so we can access it through the Xamarin dependency service. Note that we need an extra initialization routine for Android that must be called prior the login provider being called to pass along the main window of the app (also known as the context). This is done in the MainActivity.cs file after the Xamarin Forms initialization call. The dependency service is not set up until after the Xamarin Forms library is initialized, so we will not be able to get the login provider reference before that point: protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); ((DroidLoginProvider)DependencyService.Get<ILoginProvider>()).Init(this); LoadApplication(new App()); } iOS is similar, but does not require the initialization step in the main startup class. The login provider class is in Services\\iOSLoginProvider.cs (in the TaskList.iOS project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.iOS.Services; using UIKit; [assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))] namespace TaskList.iOS.Services { public class iOSLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(RootView, \"facebook\"); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; } } Note that we are using the same pattern here for registering the concrete implementation with the dependency service, so we can get it the same way. Finally, here is the UWP Services\\UWPLoginProvider.cs (in the TaskList.UWP project): using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; using TaskList.UWP.Services; [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))] namespace TaskList.UWP.Services { public class UWPLoginProvider : ILoginProvider { public async Task LoginAsync(MobileServiceClient client) { await client.LoginAsync(\"facebook\"); } } } Now that we have all the platform-specific login routines registered, we can move on to adding the login routine to the UI. We have already got a button on the entry page to enter the app. It makes sense to wire up that button so that it logs us in as well. The Command for the login button is in the ViewModels\\EntryPageViewModel.cs : async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { Debug.WriteLine($\"[ExecuteLoginCommand] Error = {ex.Message}\"); } finally { IsBusy = false; } } Info The ServiceLocator class is my basic singleton handler. It is available in the Chapter2 project. It returns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1. When you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window: Going through the authentication process will get you to the task list again. If the authentication process fails, then LoginAsync() will throw an error, which is caught at the ViewModel. Right now, the EntryPageViewModel does nothing more than print a diagnostic message to the debug window of Visual Studio. Integrating with multiple providers Each provider has a slightly different server flow. In the [Chapter2] project, I use this to call the right provider endpoint. When the user clicks on the Facebook logo, for instance, it logs in with Facebook. It's up to you as to how this UI is presented.","title":"Adding Authentication to a Mobile Client"},{"location":"chapter2/social/#client-flow-for-social-providers","text":"In each of the social providers, the identity provider SDK (provided by Facebook, Google, Microsoft or Twitter) will need to be integrated. In general, these SDKs are provided for a native platform (Objective-C or Swift for iOS, Java for Android), use callbacks or delegates (as is common practice in native libraries) and are thus more complicated to integrate with your mobile client than those that have a C#/.NET SDK delivered on NuGet. Warn Testing Client Flow for social providers requires that the social app be installed on the device. You cannot install other apps on the iOS simulator and there may be restrictions on the Android Emulator. This means that you generally need to test client flow for social providers on an actual device. The reward for doing so are a more integrated experience on mobile devices. For example, if you integrate the Google Play Services SDK in an Android app, the app will seamlessly authenticate itself with the connected Google account in the background, avoiding the need for repeatedly authenticating the client. It may ask for a fingerprint instead if the app is not trusted. If you integrate the Facebook SDK, then the app will automatically switch to the Facebook app and ask you to approve the authentication request there instead of authenticating the user through a web view. Both of these provide a more integrated experience for the end user, so this work is well worth pursuing. As an example, here is the client flow for Facebook. I've implemented this using the Xamarin.Facebook.iOS library, which can be downloaded and installed into the iOS project from NuGet. The Services\\iOSLoginProvider.cs contains the following: #region Facebook Client Flow private TaskCompletionSource<string> fbtcs; public async Task<string> LoginFacebookAsync() { fbtcs = new TaskCompletionSource<string>(); var loginManager = new LoginManager(); loginManager.LogInWithReadPermissions(new[] { \"public_profile\" }, RootView, LoginTokenHandler); return await fbtcs.Task; } private void LoginTokenHandler(LoginManagerLoginResult loginResult, NSError error) { if (loginResult.Token != null) { fbtcs.TrySetResult(loginResult.Token.TokenString); } else { fbtcs.TrySetException(new Exception(\"Facebook Client Flow Login Failed\")); } } #endregion Note the use of a TaskCompletionSource<>() here. This is used often to convert callback APIs into awaitable APIs. We set off the async call with the callback, then await on the completion (which is signified by the TaskCompletionSource ). When the callback is called, it sets the value of the TaskCompletionSource (or causes an exception) and that causes the task to complete. The LoginAsync() method can now be updated like this: public async Task LoginAsync(MobileServiceClient client) { var accessToken = await LoginFacebookAsync(); var zumoPayload = new JObject() { [\"access_token\"] = accessToken }; await client.LoginAsync(\"facebook\", zumoPayload); } public UIViewController RootView => UIApplication.SharedApplication.KeyWindow.RootViewController; Finally, you need to configure your Facebook settings within the Info.plist file. Right-click the Info.plist file and select Open with... . Select the XML editor. Add the following to the file within the <dict> element (right before the closing </dict> ): <key>FacebookAppID</key> <string>YOUR-APP-ID</string> <key>LSApplicationQueriesSchemes</key> <array> <string>fbauth2</string> </array> <key>CFBundleURLTypes</key> <array> <dict> <key>CFBundleURLSchemes</key> <array> <string>fbYOUR-APP-ID</string> </array> </dict> </array> Replace YOUR-APP-ID with the Facebook App ID from the Facebook developers console. With this version, clicking on the login button will seamlessly switch into the Facebook application and ask the user to confirm the request, before switching back authenticated. Note that it's likely that the Facebook SDK will not work within a simulator as it requires the Facebook app to be installed. There are a number of pre-built Xamarin libraries for handling provider authentication. For iOS, we have already shown Azure Active Directory and Facebook. Android has its own version of the Facebook SDK . Use the Google .NET API to access Google accounts. It's already cross-platform. In general, using the library from the provider itself is preferable to using one from a third party.","title":"Client-Flow for Social Providers"},{"location":"chapter3/client/","text":"Handling Data in Mobile Clients \u00b6 Pretty much any non-trivial mobile client will require access to data. Although we have already brushed on handling data within the client, this section will go deeper into the data handling aspects on the client. We will cover how to get and process data, how to deal with performance and reliability and some of the quirks that one must deal with when dealing with offline data. An Online Client \u00b6 We've already seen an example of an online client in our online TaskList project. There is a method for obtaining a reference to an online table: var table = client.GetTable<Model>(); This method relies on the fact that the table name is the same as the model. One must have a consistent naming scheme - the model on the server, table controller on the server, model on the client and table on the client must all be based on the same root name. This is definitely a best practice. You can produce an un-typed table: var table = client.GetTable(\"todoitem\"); This version of the method returns an untyped table. Whereas a typed table is based on a concrete model, an untyped table is based on a JSON object. This allows one to access data when the model is unknown or hard to represent in a model. You should never use an untyped table unless there is no other way of achieving whatever operation you need. All tables implement the IMobileServiceTable interface: ReadAsync() performs reads against the table. LookupAsync() reads a single record in the table, identified by its id. InsertAsync() inserts a new record into the table. UpdateAsync() updates an existing record in the table. DeleteAsync() deletes a record in the table. UndeleteAsync() un-deletes a deleted record (if soft-delete is turned on). When developing my interface, I tend to wrap my table interface into another class. This isn't because I like wrapping classes. Rather it is because the return values from many of the methods are not compatible with the general patterns used when working with a UI. For instance, the ReadAsync() method returns an IEnumerable<> type. However, the standard list management in Xamarin and UWP applications use an ObservableCollection<> instead. One has to do a conversion from one to the other. Let's look at a standard table wrapper: using System.Collections.Generic; using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudTable<T> : ICloudTable<T> where T : TableData { IMobileServiceTable<T> table; public AzureCloudTable(MobileServiceClient client) { this.table = client.GetTable<T>(); } #region ICloudTable interface public async Task<T> CreateItemAsync(T item) { await table.InsertAsync(item); return item; } public async Task<T> UpsertItemAsync(T item) { return (item.Id == null) ? await CreateItemAsync(item) : await UpdateItemAsync(item); } public async Task DeleteItemAsync(T item) => await table.DeleteAsync(item); public async Task<ICollection<T>> ReadAllItemsAsync() => await table.ToListAsync(); public async Task<T> ReadItemAsync(string id) => await table.LookupAsync(id); public async Task<T> UpdateItemAsync(T item) { await table.UpdateAsync(item); return item; } #endregion } } This is the AzureCloudTable class that our task list has been using thus far. It's actually got a few bugs in it. Let's go over them. Probably the most egregious bug is that the ReadAllItemsAsync() method does not handle paging. If you have more than 50 items, then the ToListAsync() method will do a single GET operation and then return the results. The Azure Mobile Apps Server SDK implements enforced paging. This protects two things. Firstly, the client cannot tie up the UI thread and cause a significant delay in the responsiveness of the app. More importantly, a rogue client cannot tie up the server for a long period thus helping with dealing with denial of service attacks. Paging is a good thing. To test this: Insert over 50 records into the TodoItems table in your database using a SQL client. Put a break point at the Items.ReplaceRange(list); (line 78 approximately) in ViewModels\\TaskListViewModel.cs . Run the UWP project. Note that even though there are more than 50 records, you will only see 50 records in the list. There are multiple ways to fix this and it depends on your final expectation. In the class of \"probably not what we want\", we can keep on reading records until there are no more records to read. This is the simplest to implement. In the Services\\AzureCloudTable.cs file, replace the ReadAllItemsAsync() method with the following: public async Task<ICollection<T>> ReadAllItemsAsync() { List<T> allItems = new List<T>(); var pageSize = 50; var hasMore = true; while (hasMore) { var pageOfItems = await table.Skip(allItems.Count).Take(pageSize).ToListAsync(); if (pageOfItems.Count > 0) { allItems.AddRange(pageOfItems); } else { hasMore = false; } } return allItems; } This code will always make a minimum of 2 requests if there is any data. If you have 75 records, three requests will be made - the first will bring down 50 records, the second 25 records and the third no records. Why not stop at the second request? We expect this code to run on a live system. The OData subsystem is allowed to return less than the requested value and it will do so for a variety of reasons. For example, it may be configured with a maximum transfer size and the records won't fit into the transfer buffer. The only way of knowing for sure that you have received all the records is to do a request and be told there is no more. This code could be simplified quite a bit. The reason I am not doing so is that this is not how you would want to do the transfer of items in a real application. Doing this will tie up the UI thread of your application for quite a while as the AzureCloudTable downloads all the data. Consider if there were thousands of entries? This method would be problematic very quickly. The alternative is to incrementally load the data as it is needed. This means that your UI thread will pause as the data is loaded, but the resulting UI will be less memory hungry and overall more responsive. We start by adjusting our Abstractions\\ICloudTable.cs to add a method signature for returning paged data: public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task<T> UpsertItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); Task<ICollection<T>> ReadItemsAsync(int start, int count); } The ReadItemsAsync() method is our new method here. The concrete implementation uses .Skip() and .Take() to return just the data that is required: public async Task<ICollection<T>> ReadItemsAsync(int start, int count) { return await table.Skip(start).Take(count).ToListAsync(); } Now that we have a method for paging through the contents of our table, we need to be able to wire that up to our ListView . Xamarin Forms has a concept called Behaviors that lets us add functionality to user interface controls without having to completely re-write them or sub-class them. We can use a behavior to implement a reusable paging control for a ListView. Xamarin provides a sample for this called EventToCommandBehavior (along with an explanation ). We are going to be using the ItemAppearing event and that event uses the ItemVisibilityEventArgs as a parameter. We need a converter for the EventToCommandBehavior class (in Converters\\ItemVisibilityConverter.cs ): using System; using System.Globalization; using Xamarin.Forms; namespace TaskList.Converters { public class ItemVisibilityConverter : IValueConverter { public object Convert(object value, Type targetType, object parameter, CultureInfo culture) { var eventArgs = value as ItemVisibilityEventArgs; return eventArgs.Item; } public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture) { throw new NotImplementedException(); } } } This is wired up with some XAML code in Pages\\TaskList.xaml.cs . There are two pieces. Firstly, we must define the ItemVisibilityConverter that we just wrote. This is done at the top of the file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.TaskList\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:behaviors=\"clr-namespace:TaskList.Behaviors;assembly=TaskList\" xmlns:converters=\"clr-namespace:TaskList.Converters;assembly=TaskList\" Title=\"{Binding Title}\"> <ContentPage.Resources> <ResourceDictionary> <converters:ItemVisibilityConverter x:Key=\"ItemVisibilityConverter\" /> </ResourceDictionary> </ContentPage.Resources> Next, we must define the behavior for the ListView: <ListView CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"True\" IsRefreshing=\"{Binding IsBusy, Mode=OneWay}\" ItemsSource=\"{Binding Items}\" RefreshCommand=\"{Binding RefreshCommand}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.Behaviors> <behaviors:EventToCommandBehavior Command=\"{Binding LoadMoreCommand}\" Converter=\"{StaticResource ItemVisibilityConverter}\" EventName=\"ItemAppearing\" /> </ListView.Behaviors> Finally, we need to add a new command to our TaskListViewModel to load more items. This involves firstly defining the new command: public TaskListViewModel() { CloudTable = CloudService.GetTable<TodoItem>(); Title = \"Task List\"; RefreshCommand = new Command(async () => await Refresh()); AddNewItemCommand = new Command(async () => await AddNewItem()); LogoutCommand = new Command(async () => await Logout()); LoadMoreCommand = new Command<TodoItem> (async (TodoItem item) => await LoadMore(item)); // Subscribe to events from the Task Detail Page MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await Refresh(); }); // Execute the refresh command RefreshCommand.Execute(null); } public ICommand LoadMoreCommand { get; } We also need to define the actual command code: bool hasMoreItems = true; async Task LoadMore(TodoItem item) { if (IsBusy) { Debug.WriteLine($\"LoadMore: bailing because IsBusy = true\"); return; } // If we are not displaying the last one in the list, then return. if (!Items.Last().Id.Equals(item.Id)) { Debug.WriteLine($\"LoadMore: bailing because this id is not the last id in the list\"); return; } // If we don't have more items, return if (!hasMoreItems) { Debug.WriteLine($\"LoadMore: bailing because we don't have any more items\"); return; } IsBusy = true; try { var list = await CloudTable.ReadItemsAsync(Items.Count, 20); if (list.Count > 0) { Debug.WriteLine($\"LoadMore: got {list.Count} more items\"); Items.AddRange(list); } else { Debug.WriteLine($\"LoadMore: no more items: setting hasMoreItems= false\"); hasMoreItems = false; } } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"LoadMore Failed\", ex.Message, \"OK\"); } finally { IsBusy = false; } } I've added a whole bunch of debug output because this command is called a lot, so I can scroll back through the output window instead of setting a breakpoint and clicking Continue a lot. As the UI displays each cell, it calls our command. The command figures out if the record being displayed is the last one in the list. If it is, it asks for more records. Once no more records are available, it sets the flag hasMoreItems to false so it can short-circuit the network request. Tip Be careful when using OrderBy() with online data. Earlier pages may change, causing duplicated data or missed data in your result set. There is little you can do about this other than pulling down all the data ordered by the CreatedAt field (which is the default). Finally, our current implementation of the Refresh() method loads all the items. We need to adjust it to only load the first page: async Task Refresh() { if (IsBusy) return; IsBusy = true; try { var identity = await CloudService.GetIdentityAsync(); if (identity != null) { var name = identity.UserClaims.FirstOrDefault(c => c.Type.Equals(\"name\")).Value; Title = $\"Tasks for {name}\"; } var list = await CloudTable.ReadItemsAsync(0, 20); Items.ReplaceRange(list); hasMoreItems = true; } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Items Not Loaded\", ex.Message, \"OK\"); } finally { IsBusy = false; } } We've done two things here. We have altered the first request so that only the first 20 records are retrieved. We have set hasMoreItems to true so that the LoadMore() command will do network requests again. Info The Azure Mobile Apps SDK also has a collection class that can be used: MobileServiceCollection<T,T> . This handles the paging for you and implements the ICollectionChanged event. For simple cases where you don't want to do on-demand loading, you can use the MobileServiceCollection<T,T> collection class. Query Support in Online Clients \u00b6 When using an online client, you can also use an OData query to look for records. The following code snippet, for example, will only return records that are incomplete: return await table .Where(item => item.Complete == false) .ToListAsync() This is a standard LINQ query. Just as the LINQ query was used to adjust the SQL that is generated in the server-side code, the LINQ query here is used to adjust the OData query that is generated to call the server. This particular query will generate the follow HTTP call: GET /tables/todoitem?$filter=(complete+eq+false) HTTP/1.1 LINQ queries are very useful in dealing with online data. In general they should be formatted in a particular order so that the best possible format across the Internet to your backend. table // start with the table reference .Where(filter) // filter the results .Select(filter) // Only return certain fields .Skip(start).Take(count) // paging support .ToListAsync() // convert to something we can use This format allows you to be very specific about what you want to return from the server, thus allowing you to balance the optimization of bandwidth with the responsiveness of the UI. An Offline Client \u00b6 Another method of optimizing bandwidth utilization with an added benefit of providing a resilient data connection is to use an offline sync database. The Azure Mobile Apps Client SDK has a built-in offline mode that allows for the common requirements of bandwidth optimization, connection resliency, and performance optimization. An offline capable table uses Incremental Sync to only bring down new records from the server. Connection detection allows you to defer requests until after a connection is available. In-built Conflict Resolution provides a robust mechanism for handling changes while your client was offline. Since the data is always local, the UI will be much more responsive to changes. All of this comes at a cost. We need to maintain an offline sync database for the tables you wish to provide offline, which takes up memory. It may result in large data transfers (especially in the cases of the first sync operation and rapidly changing tables). Finally, there is more complexity. We have to deal with the offline table maintenance and conflict resolution patterns. Azure Mobile Apps v3.0 Azure Mobile Apps introduced a breaking change in how SQLite was used in v3.0 of the Client SDK. The breaking change ensured support for Android Nougat. This book covers the v3.1 release of Azure Mobile Apps. In general, always start a project using the latest generally available release of the SDK from NuGet. There are three stages to using an offline client: Initialize the local SQLite database before use. Set up push / pull logic to maintain the contents of the SQLite database. Configure appropriate conflict resolution. The actual code for your mobile client that deals with tables remains identical to the online case. To effect the offline sync, the Azure Mobile Apps client SDK keeps a table in the local SQLite database called the Operations Queue . An entry is placed in the operations queue whenever a modification is done to a sync table. These modifications are collapsed. Later modifications to the same record in a sync table will be collapsed into a single update when transmitted to the mobile backend. Configuring the Local SQLite database \u00b6 We must initialize the local database before we can use it. This happens on start up and the Azure Mobile Apps SDK knows enough of the models to understand the basics of maintaining the database. Initializing the database is deceptively simple. Install the Microsoft.Azure.Mobile.Client.SQLiteStore package to all the client projects. Tip The Microsoft.Azure.Mobile.Client package v3.0.2 drastically improved the startup experience for offline sync. Make sure you are using it. If you are using an earlier version, additional steps are required for initializing your offline cache. In the Services\\AzureCloudService.cs file, add the following method: #region Offline Sync Initialization async Task InitializeAsync() { // Short circuit - local database is already initialized if (Client.SyncContext.IsInitialized) return; // Create a reference to the local sqlite store var store = new MobileServiceSQLiteStore(\"offlinecache.db\"); // Define the database schema store.DefineTable<TodoItem>(); // Actually create the store and update the schema await Client.SyncContext.InitializeAsync(store); } #endregion Tip The Microsoft.Azure.Mobile.Client package does not support Android API versions earlier than API 19. To set this, right-click on the TaskList.Droid project and ensure the Minimum Android version to target is set to a minimum of API level 19. We need to ensure the initialization is carried out before we use the local database. The best place to do this is in the GetTable<> method: /// <summary> /// Returns a link to the specific table. /// </summary> /// <typeparam name=\"T\">The model</typeparam> /// <returns>The table reference</returns> public async Task<ICloudTable<T>> GetTableAsync<T>() where T : TableData { await InitializeAsync(); return new AzureCloudTable<T>(Client); } Note that we made the routine async during this process. Adjust the ICloudService interface and the calls to GetTable<> in the rest of the code to compensate for this. Updating the Sync tables \u00b6 We also need to add some routines to our ICloudTable<> and AzureCloudTable<> classes to effect a synchronization. The first method is added to the ICloudTable<> interface and pulls data from the mobile backend: public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task<T> UpsertItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); Task<ICollection<T>> ReadItemsAsync(int start, int count); Task PullAsync(); } The new method is PullAsync() which will do the \"pull data from the server\" operation. The AzureCloudTable<> class has the concrete implementation: IMobileServiceSyncTable<T> table; public AzureCloudTable(MobileServiceClient client) { table = client.GetSyncTable<T>(); } public async Task PullAsync() { string queryName = $\"incsync_{typeof(T).Name}\"; await table.PullAsync(queryName, table.CreateQuery()); } Note that we have changed the call to get the table reference. It's now a SyncTable . You can test to see if the sync table has been defined with the following: if (client.IsSyncTable(typeof(T).Name)) { // There is a sync table defined } This can be used to have a single AzureCloudTable implementation for both the online and offline capabilities. The concrete implementation of the PullAsync() method implements incremental sync. If we set the queryName to null , then the entire table is pulled across each time. By setting a queryName, the last updated time for the table is stored and associated with the queryName. The last updated time is used to request only records that have changed since the last updated time when doing the pull from the backend. The push of the operations queue up to the mobile backend is handled by a single call in the AzureCloudService class: public async Task SyncOfflineCacheAsync() { await InitializeAsync(); // Push the Operations Queue to the mobile backend await Client.SyncContext.PushAsync(); // Pull each sync table var taskTable = await GetTableAsync<TodoItem>(); await taskTable.PullAsync(); } note that if the mobile client tries to pull data while there are pending operations in the operations queue, the Azure Mobile Apps client SDK will perform an implicit push. You can check the state of the operations queue with: if (Client.SyncContext.PendingOperations > 0) { // There are pending operations } The only thing left to do now is to decide when to synchronize the local database on the mobile device. In this example, I am going to synchronize the database during the refresh command of the TaskListViewModel and on saving or deleting an item in the TaskDetailViewModel . Each synchronization will be called with the following: await CloudService.SyncOfflineCacheAsync(); Info The offline sync cache automatically handles paging of results during the transfer for you so you never have to worry about it. Additional Steps on Universal Windows \u00b6 If you are compiling the Universal Windows (UWP) project, there is an extra step. You must Add a reference for SQLite to the project: Open the TaskList.UWP project. Right-click on the References node, then select Add Reference . Select Universal Windows -> Extensions . Place a check mark next to the SQLite for Universal Windows and Visual C++ 2015 Runtime for Universal Windows components. If you don't do this, you will get the rather cryptic error message \"Unable to set temporary directory\" when running the application. Detecting Connection State \u00b6 Xamarin has a rather cool plugin called the Connectivity Plugin for testing connectivity state. We can install it by installing the Xam.Plugin.Connectivity NuGet package. Once that is installed, we can update our SyncOfflineCacheAsync() method to use it: public async Task SyncOfflineCacheAsync() { await InitializeAsync(); if (!(await CrossConnectivity.Current.IsRemoteReachable(Client.MobileAppUri.Host, 443))) { Debug.WriteLine($\"Cannot connect to {Client.MobileAppUri} right now - offline\"); return; } // Push the Operations Queue to the mobile backend await Client.SyncContext.PushAsync(); // Pull each sync table var taskTable = await GetTableAsync<TodoItem>(); await taskTable.PullAsync(); } The Connectivity Plugin (which is accessible through CrossConnectivity.Current ) has other parameters that allow the mobile client to test for specific types of connectivity. For example, let's say you only wanted to pull one of the tables over wifi. You could do this as follows: var connections = CrossConnectivity.Current.ConnectionTypes; if (connections.Contains(ConnectionType.Wifi)) { var largeTable = await GetTableAsync<LargeModel>(); largeTable.PullAsync(); } The Connectivity plugin when paired with the Azure Mobile Apps Client SDK gives you a lot of flexibility in deciding what to sync and over what type of connection you should sync. Handling Conflict Resolution \u00b6 When the mobile client submits a modification to the mobile backend, it's generally done as a two-step process: A \"pre-condition check\" is done, comparing the Version to the ETag of the record (which is derived from the Version of the record). The actual request is done, where the Version of the mobile client is compared to the Version of the record on the mobile backend. If the version of the record being submitted is different from the version on the server, the test will fail. In the case of the first check, a 412 Precondition Failed will be returned for the modification. If the second test fails, a 409 Conflict response is returned. Normally, you will see the 412 Precondition Failed response, but you should be prepared for either response. Both responses indicate a conflict that needs to be resolved before continuing. Programatically, when doing a modification in an online table (such as an Insert, Update, Delete), you should be capture the MobileServicePreconditionFailedException<> Exception to handle the conflict. In an offline sync table, you should capture the MobileServicePushFailedException during the PushAsync() operation. This will potentially have an array of conflicts for you to deal with. For the online case, the code would look like this: try { await CloudService.InsertAsync(item); } catch (MobileServicePreconditionFailedException<TodoItem> ex) { await ResolveConflictAsync(item, ex.Item); } For the offline case, the code would look like this: try { await CloudService.PushAsync(); } catch (MobileServicePushFailedException ex) { if (ex.PushResult != null) { foreach (var error in ex.PushResult.Errors) { await ResolveConflictAsync(error); } } } In both cases, the ResolveConflictAsync() method is called for each conflict in turn. Resolving the conflict involves deciding between the two options or making corrections to the item. If you wish to keep the client copy, set the client Version property to the server Version value and re-submit. If you wish to keep the server copy, discard the update. If you wish to do something else, create a new copy of the record, set the Version to be the same as the server Version, then re-submit. Info If the model on your mobile client does not have a Version field, the policy is \"last write wins\". The online case is relatively simple as you are going to be ignoring the old request and creating a new request. Here is some template code that implements both client-wins and server-wins strategies for the offline case: sync Task ResolveConflictAsync(MobileServiceTableOperationError error) { var serverItem = error.Result.ToObject<T>(); var localItem = error.Item.ToObject<T>(); // Note that you need to implement the public override Equals(TodoItem item) // method in the Model for this to work if (serverItem.Equals(localItem)) { // Items are the same, so ignore the conflict await error.CancelAndDiscardItemAsync(); return; } // Client Always Wins localItem.Version = serverItem.Version; await error.UpdateOperationAsync(JObject.FromObject(localItem)); // Server Always Wins // await error.CancelAndDiscardItemAsync(); } You could also ask the user as an option. Finally, you could do some processing. For example, let's say that you wanted to keep the local version of the Text, but keep the server version of Complete: localItem.Complete = serverItem.Complete; localItem.Version = serverItem.Version; await error.UpdateOperationAsync(JObject.FromObject(serverItem)); There are many ways to resolve a conflict. You should consider your requirements carefully. Asking the user should always be the last resort for conflict resolution. In the majority of applications, most users will click the \"keep my version\" button to resolve conflicts, so the UI for resolving conflicts should do more than just ask the user to decide between a server and a client version. Query Management \u00b6 Record selection is based on two factors: Security policy is enforced at the server. User preference is enabled at the client. We've already discussed security policy in depth in the last section . We've also discussed user queries for online usage. We just use LINQ to affect a change in the query sent to the server. However, what about offline cases? There are situations where you want to keep a smaller subset of the data that you are allowed to see for offline usage. A common request, for example, is to have the last X days of records available offline. In our PullAsync() call for the table, we use table.CreateQuery() to create a query that is designed to get all the records available to the user from the server. This is not always appropriate and can be adjusted. Let's adjust it to only obtain the records for our TodoItem table where either of the following conditions apply: The record has been updated within the last day. The task is incomplete. Once the task is marked completed, it will remain in the cache for 1 day, then be removed automatically. You can still obtain the task while online. The table.CreateQuery() method produces an IMobileServiceTableQuery<> object. This can be dealt with via LINQ: var queryName = $\"incsync:r:{typeof(T).Name}\"; var query = table.CreateQuery() .Where(r => !r.Complete || r.UpdatedAt < DateTimeOffset.Now.AddDays(-1)); await table.PullAsync(queryName, query); Note that I am using a different query name in this case. The maximum value of UpdatedAt for the records is stored and associated with the query name. If the same query name is used, then only the records updated since the stored date will be retrieved. If you change the query, then change the query name. Another common request is to restrict the properties that are in the local cache. For instance, maybe you have a particularly large text blob that you want to make available online, but not offline: var queryName = $\"incsync:s:{typeof(T).Name}\"; var query = table.CreateQuery() .Select(r => new { r.Text, r.Complete, r.UpdatedAt, r.Version }); await table.PullAsync(queryName, query); You can also use the Fluent syntax: var query = from r in table.CreateQuery() select new { r.Text, r.Complete, r.UpdatedAt, r.Version }; You should always construct the object including the UpdatedAt and Version properties. UpdatedAt is used for incremental sync and Version is used for conflict resolution. Both of these cases use standard LINQ syntax to adjust the query being sent to the mobile backend in exactly the same way that we adjusted the query when we were doing online searches. An offline \"pull\" is exactly the same as an online \"query\". Tip There are times when you want to download two different queries. Avoid this if at all possible as it will cause additional requests to the backend that are un-necessary. Construct your query such that all records that you want to download to the offline cache are requested at once. Dealing with Historical Data \u00b6 Let's continue our example with a small extension. Let's say you want to have the last 7 days worth of records available offline, but you still want the ability to do a historical search. In this case, you can create two table references to the same table - one online for historical searches and one offline for the normal use case: var table = client.GetSyncTable<Message>(); var historicalTable = client.GetTable<Message>(); In this case, the table reference is used to access the offline data. However, you could implement a search capability that does a query against the historicalTable instead. They both point to the same table. In one case, the server is referenced (and only available online) and in the other, the local cache is referenced (and available offline). Purging the Local Cache \u00b6 It will be inevitable that you will want to clear the local cache at some point. You have just changed the model and underlying data and need to re-establish a baseline. You only cache newer data and want to remove historical data. Things got corrupt for some reason and you need to refresh everything. Whatever the reason, clearing the cache is one of those infrequent things that is going to be a necessity. There are three forms of this operation: Deleting the backing store \u00b6 The major requirement during development is that you want to delete the SQLite file that backs the offline cache. It's likely that the model evolves over time during development. Add test data that can sometimes cause things to go wrong and you have a recipe for bugs that are only there because you are developing. If you suspect bad data in the offline cache, the first thing you want to do is start afresh. Each platform stores the offline cache in a different place and the mechanism for removing it is different in each case. For Universal Windows projects, the offline cache is stored in the Local AppData folder for the application. This is a dedicated area that each Universal Windows app has access to for anything from temporary files to settings and cache files. To find the location of the file, open the TaskList.UWP project and open the Package.appxmanifest file. Go to the Packaging tab. Note the long Package family name field. Your backing file is in your home directory under AppData\\Local\\Packages\\{family_name}\\LocalState . You specified the name of the file when you created the store. You need to find the Package Name for Android. Right-click on the TaskList.Droid project and select Properties , then select the Android Manifest tab. The database will be located in /data/data/{package_name}/files directory on the emulator. Google has provided utilities for handling developer connections to devices (including emulators). In this case, we can use the adb utility. First, start your emulator of choice through the Tools -> Android -> Android Emulator Manager menu option. Highlight the emulator you have been using, then click Start . Ensure the emulator is fully started before continuing. The adb utility can be accessed through Visual Studio using Tools -> Android -> Android Adb Command Prompt . You will be able to use adb commands from there. Use adb devices to find the device and adb connect to connect to the device. This opens up a Linux-like shell onto the Android device. You can use normal Linux commands to move around. You can remove the entire private data area for your package using the following: **root@donatello:/#** cd /data/data/Tasklist.Droid.TaskList.Droid **root@donatollo:/#** find . -name tasklist.db -print | xargs rm The database will normally be in the files directory. Use exit to close the shell prompt on the Android device. Each disk image file is independent. You must remove the database file on each emulator individually. Tip You can use the same adb commands to connect to a real Android device connected via USB. Ensure USB Debugging is enabled on the device. Use adb devices to find the device. For more information, see the Android documentation . The iOS Simulator does not use an image files. Instead, it stores files on your Mac disk in ~/Library/Developer/CoreSimulator/Devices . There is a file called device_set.plist that contains the list of devices that are defined and their location. It is most easy to find a specific device. For example, if you are testing on the iPhone 6x simulator: $ grep -B 1 'iPhone-6s<' device_set.plist <string>A3536AA4-0678-43CC-BA21-DD997B89778A</string> <key>com.apple.CoreSimulator.SimDeviceType.iPhone-6s</key> -- <string>83D08BC0-2F9A-4479-ABBD-A69858819E93</string> <key>com.apple.CoreSimulator.SimDeviceType.iPhone-6s</key> -- <string>ECAE441D-93F8-4D7A-BF14-7FA2D11BC152</string> <key>com.apple.CoreSimulator.SimDeviceType.iPhone-6s</key> Each one of these corresponds to a different OS version. You can find the ordering like this: $ grep SimRuntime.iOS device_set.plist <key>com.apple.CoreSimulator.SimRuntime.iOS-9-1</key> <key>com.apple.CoreSimulator.SimRuntime.iOS-9-2</key> <key>com.apple.CoreSimulator.SimRuntime.iOS-9-3</key> My simulator is an iPhone 6s running iOS 9.3, so I can see the GUID is the third one: ECAE441D-93F8-4D7A-BF14-7FA2D11BC152 . This GUID is a directory in the same directory as the device_set.plist file. You can use normal UNIX style commands to remove the backing store: $ cd ECAE441D-93F8-4D7A-BF14-7FA2D11BC152 $ find . -name 'tasklist.db' -print | xargs rm You can also use the normal Finder utilities to search for and remove the database file for your app. Purging Records from the Offline Cache \u00b6 The IMobileServiceSyncTable interface also includes a capability for purging records that are stored in the offline sync by query. This is done in code like this: var lowerBound = DateTimeOffset.Now.AddDays(-7); var query = syncTable.CreateQuery().Where(item => item.UpdatedAt < lowerBound); var force = true; await syncTable.PurgeAsync(\"incsync_Tag\", query, force); There are four parameters for the PurgeAsync call: A query name. An OData query. A boolean for forcing the operation. A cancellation token (for the async operation). Each incremental sync query has a unique name that is specified during the PullAsync() method call. If you use the same name during the PurgeAsync() call, then the date associated with the incremental sync query is reset, causing a full refresh of the data. This allows you to do a \"purge and refresh\" operation. If you don't want this to happen, set the query name to null. The OData query is a similar query format to the incremental sync query that we used with PullAsync() . In this case, it selects the records that should be purged from the offline sync cache. If we wished to purge everything, we could just use syncTable.CreateQuery() . If we want to purge only certain records, then we can adjust the query with a .Where() LINQ query. In the example above, records that have not been updated within the last 7 days are purged. Finally, the PurgeAsync() call will fail (and generate an exception) if there are any operations pending in the operations queue. If we specify force = true , then the operations queue check is bypassed and pending operations in the operations queue are flushed without being uploaded. It is important that this option is used only when absolutely required. You can leave your database in an inconsistent state if you expect referential integrity between different tables. Use SyncContext.PushAsync() to push the operations queue to the remote server before calling PurgeAsync() . If you use force = true , then also specify a query name to reset the incremental sync state. Schema changes in the Offline Cache \u00b6 During development, it's very likely that you will want to change the schema of the data being stored on the client. Unfortunately, there isn't a really good way of dealing with this situation. Here is my solution: Create a constant within your mobile client containing a schema version number. The actual value (text or int) doesn't matter. You should never have the same schema version number twice though. Change this constant whenever you change the schema. Read a file from the mobile device with the schema version in it. If the file exists and the read value is different from the constant you created in step 1, then you are \"refreshing the cache\" If you are refreshing the cache, delete the offline cache file, then write the current schema version number to the file that you read in step 2. Set up your offline cache, define your tables, etc. If you are \"refreshing the cache\", do a full PullAsync() on all tables to populate the cache. You obviously want to avoid this process as much as possible. One possible solution is to use the VersionTrackingPlugin by Colby Williams to detect when the schema has been updated. The following code can be added to automatically delete the offline cache when the version of the app changes: // Add an isRemoteDatabaseSchemaChanged boolean somewhere if (CrossVersionTracking.Current.IsFirstLaunchForBuild) { isRemoteDatabaseSchemaChanged = true; // Drop SQLite Store file to overcome remote-db schema changes File.Delete(Constants.SqliteStorePath); } Debugging the Offline Cache \u00b6 One of the most difficult parts of the offline cache is that it is opaque - you can't really see what is going on. Fortunately, the SQLiteStore that is used is relatively straight forward to sub-class so we can add logging to it. The following helper method can be substituted for a SQLiteStore in any code. public class MobileServiceSQLiteStoreWithLogging : MobileServiceSQLiteStore { private bool logResults; private bool logParameters; public MobileServiceSQLiteStoreWithLogging(string fileName, bool logResults = false, bool logParameters = false) : base(fileName) { this.logResults = logResults; this.logParameters = logParameters; } protected override IList<Newtonsoft.Json.Linq.JObject> ExecuteQuery(string tableName, string sql, IDictionary<string, object> parameters) { Console.WriteLine (sql); if(logParameters) PrintDictionary (parameters); var result = base.ExecuteQuery(tableName, sql, parameters); if (logResults && result != null) { foreach (var token in result) Console.WriteLine (token); } return result; } protected override void ExecuteNonQuery(string sql, IDictionary<string, object> parameters) { Console.WriteLine (sql); if(logParameters) PrintDictionary (parameters); base.ExecuteNonQuery(sql, parameters); } private void PrintDictionary(IDictionary<string,object> dictionary) { if (dictionary == null) return; foreach (var pair in dictionary) Console.WriteLine (\"{0}:{1}\", pair.Key, pair.Value); } } public class LoggingHandler : DelegatingHandler { private bool logRequestResponseBody; public LoggingHandler(bool logRequestResponseBody = false) { this.logRequestResponseBody = logRequestResponseBody; } protected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, System.Threading.CancellationToken cancellationToken) { Console.WriteLine(\"Request: {0} {1}\", request.Method, request.RequestUri.ToString()); if (logRequestResponseBody && request.Content != null) { var requestContent = await request.Content.ReadAsStringAsync (); Console.WriteLine (requestContent); } var response = await base.SendAsync(request, cancellationToken); Console.WriteLine (\"Response: {0}\", response.StatusCode); if (logRequestResponseBody) { var responseContent = await response.Content.ReadAsStringAsync (); Console.WriteLine (responseContent); } return response; } } Using this class will print all the SQL commands that are executed against the SQLite store. Ensure you are capturing the console somewhere so that you can see the debug messages as you are running your application.","title":"The Mobile Client"},{"location":"chapter3/client/#handling-data-in-mobile-clients","text":"Pretty much any non-trivial mobile client will require access to data. Although we have already brushed on handling data within the client, this section will go deeper into the data handling aspects on the client. We will cover how to get and process data, how to deal with performance and reliability and some of the quirks that one must deal with when dealing with offline data.","title":"Handling Data in Mobile Clients"},{"location":"chapter3/client/#an-online-client","text":"We've already seen an example of an online client in our online TaskList project. There is a method for obtaining a reference to an online table: var table = client.GetTable<Model>(); This method relies on the fact that the table name is the same as the model. One must have a consistent naming scheme - the model on the server, table controller on the server, model on the client and table on the client must all be based on the same root name. This is definitely a best practice. You can produce an un-typed table: var table = client.GetTable(\"todoitem\"); This version of the method returns an untyped table. Whereas a typed table is based on a concrete model, an untyped table is based on a JSON object. This allows one to access data when the model is unknown or hard to represent in a model. You should never use an untyped table unless there is no other way of achieving whatever operation you need. All tables implement the IMobileServiceTable interface: ReadAsync() performs reads against the table. LookupAsync() reads a single record in the table, identified by its id. InsertAsync() inserts a new record into the table. UpdateAsync() updates an existing record in the table. DeleteAsync() deletes a record in the table. UndeleteAsync() un-deletes a deleted record (if soft-delete is turned on). When developing my interface, I tend to wrap my table interface into another class. This isn't because I like wrapping classes. Rather it is because the return values from many of the methods are not compatible with the general patterns used when working with a UI. For instance, the ReadAsync() method returns an IEnumerable<> type. However, the standard list management in Xamarin and UWP applications use an ObservableCollection<> instead. One has to do a conversion from one to the other. Let's look at a standard table wrapper: using System.Collections.Generic; using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Abstractions; namespace TaskList.Services { public class AzureCloudTable<T> : ICloudTable<T> where T : TableData { IMobileServiceTable<T> table; public AzureCloudTable(MobileServiceClient client) { this.table = client.GetTable<T>(); } #region ICloudTable interface public async Task<T> CreateItemAsync(T item) { await table.InsertAsync(item); return item; } public async Task<T> UpsertItemAsync(T item) { return (item.Id == null) ? await CreateItemAsync(item) : await UpdateItemAsync(item); } public async Task DeleteItemAsync(T item) => await table.DeleteAsync(item); public async Task<ICollection<T>> ReadAllItemsAsync() => await table.ToListAsync(); public async Task<T> ReadItemAsync(string id) => await table.LookupAsync(id); public async Task<T> UpdateItemAsync(T item) { await table.UpdateAsync(item); return item; } #endregion } } This is the AzureCloudTable class that our task list has been using thus far. It's actually got a few bugs in it. Let's go over them. Probably the most egregious bug is that the ReadAllItemsAsync() method does not handle paging. If you have more than 50 items, then the ToListAsync() method will do a single GET operation and then return the results. The Azure Mobile Apps Server SDK implements enforced paging. This protects two things. Firstly, the client cannot tie up the UI thread and cause a significant delay in the responsiveness of the app. More importantly, a rogue client cannot tie up the server for a long period thus helping with dealing with denial of service attacks. Paging is a good thing. To test this: Insert over 50 records into the TodoItems table in your database using a SQL client. Put a break point at the Items.ReplaceRange(list); (line 78 approximately) in ViewModels\\TaskListViewModel.cs . Run the UWP project. Note that even though there are more than 50 records, you will only see 50 records in the list. There are multiple ways to fix this and it depends on your final expectation. In the class of \"probably not what we want\", we can keep on reading records until there are no more records to read. This is the simplest to implement. In the Services\\AzureCloudTable.cs file, replace the ReadAllItemsAsync() method with the following: public async Task<ICollection<T>> ReadAllItemsAsync() { List<T> allItems = new List<T>(); var pageSize = 50; var hasMore = true; while (hasMore) { var pageOfItems = await table.Skip(allItems.Count).Take(pageSize).ToListAsync(); if (pageOfItems.Count > 0) { allItems.AddRange(pageOfItems); } else { hasMore = false; } } return allItems; } This code will always make a minimum of 2 requests if there is any data. If you have 75 records, three requests will be made - the first will bring down 50 records, the second 25 records and the third no records. Why not stop at the second request? We expect this code to run on a live system. The OData subsystem is allowed to return less than the requested value and it will do so for a variety of reasons. For example, it may be configured with a maximum transfer size and the records won't fit into the transfer buffer. The only way of knowing for sure that you have received all the records is to do a request and be told there is no more. This code could be simplified quite a bit. The reason I am not doing so is that this is not how you would want to do the transfer of items in a real application. Doing this will tie up the UI thread of your application for quite a while as the AzureCloudTable downloads all the data. Consider if there were thousands of entries? This method would be problematic very quickly. The alternative is to incrementally load the data as it is needed. This means that your UI thread will pause as the data is loaded, but the resulting UI will be less memory hungry and overall more responsive. We start by adjusting our Abstractions\\ICloudTable.cs to add a method signature for returning paged data: public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task<T> UpsertItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); Task<ICollection<T>> ReadItemsAsync(int start, int count); } The ReadItemsAsync() method is our new method here. The concrete implementation uses .Skip() and .Take() to return just the data that is required: public async Task<ICollection<T>> ReadItemsAsync(int start, int count) { return await table.Skip(start).Take(count).ToListAsync(); } Now that we have a method for paging through the contents of our table, we need to be able to wire that up to our ListView . Xamarin Forms has a concept called Behaviors that lets us add functionality to user interface controls without having to completely re-write them or sub-class them. We can use a behavior to implement a reusable paging control for a ListView. Xamarin provides a sample for this called EventToCommandBehavior (along with an explanation ). We are going to be using the ItemAppearing event and that event uses the ItemVisibilityEventArgs as a parameter. We need a converter for the EventToCommandBehavior class (in Converters\\ItemVisibilityConverter.cs ): using System; using System.Globalization; using Xamarin.Forms; namespace TaskList.Converters { public class ItemVisibilityConverter : IValueConverter { public object Convert(object value, Type targetType, object parameter, CultureInfo culture) { var eventArgs = value as ItemVisibilityEventArgs; return eventArgs.Item; } public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture) { throw new NotImplementedException(); } } } This is wired up with some XAML code in Pages\\TaskList.xaml.cs . There are two pieces. Firstly, we must define the ItemVisibilityConverter that we just wrote. This is done at the top of the file: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.TaskList\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:behaviors=\"clr-namespace:TaskList.Behaviors;assembly=TaskList\" xmlns:converters=\"clr-namespace:TaskList.Converters;assembly=TaskList\" Title=\"{Binding Title}\"> <ContentPage.Resources> <ResourceDictionary> <converters:ItemVisibilityConverter x:Key=\"ItemVisibilityConverter\" /> </ResourceDictionary> </ContentPage.Resources> Next, we must define the behavior for the ListView: <ListView CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"True\" IsRefreshing=\"{Binding IsBusy, Mode=OneWay}\" ItemsSource=\"{Binding Items}\" RefreshCommand=\"{Binding RefreshCommand}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.Behaviors> <behaviors:EventToCommandBehavior Command=\"{Binding LoadMoreCommand}\" Converter=\"{StaticResource ItemVisibilityConverter}\" EventName=\"ItemAppearing\" /> </ListView.Behaviors> Finally, we need to add a new command to our TaskListViewModel to load more items. This involves firstly defining the new command: public TaskListViewModel() { CloudTable = CloudService.GetTable<TodoItem>(); Title = \"Task List\"; RefreshCommand = new Command(async () => await Refresh()); AddNewItemCommand = new Command(async () => await AddNewItem()); LogoutCommand = new Command(async () => await Logout()); LoadMoreCommand = new Command<TodoItem> (async (TodoItem item) => await LoadMore(item)); // Subscribe to events from the Task Detail Page MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await Refresh(); }); // Execute the refresh command RefreshCommand.Execute(null); } public ICommand LoadMoreCommand { get; } We also need to define the actual command code: bool hasMoreItems = true; async Task LoadMore(TodoItem item) { if (IsBusy) { Debug.WriteLine($\"LoadMore: bailing because IsBusy = true\"); return; } // If we are not displaying the last one in the list, then return. if (!Items.Last().Id.Equals(item.Id)) { Debug.WriteLine($\"LoadMore: bailing because this id is not the last id in the list\"); return; } // If we don't have more items, return if (!hasMoreItems) { Debug.WriteLine($\"LoadMore: bailing because we don't have any more items\"); return; } IsBusy = true; try { var list = await CloudTable.ReadItemsAsync(Items.Count, 20); if (list.Count > 0) { Debug.WriteLine($\"LoadMore: got {list.Count} more items\"); Items.AddRange(list); } else { Debug.WriteLine($\"LoadMore: no more items: setting hasMoreItems= false\"); hasMoreItems = false; } } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"LoadMore Failed\", ex.Message, \"OK\"); } finally { IsBusy = false; } } I've added a whole bunch of debug output because this command is called a lot, so I can scroll back through the output window instead of setting a breakpoint and clicking Continue a lot. As the UI displays each cell, it calls our command. The command figures out if the record being displayed is the last one in the list. If it is, it asks for more records. Once no more records are available, it sets the flag hasMoreItems to false so it can short-circuit the network request. Tip Be careful when using OrderBy() with online data. Earlier pages may change, causing duplicated data or missed data in your result set. There is little you can do about this other than pulling down all the data ordered by the CreatedAt field (which is the default). Finally, our current implementation of the Refresh() method loads all the items. We need to adjust it to only load the first page: async Task Refresh() { if (IsBusy) return; IsBusy = true; try { var identity = await CloudService.GetIdentityAsync(); if (identity != null) { var name = identity.UserClaims.FirstOrDefault(c => c.Type.Equals(\"name\")).Value; Title = $\"Tasks for {name}\"; } var list = await CloudTable.ReadItemsAsync(0, 20); Items.ReplaceRange(list); hasMoreItems = true; } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Items Not Loaded\", ex.Message, \"OK\"); } finally { IsBusy = false; } } We've done two things here. We have altered the first request so that only the first 20 records are retrieved. We have set hasMoreItems to true so that the LoadMore() command will do network requests again. Info The Azure Mobile Apps SDK also has a collection class that can be used: MobileServiceCollection<T,T> . This handles the paging for you and implements the ICollectionChanged event. For simple cases where you don't want to do on-demand loading, you can use the MobileServiceCollection<T,T> collection class.","title":"An Online Client"},{"location":"chapter3/client/#query-support-in-online-clients","text":"When using an online client, you can also use an OData query to look for records. The following code snippet, for example, will only return records that are incomplete: return await table .Where(item => item.Complete == false) .ToListAsync() This is a standard LINQ query. Just as the LINQ query was used to adjust the SQL that is generated in the server-side code, the LINQ query here is used to adjust the OData query that is generated to call the server. This particular query will generate the follow HTTP call: GET /tables/todoitem?$filter=(complete+eq+false) HTTP/1.1 LINQ queries are very useful in dealing with online data. In general they should be formatted in a particular order so that the best possible format across the Internet to your backend. table // start with the table reference .Where(filter) // filter the results .Select(filter) // Only return certain fields .Skip(start).Take(count) // paging support .ToListAsync() // convert to something we can use This format allows you to be very specific about what you want to return from the server, thus allowing you to balance the optimization of bandwidth with the responsiveness of the UI.","title":"Query Support in Online Clients"},{"location":"chapter3/client/#an-offline-client","text":"Another method of optimizing bandwidth utilization with an added benefit of providing a resilient data connection is to use an offline sync database. The Azure Mobile Apps Client SDK has a built-in offline mode that allows for the common requirements of bandwidth optimization, connection resliency, and performance optimization. An offline capable table uses Incremental Sync to only bring down new records from the server. Connection detection allows you to defer requests until after a connection is available. In-built Conflict Resolution provides a robust mechanism for handling changes while your client was offline. Since the data is always local, the UI will be much more responsive to changes. All of this comes at a cost. We need to maintain an offline sync database for the tables you wish to provide offline, which takes up memory. It may result in large data transfers (especially in the cases of the first sync operation and rapidly changing tables). Finally, there is more complexity. We have to deal with the offline table maintenance and conflict resolution patterns. Azure Mobile Apps v3.0 Azure Mobile Apps introduced a breaking change in how SQLite was used in v3.0 of the Client SDK. The breaking change ensured support for Android Nougat. This book covers the v3.1 release of Azure Mobile Apps. In general, always start a project using the latest generally available release of the SDK from NuGet. There are three stages to using an offline client: Initialize the local SQLite database before use. Set up push / pull logic to maintain the contents of the SQLite database. Configure appropriate conflict resolution. The actual code for your mobile client that deals with tables remains identical to the online case. To effect the offline sync, the Azure Mobile Apps client SDK keeps a table in the local SQLite database called the Operations Queue . An entry is placed in the operations queue whenever a modification is done to a sync table. These modifications are collapsed. Later modifications to the same record in a sync table will be collapsed into a single update when transmitted to the mobile backend.","title":"An Offline Client"},{"location":"chapter3/client/#configuring-the-local-sqlite-database","text":"We must initialize the local database before we can use it. This happens on start up and the Azure Mobile Apps SDK knows enough of the models to understand the basics of maintaining the database. Initializing the database is deceptively simple. Install the Microsoft.Azure.Mobile.Client.SQLiteStore package to all the client projects. Tip The Microsoft.Azure.Mobile.Client package v3.0.2 drastically improved the startup experience for offline sync. Make sure you are using it. If you are using an earlier version, additional steps are required for initializing your offline cache. In the Services\\AzureCloudService.cs file, add the following method: #region Offline Sync Initialization async Task InitializeAsync() { // Short circuit - local database is already initialized if (Client.SyncContext.IsInitialized) return; // Create a reference to the local sqlite store var store = new MobileServiceSQLiteStore(\"offlinecache.db\"); // Define the database schema store.DefineTable<TodoItem>(); // Actually create the store and update the schema await Client.SyncContext.InitializeAsync(store); } #endregion Tip The Microsoft.Azure.Mobile.Client package does not support Android API versions earlier than API 19. To set this, right-click on the TaskList.Droid project and ensure the Minimum Android version to target is set to a minimum of API level 19. We need to ensure the initialization is carried out before we use the local database. The best place to do this is in the GetTable<> method: /// <summary> /// Returns a link to the specific table. /// </summary> /// <typeparam name=\"T\">The model</typeparam> /// <returns>The table reference</returns> public async Task<ICloudTable<T>> GetTableAsync<T>() where T : TableData { await InitializeAsync(); return new AzureCloudTable<T>(Client); } Note that we made the routine async during this process. Adjust the ICloudService interface and the calls to GetTable<> in the rest of the code to compensate for this.","title":"Configuring the Local SQLite database"},{"location":"chapter3/client/#updating-the-sync-tables","text":"We also need to add some routines to our ICloudTable<> and AzureCloudTable<> classes to effect a synchronization. The first method is added to the ICloudTable<> interface and pulls data from the mobile backend: public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task<T> UpsertItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); Task<ICollection<T>> ReadItemsAsync(int start, int count); Task PullAsync(); } The new method is PullAsync() which will do the \"pull data from the server\" operation. The AzureCloudTable<> class has the concrete implementation: IMobileServiceSyncTable<T> table; public AzureCloudTable(MobileServiceClient client) { table = client.GetSyncTable<T>(); } public async Task PullAsync() { string queryName = $\"incsync_{typeof(T).Name}\"; await table.PullAsync(queryName, table.CreateQuery()); } Note that we have changed the call to get the table reference. It's now a SyncTable . You can test to see if the sync table has been defined with the following: if (client.IsSyncTable(typeof(T).Name)) { // There is a sync table defined } This can be used to have a single AzureCloudTable implementation for both the online and offline capabilities. The concrete implementation of the PullAsync() method implements incremental sync. If we set the queryName to null , then the entire table is pulled across each time. By setting a queryName, the last updated time for the table is stored and associated with the queryName. The last updated time is used to request only records that have changed since the last updated time when doing the pull from the backend. The push of the operations queue up to the mobile backend is handled by a single call in the AzureCloudService class: public async Task SyncOfflineCacheAsync() { await InitializeAsync(); // Push the Operations Queue to the mobile backend await Client.SyncContext.PushAsync(); // Pull each sync table var taskTable = await GetTableAsync<TodoItem>(); await taskTable.PullAsync(); } note that if the mobile client tries to pull data while there are pending operations in the operations queue, the Azure Mobile Apps client SDK will perform an implicit push. You can check the state of the operations queue with: if (Client.SyncContext.PendingOperations > 0) { // There are pending operations } The only thing left to do now is to decide when to synchronize the local database on the mobile device. In this example, I am going to synchronize the database during the refresh command of the TaskListViewModel and on saving or deleting an item in the TaskDetailViewModel . Each synchronization will be called with the following: await CloudService.SyncOfflineCacheAsync(); Info The offline sync cache automatically handles paging of results during the transfer for you so you never have to worry about it.","title":"Updating the Sync tables"},{"location":"chapter3/client/#additional-steps-on-universal-windows","text":"If you are compiling the Universal Windows (UWP) project, there is an extra step. You must Add a reference for SQLite to the project: Open the TaskList.UWP project. Right-click on the References node, then select Add Reference . Select Universal Windows -> Extensions . Place a check mark next to the SQLite for Universal Windows and Visual C++ 2015 Runtime for Universal Windows components. If you don't do this, you will get the rather cryptic error message \"Unable to set temporary directory\" when running the application.","title":"Additional Steps on Universal Windows"},{"location":"chapter3/client/#detecting-connection-state","text":"Xamarin has a rather cool plugin called the Connectivity Plugin for testing connectivity state. We can install it by installing the Xam.Plugin.Connectivity NuGet package. Once that is installed, we can update our SyncOfflineCacheAsync() method to use it: public async Task SyncOfflineCacheAsync() { await InitializeAsync(); if (!(await CrossConnectivity.Current.IsRemoteReachable(Client.MobileAppUri.Host, 443))) { Debug.WriteLine($\"Cannot connect to {Client.MobileAppUri} right now - offline\"); return; } // Push the Operations Queue to the mobile backend await Client.SyncContext.PushAsync(); // Pull each sync table var taskTable = await GetTableAsync<TodoItem>(); await taskTable.PullAsync(); } The Connectivity Plugin (which is accessible through CrossConnectivity.Current ) has other parameters that allow the mobile client to test for specific types of connectivity. For example, let's say you only wanted to pull one of the tables over wifi. You could do this as follows: var connections = CrossConnectivity.Current.ConnectionTypes; if (connections.Contains(ConnectionType.Wifi)) { var largeTable = await GetTableAsync<LargeModel>(); largeTable.PullAsync(); } The Connectivity plugin when paired with the Azure Mobile Apps Client SDK gives you a lot of flexibility in deciding what to sync and over what type of connection you should sync.","title":"Detecting Connection State"},{"location":"chapter3/client/#handling-conflict-resolution","text":"When the mobile client submits a modification to the mobile backend, it's generally done as a two-step process: A \"pre-condition check\" is done, comparing the Version to the ETag of the record (which is derived from the Version of the record). The actual request is done, where the Version of the mobile client is compared to the Version of the record on the mobile backend. If the version of the record being submitted is different from the version on the server, the test will fail. In the case of the first check, a 412 Precondition Failed will be returned for the modification. If the second test fails, a 409 Conflict response is returned. Normally, you will see the 412 Precondition Failed response, but you should be prepared for either response. Both responses indicate a conflict that needs to be resolved before continuing. Programatically, when doing a modification in an online table (such as an Insert, Update, Delete), you should be capture the MobileServicePreconditionFailedException<> Exception to handle the conflict. In an offline sync table, you should capture the MobileServicePushFailedException during the PushAsync() operation. This will potentially have an array of conflicts for you to deal with. For the online case, the code would look like this: try { await CloudService.InsertAsync(item); } catch (MobileServicePreconditionFailedException<TodoItem> ex) { await ResolveConflictAsync(item, ex.Item); } For the offline case, the code would look like this: try { await CloudService.PushAsync(); } catch (MobileServicePushFailedException ex) { if (ex.PushResult != null) { foreach (var error in ex.PushResult.Errors) { await ResolveConflictAsync(error); } } } In both cases, the ResolveConflictAsync() method is called for each conflict in turn. Resolving the conflict involves deciding between the two options or making corrections to the item. If you wish to keep the client copy, set the client Version property to the server Version value and re-submit. If you wish to keep the server copy, discard the update. If you wish to do something else, create a new copy of the record, set the Version to be the same as the server Version, then re-submit. Info If the model on your mobile client does not have a Version field, the policy is \"last write wins\". The online case is relatively simple as you are going to be ignoring the old request and creating a new request. Here is some template code that implements both client-wins and server-wins strategies for the offline case: sync Task ResolveConflictAsync(MobileServiceTableOperationError error) { var serverItem = error.Result.ToObject<T>(); var localItem = error.Item.ToObject<T>(); // Note that you need to implement the public override Equals(TodoItem item) // method in the Model for this to work if (serverItem.Equals(localItem)) { // Items are the same, so ignore the conflict await error.CancelAndDiscardItemAsync(); return; } // Client Always Wins localItem.Version = serverItem.Version; await error.UpdateOperationAsync(JObject.FromObject(localItem)); // Server Always Wins // await error.CancelAndDiscardItemAsync(); } You could also ask the user as an option. Finally, you could do some processing. For example, let's say that you wanted to keep the local version of the Text, but keep the server version of Complete: localItem.Complete = serverItem.Complete; localItem.Version = serverItem.Version; await error.UpdateOperationAsync(JObject.FromObject(serverItem)); There are many ways to resolve a conflict. You should consider your requirements carefully. Asking the user should always be the last resort for conflict resolution. In the majority of applications, most users will click the \"keep my version\" button to resolve conflicts, so the UI for resolving conflicts should do more than just ask the user to decide between a server and a client version.","title":"Handling Conflict Resolution"},{"location":"chapter3/client/#query-management","text":"Record selection is based on two factors: Security policy is enforced at the server. User preference is enabled at the client. We've already discussed security policy in depth in the last section . We've also discussed user queries for online usage. We just use LINQ to affect a change in the query sent to the server. However, what about offline cases? There are situations where you want to keep a smaller subset of the data that you are allowed to see for offline usage. A common request, for example, is to have the last X days of records available offline. In our PullAsync() call for the table, we use table.CreateQuery() to create a query that is designed to get all the records available to the user from the server. This is not always appropriate and can be adjusted. Let's adjust it to only obtain the records for our TodoItem table where either of the following conditions apply: The record has been updated within the last day. The task is incomplete. Once the task is marked completed, it will remain in the cache for 1 day, then be removed automatically. You can still obtain the task while online. The table.CreateQuery() method produces an IMobileServiceTableQuery<> object. This can be dealt with via LINQ: var queryName = $\"incsync:r:{typeof(T).Name}\"; var query = table.CreateQuery() .Where(r => !r.Complete || r.UpdatedAt < DateTimeOffset.Now.AddDays(-1)); await table.PullAsync(queryName, query); Note that I am using a different query name in this case. The maximum value of UpdatedAt for the records is stored and associated with the query name. If the same query name is used, then only the records updated since the stored date will be retrieved. If you change the query, then change the query name. Another common request is to restrict the properties that are in the local cache. For instance, maybe you have a particularly large text blob that you want to make available online, but not offline: var queryName = $\"incsync:s:{typeof(T).Name}\"; var query = table.CreateQuery() .Select(r => new { r.Text, r.Complete, r.UpdatedAt, r.Version }); await table.PullAsync(queryName, query); You can also use the Fluent syntax: var query = from r in table.CreateQuery() select new { r.Text, r.Complete, r.UpdatedAt, r.Version }; You should always construct the object including the UpdatedAt and Version properties. UpdatedAt is used for incremental sync and Version is used for conflict resolution. Both of these cases use standard LINQ syntax to adjust the query being sent to the mobile backend in exactly the same way that we adjusted the query when we were doing online searches. An offline \"pull\" is exactly the same as an online \"query\". Tip There are times when you want to download two different queries. Avoid this if at all possible as it will cause additional requests to the backend that are un-necessary. Construct your query such that all records that you want to download to the offline cache are requested at once.","title":"Query Management"},{"location":"chapter3/client/#dealing-with-historical-data","text":"Let's continue our example with a small extension. Let's say you want to have the last 7 days worth of records available offline, but you still want the ability to do a historical search. In this case, you can create two table references to the same table - one online for historical searches and one offline for the normal use case: var table = client.GetSyncTable<Message>(); var historicalTable = client.GetTable<Message>(); In this case, the table reference is used to access the offline data. However, you could implement a search capability that does a query against the historicalTable instead. They both point to the same table. In one case, the server is referenced (and only available online) and in the other, the local cache is referenced (and available offline).","title":"Dealing with Historical Data"},{"location":"chapter3/client/#purging-the-local-cache","text":"It will be inevitable that you will want to clear the local cache at some point. You have just changed the model and underlying data and need to re-establish a baseline. You only cache newer data and want to remove historical data. Things got corrupt for some reason and you need to refresh everything. Whatever the reason, clearing the cache is one of those infrequent things that is going to be a necessity. There are three forms of this operation:","title":"Purging the Local Cache"},{"location":"chapter3/client/#deleting-the-backing-store","text":"The major requirement during development is that you want to delete the SQLite file that backs the offline cache. It's likely that the model evolves over time during development. Add test data that can sometimes cause things to go wrong and you have a recipe for bugs that are only there because you are developing. If you suspect bad data in the offline cache, the first thing you want to do is start afresh. Each platform stores the offline cache in a different place and the mechanism for removing it is different in each case. For Universal Windows projects, the offline cache is stored in the Local AppData folder for the application. This is a dedicated area that each Universal Windows app has access to for anything from temporary files to settings and cache files. To find the location of the file, open the TaskList.UWP project and open the Package.appxmanifest file. Go to the Packaging tab. Note the long Package family name field. Your backing file is in your home directory under AppData\\Local\\Packages\\{family_name}\\LocalState . You specified the name of the file when you created the store. You need to find the Package Name for Android. Right-click on the TaskList.Droid project and select Properties , then select the Android Manifest tab. The database will be located in /data/data/{package_name}/files directory on the emulator. Google has provided utilities for handling developer connections to devices (including emulators). In this case, we can use the adb utility. First, start your emulator of choice through the Tools -> Android -> Android Emulator Manager menu option. Highlight the emulator you have been using, then click Start . Ensure the emulator is fully started before continuing. The adb utility can be accessed through Visual Studio using Tools -> Android -> Android Adb Command Prompt . You will be able to use adb commands from there. Use adb devices to find the device and adb connect to connect to the device. This opens up a Linux-like shell onto the Android device. You can use normal Linux commands to move around. You can remove the entire private data area for your package using the following: **root@donatello:/#** cd /data/data/Tasklist.Droid.TaskList.Droid **root@donatollo:/#** find . -name tasklist.db -print | xargs rm The database will normally be in the files directory. Use exit to close the shell prompt on the Android device. Each disk image file is independent. You must remove the database file on each emulator individually. Tip You can use the same adb commands to connect to a real Android device connected via USB. Ensure USB Debugging is enabled on the device. Use adb devices to find the device. For more information, see the Android documentation . The iOS Simulator does not use an image files. Instead, it stores files on your Mac disk in ~/Library/Developer/CoreSimulator/Devices . There is a file called device_set.plist that contains the list of devices that are defined and their location. It is most easy to find a specific device. For example, if you are testing on the iPhone 6x simulator: $ grep -B 1 'iPhone-6s<' device_set.plist <string>A3536AA4-0678-43CC-BA21-DD997B89778A</string> <key>com.apple.CoreSimulator.SimDeviceType.iPhone-6s</key> -- <string>83D08BC0-2F9A-4479-ABBD-A69858819E93</string> <key>com.apple.CoreSimulator.SimDeviceType.iPhone-6s</key> -- <string>ECAE441D-93F8-4D7A-BF14-7FA2D11BC152</string> <key>com.apple.CoreSimulator.SimDeviceType.iPhone-6s</key> Each one of these corresponds to a different OS version. You can find the ordering like this: $ grep SimRuntime.iOS device_set.plist <key>com.apple.CoreSimulator.SimRuntime.iOS-9-1</key> <key>com.apple.CoreSimulator.SimRuntime.iOS-9-2</key> <key>com.apple.CoreSimulator.SimRuntime.iOS-9-3</key> My simulator is an iPhone 6s running iOS 9.3, so I can see the GUID is the third one: ECAE441D-93F8-4D7A-BF14-7FA2D11BC152 . This GUID is a directory in the same directory as the device_set.plist file. You can use normal UNIX style commands to remove the backing store: $ cd ECAE441D-93F8-4D7A-BF14-7FA2D11BC152 $ find . -name 'tasklist.db' -print | xargs rm You can also use the normal Finder utilities to search for and remove the database file for your app.","title":"Deleting the backing store"},{"location":"chapter3/client/#purging-records-from-the-offline-cache","text":"The IMobileServiceSyncTable interface also includes a capability for purging records that are stored in the offline sync by query. This is done in code like this: var lowerBound = DateTimeOffset.Now.AddDays(-7); var query = syncTable.CreateQuery().Where(item => item.UpdatedAt < lowerBound); var force = true; await syncTable.PurgeAsync(\"incsync_Tag\", query, force); There are four parameters for the PurgeAsync call: A query name. An OData query. A boolean for forcing the operation. A cancellation token (for the async operation). Each incremental sync query has a unique name that is specified during the PullAsync() method call. If you use the same name during the PurgeAsync() call, then the date associated with the incremental sync query is reset, causing a full refresh of the data. This allows you to do a \"purge and refresh\" operation. If you don't want this to happen, set the query name to null. The OData query is a similar query format to the incremental sync query that we used with PullAsync() . In this case, it selects the records that should be purged from the offline sync cache. If we wished to purge everything, we could just use syncTable.CreateQuery() . If we want to purge only certain records, then we can adjust the query with a .Where() LINQ query. In the example above, records that have not been updated within the last 7 days are purged. Finally, the PurgeAsync() call will fail (and generate an exception) if there are any operations pending in the operations queue. If we specify force = true , then the operations queue check is bypassed and pending operations in the operations queue are flushed without being uploaded. It is important that this option is used only when absolutely required. You can leave your database in an inconsistent state if you expect referential integrity between different tables. Use SyncContext.PushAsync() to push the operations queue to the remote server before calling PurgeAsync() . If you use force = true , then also specify a query name to reset the incremental sync state.","title":"Purging Records from the Offline Cache"},{"location":"chapter3/client/#schema-changes-in-the-offline-cache","text":"During development, it's very likely that you will want to change the schema of the data being stored on the client. Unfortunately, there isn't a really good way of dealing with this situation. Here is my solution: Create a constant within your mobile client containing a schema version number. The actual value (text or int) doesn't matter. You should never have the same schema version number twice though. Change this constant whenever you change the schema. Read a file from the mobile device with the schema version in it. If the file exists and the read value is different from the constant you created in step 1, then you are \"refreshing the cache\" If you are refreshing the cache, delete the offline cache file, then write the current schema version number to the file that you read in step 2. Set up your offline cache, define your tables, etc. If you are \"refreshing the cache\", do a full PullAsync() on all tables to populate the cache. You obviously want to avoid this process as much as possible. One possible solution is to use the VersionTrackingPlugin by Colby Williams to detect when the schema has been updated. The following code can be added to automatically delete the offline cache when the version of the app changes: // Add an isRemoteDatabaseSchemaChanged boolean somewhere if (CrossVersionTracking.Current.IsFirstLaunchForBuild) { isRemoteDatabaseSchemaChanged = true; // Drop SQLite Store file to overcome remote-db schema changes File.Delete(Constants.SqliteStorePath); }","title":"Schema changes in the Offline Cache"},{"location":"chapter3/client/#debugging-the-offline-cache","text":"One of the most difficult parts of the offline cache is that it is opaque - you can't really see what is going on. Fortunately, the SQLiteStore that is used is relatively straight forward to sub-class so we can add logging to it. The following helper method can be substituted for a SQLiteStore in any code. public class MobileServiceSQLiteStoreWithLogging : MobileServiceSQLiteStore { private bool logResults; private bool logParameters; public MobileServiceSQLiteStoreWithLogging(string fileName, bool logResults = false, bool logParameters = false) : base(fileName) { this.logResults = logResults; this.logParameters = logParameters; } protected override IList<Newtonsoft.Json.Linq.JObject> ExecuteQuery(string tableName, string sql, IDictionary<string, object> parameters) { Console.WriteLine (sql); if(logParameters) PrintDictionary (parameters); var result = base.ExecuteQuery(tableName, sql, parameters); if (logResults && result != null) { foreach (var token in result) Console.WriteLine (token); } return result; } protected override void ExecuteNonQuery(string sql, IDictionary<string, object> parameters) { Console.WriteLine (sql); if(logParameters) PrintDictionary (parameters); base.ExecuteNonQuery(sql, parameters); } private void PrintDictionary(IDictionary<string,object> dictionary) { if (dictionary == null) return; foreach (var pair in dictionary) Console.WriteLine (\"{0}:{1}\", pair.Key, pair.Value); } } public class LoggingHandler : DelegatingHandler { private bool logRequestResponseBody; public LoggingHandler(bool logRequestResponseBody = false) { this.logRequestResponseBody = logRequestResponseBody; } protected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, System.Threading.CancellationToken cancellationToken) { Console.WriteLine(\"Request: {0} {1}\", request.Method, request.RequestUri.ToString()); if (logRequestResponseBody && request.Content != null) { var requestContent = await request.Content.ReadAsStringAsync (); Console.WriteLine (requestContent); } var response = await base.SendAsync(request, cancellationToken); Console.WriteLine (\"Response: {0}\", response.StatusCode); if (logRequestResponseBody) { var responseContent = await response.Content.ReadAsStringAsync (); Console.WriteLine (responseContent); } return response; } } Using this class will print all the SQL commands that are executed against the SQLite store. Ensure you are capturing the console somewhere so that you can see the debug messages as you are running your application.","title":"Debugging the Offline Cache"},{"location":"chapter3/dataconcepts/","text":"Data Access Concepts \u00b6 At some point in the development of your application, you are going to want to store or retrieve data. This could be as simple as a key-value store for storing personal settings, or as complex as a multi-table customer relationship database. The key ingredient to all these scenarios is structured data. I say structured here deliberately. It is an important concept. Most applications you write will require data in some sort of form that your application understands. It's very tempting to use an unstructured data source (like NoSQL). I'm a big fan of NoSQL stores since they free me up to concentrate on the mobile client without worrying about the data format. After all, you can store whatever entities you want in a NoSQL data source. However, this is really just an example of sloppy programming. Most developers that use NoSQL are storing structured data in that NoSQL store to get around the problem that you actually have to decide what data you want to store in the database. For this reason, I prefer a SQL database. It ensures that I am thinking about the data format up front. It also helps to ensure that a bad actor is not going to store data I don't expect in my store. This isn't to say that NoSQL doesn't have its place. There are times when you need to store structured data where the data format varies between each entity that you are storing. This tends to be an outlier situation though. Tables, Entities and Properties \u00b6 When we talk about data, we tend to talk in terms of Tables , Entities and Properties . These have equivalents in the SQL world (tables, rows and fields). A Table is a collection of Entities that share a common format. That format is described in terms of Properties . Properties are basic types (like strings, numbers, booleans and dates). We create a Table Controller to expose the tables to a mobile client. A Table Controller is a REST endpoint that implements an OData v3 interface. OData is a standard interface to table data that allows the client to perform CRUD (create, read, update and delete) operations on the data. In addition, it provides for a standard way for querying the data. More normally, the Table Controller is accessed through the Azure Mobile Apps Client SDK. We saw an example of this in Chapter 1 when we first introduced the mobile client. Azure Mobile Apps also deals with tables in a manner that enables offline synchronization of the data. That means it must be opinionated about the data format. Specifically, There are four system properties on each entity. There are limitations on relationships between tables. Complex types need special handling. Let's take each of these in turn. We implemented this Model within the mobile client in Chapter 1: using TaskList.Abstractions; namespace TaskList.Models { public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } } } It's a fairly basic model class. Note the TableData base class. I often say that Azure Mobile Apps implements an opinionated version of OData. What I mean by that is that the protocol expects certain system fields to be present during the transfer. The TableData class is a base class that implements that specification. It looks like this: using System; namespace TaskList.Abstractions { public abstract class TableData { public string Id { get; set; } public DateTimeOffset? UpdatedAt { get; set; } public byte[] Version { get; set; } } } The server side version adds another field - the Deleted boolean. This is described in the ITableData interface that is provided with the Azure Mobile Apps Server SDK. Info The Azure Mobile Apps SDK uses DateTimeOffset instead of DateTime . A DateTime object is time zone aware, and time zone definitions change over time. The DateTimeOffset does not know anything about time zones. The DateTime representation can change depending on where you are. The DateTimeOffset will never change. This makes it a better choice for these things. You will see dates stored in UTC in your database as a result of this. Each element of the TableData (and ITableData) has a purpose, nominally to deal with situations with Offline Sync. The Id field \u00b6 One of the common questions is this: Can I use an auto-incrementing integer as an Id field? Let's take a look at a simple situation. You have two clients writing to the same table. It might look something like this: Device A inserts a new record in to the database, with ID 1. Device B is also inserting and decides to do ID 1 as well. This causes an immediate conflict that must be resolved. We could fix this by requiring that new inserts do not insert an ID. However, this can cause problems in offline cases, where you may be inserting many records and have to refer back to them during your offline state. The compromise here is to use a globally unique ID. The GUID is a well-known algorithm and easily generated in offline scenarios. This is stored as a string during transfer. Tip You can specify an Id field when creating an Entity. However, you must ensure that it is globally unique. The UpdatedAt field \u00b6 One of the concepts that is always top of mind is ensuring that we are a good mobile citizen. This means that we care about bandwidth utilization. Reducing transfer size is good for your users. They use less of their data allowance and save time by transferring less. One of the key components to reduce bandwidth utilization is Incremental Sync . With each record, we record the date it was last updated. This is generally done for us as a database trigger, so we never have to worry about setting this value. When we synchronize our table, only the records that have been updated since the last synchronization are requested. The UpdatedAt field contains the date and time that the record was updated within the central (server-side) database. Do not maintain this field yourself. If you need a \"last updated timestamp\" for the client-side insert, update or delete methods, you must generate it yourself. The Version field \u00b6 The Version field is all about conflict detection. Let's take two devices requesting the same table again: In this diagram, Device A submits the first version of the entity. Device B then updates the entity and posts it back. This is accepted because Device B is sending the same version, so the server knows that this is an update to the latest version. The server will send back a response with the updated version so that Device B knows that it has the latest version. Later on, Device A sends an update to the same entity. It, however, still has version 1 of the entity. The server will reject that because of a version mismatch. The Version field is maintained by the central (sever-side) SDK. Do not maintain this field yourself. The Deleted field \u00b6 When you are operating a service with an offline scope, you can't just delete entities. If an entity is deleted on Device A, it is then removed from the server. The server does not know to send an update to that entity to Device B because it no longer exists. For this reason, we never delete entities. We use Soft Delete . Soft Delete is a feature whereby entities are marked as deleted by setting the Deleted flag to true. When you query the server, the deleted records are not shown unless you explicitly ask for them. This is done as part of the offline sync process. Info The UpdatedAt and Version fields are maintained by the Azure Mobile Apps Domain Manager, generally by triggers within the database. You must not set these fields yourself. The Data Access Protocol \u00b6 Given any particular table, there are a few endpoints that are important. Given our TodoItem table from Chapter 1: Operation Endpoint Description GET /tables/todoitem Query the table GET /tables/todoitem/ id Retrieve a single entity POST /tables/todoitem Add a new entity PATCH /tables/todoitem/ id Update an existing entity DELETE /tables/todoitem/ id Deletes an existing entity POST /tables/todoitem/ id Undelete a previously deleted entity We can take a look at each of these in turn with the Azure App Service. These can be done with Postman easily. Info The first request to a new Azure App Service will take some time, especially if the site has to set up the database. Let's start with a basic Query operation: We always get an array of elements back. These contain five system properties. We didn't mention createdAt earlier - it's optional and will be maintained for you if you don't use it. In addition, we have the fields that were in our model. If there are no elements in a table, we get an empty array. If the table does not exist, we will get a 404 Not Found error. Info Any operation can also return a 401 Unauthorized if you are not allowed to do the operation with the current authentication, 400 Bad Request if you supplied bad data and 500 Internal Server Error if the server crashed. We can also do a GET for an Id: The return is the entity serialized as an object. If the Id does not exist, then a 404 Not Found is returned. Adding an item requires a POST: Note that you do not need to provide all the fields. In particular, the system fields will be automatically filled in for you. Any fields with a default value will be similarly auto-created. It is fairly easy to generate a 400 Bad Request when updating or inserting data. For example, if you submit a string when a number is expected or submit a malformed date, you can expect a 400 Bad Request . On success, the response has a Location field in the headers: This is the URI of the entity. You can do a GET on this location to get the entity again. Updating an entity involves sending the updated fields to the Id endpoint with the changed properties: Note that you do not need to send the entire entity - just the properties that are changed. The new entity is returned on success. As with the insert operation, data format errors will result in a 400 Bad Request . Note, however, that if you do not submit a version field, no conflict handling is done and the server just accepts the record. We can fix this with server code later on by requiring a version field on updates. Deletion is fairly straight forward. The main different is that it returns a 204 No Content . The table controller does not support soft delete out of the box. If you have followed the sequence, the record we just deleted is gone. You can verify this using a SQL Browser. To enable soft delete, you need to adjust the domain manager in the TodoItemController: protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request, enableSoftDelete: true); } Adding the enableSoftDelete parameter and setting it to true will enable the appropriate logic in the domain manager. Info We haven't introduced the Domain Manager yet. Azure Mobile Apps doesn't really care what sort of data store you are using on the backend. It proxies all requests through a class that implements the IDomainManager interface. Azure Mobile Apps Server SDK supplies one such domain manager - the EntityDomainManager uses Entity Framework underneath for this purpose. Go through the same process of adding and deleting an entity. You can see the entity by using the SQL Server Object Explorer in Visual Studio: Go to Server Explorer . Expand Azure and SQL Databases . Select your database, then right-click and select Open in SQL Server Object Explorer . You will be prompted for your username and password. Enter them, then click OK . You may be prompted to update the firewall for SQL access. Select My Client IP , then OK . Expand your database node, then Tables . Right-click on dbo.TodoItems and select View Data . I find the SQL Server Object Explorer to be relatively slow when it comes to database options. However, it doesn't require any additional installs. You can also use SQL Server Management Studio if you have it installed. You will need to update the firewall for access (something the SQL Server Object Explorer will do for you). Note the third record has the Deleted column set to true. We will not see that record when we do a query. We can see the deleted records only if we use the parameter __includeDeleted=true : We can now undelete that record by POSTing to the Id endpoint: A success results in a 201 Created response, with a failure resulting in a 404 Not Found response (assuming the failure is because the Id does not exist in the table). Filtering Data \u00b6 If you have followed along, we have three entities in our table now. We can do searches by utilizing the OData $filter operator as a query: The $filter parameter takes an OData filter and returns the list of entities that match the search. The Azure Mobile Apps SDK supports quite a bit of the OData v3 specification that is supported by the Microsoft.Data.OData package, but not everything. There are features of the OData package that are explicitly disabled because they do not work in an offline context. OData was defined as a method of transferring data between client and server in an online context so we can expect some things to work differently. We can also select specific fields by using the $select clause: Paging Results \u00b6 At some point, we are going to bump into an in-built limit of the server. You can clearly see this by inserting a lot of entities then querying the results. Once the number of entities gets above 50, paging will occur. You can adjust the paging size on the server by adding an [EnableQuery()] attribute to the class. For example, the following will set the page size at 10: namespace Chapter3.Controllers { [EnableQuery(PageSize=10)] public class TodoItemController : TableController<TodoItem> { You cannot make the page size infinite, so you should always implement paging controls in your mobile client. We can always receive the number of records that would have been sent if paging had not been in place by including $inlinecount=allpages with the query. The query response turns into an object with two properties - the results property contains the array of results. This is the same response as we received before. There is now another property called count that contains the count of the records: We can implement paging by using $top and $skip parameters. The $top parameter tells the server how many entities you want to return. The $skip parameter tells the server how many entities to skip before it starts counting. For example, let's say you wanted to receive individual entities. You could request: /tables/todoitem?$top=1&$skip=0 /tables/todoitem?$top=1&$skip=1 /tables/todoitem?$top=1&$skip=2 /tables/todoitem?$top=1&$skip=3 At this point, no entities would be returned and you would know you are at the end. Warn Although it is tempting to suggest removing the limit on the number of entities that can be returned (so you can receive all entities in one shot), it's better to implement paging. The Azure App Service will run in a smaller App Service Plan because it won't require as much memory. You will be able to support more users and your code will be more resilient to network issues that occur during transmission. Offline synchronization \u00b6 One of the many reasons that developers choose the Azure Mobile Apps SDK is that it natively supports offline sync. Offline sync provides a number of benefits. It improves app responsiveness by caching server data locally on the device. It allows the app to survive network issues including little or no connectivity, and it allows the developer to decide when to synchronize, thus allowing the deferral of large updates to when there is wifi available, for example. The Azure Mobile Apps SDKs provide incremental sync (thereby ensuring the minimal amount of mobile data is used), optimistic concurrency and conflict resolution. To do this, Azure Mobile Apps provides a SQLite based backing store for data persistence on the mobile client. You don't have to use SQLite, but it's built in and there are very few reasons to not use it. If you are using iOS, the implementation is based on Core Data (which is itself based on SQLite). When you perform changes to an offline table, a Sync Context is created along side the offline table. One of the elements of this sync context is an Operation Queue . This is an ordered list of Create, Update and Delete operations against the offline table. When you PUSH the Sync Context, the list of creates, updates and Deletes are sent one by one to the Azure App Service, which then executes them as if they were done online. Nothing is sent to the Azure App Service until your call to PUSH . To retrieve entities, your mobile client will perform a PULL against a query. The query is based on the filter that we reviewed earlier. By default, all properties of all entities are pulled down. An Implicit Push happens if there are entities in the operation queue at the time of a pull request. If you specify a query name (which is just a text string) to the PullAsync() method, the mobile client will do an Incremental Sync . In this case, the latest UpdatedAt timestamp that the mobile client saw is recorded in the Sync Context (and associated with the query name). This allows the pull operation to pick up where it left off. Tip The query name must be unique within a Sync Context for incremental sync to work. The sync process implements Optimistic Concurrency . With optimistic concurrency, the mobile client assumes that its change is valid. Conflicts are handled only on push operations. If the mobile client submits a record with a version field that does not match the server version field, the server will return a 409 or 412 response code. Info What's the difference between 409 and 412? Most of the time, you will see 412 Precondition Failed. This means the ETag of the request did not match. The ETag is a header that is equivalent to the version value. 409 Conflict occurs when you don't submit an ETag but do submit a version field in the update. If no version field (or ETag header) is submitted, the client entity is used for the create or update irrespective of the value on the server.","title":"Concepts"},{"location":"chapter3/dataconcepts/#data-access-concepts","text":"At some point in the development of your application, you are going to want to store or retrieve data. This could be as simple as a key-value store for storing personal settings, or as complex as a multi-table customer relationship database. The key ingredient to all these scenarios is structured data. I say structured here deliberately. It is an important concept. Most applications you write will require data in some sort of form that your application understands. It's very tempting to use an unstructured data source (like NoSQL). I'm a big fan of NoSQL stores since they free me up to concentrate on the mobile client without worrying about the data format. After all, you can store whatever entities you want in a NoSQL data source. However, this is really just an example of sloppy programming. Most developers that use NoSQL are storing structured data in that NoSQL store to get around the problem that you actually have to decide what data you want to store in the database. For this reason, I prefer a SQL database. It ensures that I am thinking about the data format up front. It also helps to ensure that a bad actor is not going to store data I don't expect in my store. This isn't to say that NoSQL doesn't have its place. There are times when you need to store structured data where the data format varies between each entity that you are storing. This tends to be an outlier situation though.","title":"Data Access Concepts"},{"location":"chapter3/dataconcepts/#tables-entities-and-properties","text":"When we talk about data, we tend to talk in terms of Tables , Entities and Properties . These have equivalents in the SQL world (tables, rows and fields). A Table is a collection of Entities that share a common format. That format is described in terms of Properties . Properties are basic types (like strings, numbers, booleans and dates). We create a Table Controller to expose the tables to a mobile client. A Table Controller is a REST endpoint that implements an OData v3 interface. OData is a standard interface to table data that allows the client to perform CRUD (create, read, update and delete) operations on the data. In addition, it provides for a standard way for querying the data. More normally, the Table Controller is accessed through the Azure Mobile Apps Client SDK. We saw an example of this in Chapter 1 when we first introduced the mobile client. Azure Mobile Apps also deals with tables in a manner that enables offline synchronization of the data. That means it must be opinionated about the data format. Specifically, There are four system properties on each entity. There are limitations on relationships between tables. Complex types need special handling. Let's take each of these in turn. We implemented this Model within the mobile client in Chapter 1: using TaskList.Abstractions; namespace TaskList.Models { public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } } } It's a fairly basic model class. Note the TableData base class. I often say that Azure Mobile Apps implements an opinionated version of OData. What I mean by that is that the protocol expects certain system fields to be present during the transfer. The TableData class is a base class that implements that specification. It looks like this: using System; namespace TaskList.Abstractions { public abstract class TableData { public string Id { get; set; } public DateTimeOffset? UpdatedAt { get; set; } public byte[] Version { get; set; } } } The server side version adds another field - the Deleted boolean. This is described in the ITableData interface that is provided with the Azure Mobile Apps Server SDK. Info The Azure Mobile Apps SDK uses DateTimeOffset instead of DateTime . A DateTime object is time zone aware, and time zone definitions change over time. The DateTimeOffset does not know anything about time zones. The DateTime representation can change depending on where you are. The DateTimeOffset will never change. This makes it a better choice for these things. You will see dates stored in UTC in your database as a result of this. Each element of the TableData (and ITableData) has a purpose, nominally to deal with situations with Offline Sync.","title":"Tables, Entities and Properties"},{"location":"chapter3/dataconcepts/#the-id-field","text":"One of the common questions is this: Can I use an auto-incrementing integer as an Id field? Let's take a look at a simple situation. You have two clients writing to the same table. It might look something like this: Device A inserts a new record in to the database, with ID 1. Device B is also inserting and decides to do ID 1 as well. This causes an immediate conflict that must be resolved. We could fix this by requiring that new inserts do not insert an ID. However, this can cause problems in offline cases, where you may be inserting many records and have to refer back to them during your offline state. The compromise here is to use a globally unique ID. The GUID is a well-known algorithm and easily generated in offline scenarios. This is stored as a string during transfer. Tip You can specify an Id field when creating an Entity. However, you must ensure that it is globally unique.","title":"The Id field"},{"location":"chapter3/dataconcepts/#the-updatedat-field","text":"One of the concepts that is always top of mind is ensuring that we are a good mobile citizen. This means that we care about bandwidth utilization. Reducing transfer size is good for your users. They use less of their data allowance and save time by transferring less. One of the key components to reduce bandwidth utilization is Incremental Sync . With each record, we record the date it was last updated. This is generally done for us as a database trigger, so we never have to worry about setting this value. When we synchronize our table, only the records that have been updated since the last synchronization are requested. The UpdatedAt field contains the date and time that the record was updated within the central (server-side) database. Do not maintain this field yourself. If you need a \"last updated timestamp\" for the client-side insert, update or delete methods, you must generate it yourself.","title":"The UpdatedAt field"},{"location":"chapter3/dataconcepts/#the-version-field","text":"The Version field is all about conflict detection. Let's take two devices requesting the same table again: In this diagram, Device A submits the first version of the entity. Device B then updates the entity and posts it back. This is accepted because Device B is sending the same version, so the server knows that this is an update to the latest version. The server will send back a response with the updated version so that Device B knows that it has the latest version. Later on, Device A sends an update to the same entity. It, however, still has version 1 of the entity. The server will reject that because of a version mismatch. The Version field is maintained by the central (sever-side) SDK. Do not maintain this field yourself.","title":"The Version field"},{"location":"chapter3/dataconcepts/#the-deleted-field","text":"When you are operating a service with an offline scope, you can't just delete entities. If an entity is deleted on Device A, it is then removed from the server. The server does not know to send an update to that entity to Device B because it no longer exists. For this reason, we never delete entities. We use Soft Delete . Soft Delete is a feature whereby entities are marked as deleted by setting the Deleted flag to true. When you query the server, the deleted records are not shown unless you explicitly ask for them. This is done as part of the offline sync process. Info The UpdatedAt and Version fields are maintained by the Azure Mobile Apps Domain Manager, generally by triggers within the database. You must not set these fields yourself.","title":"The Deleted field"},{"location":"chapter3/dataconcepts/#the-data-access-protocol","text":"Given any particular table, there are a few endpoints that are important. Given our TodoItem table from Chapter 1: Operation Endpoint Description GET /tables/todoitem Query the table GET /tables/todoitem/ id Retrieve a single entity POST /tables/todoitem Add a new entity PATCH /tables/todoitem/ id Update an existing entity DELETE /tables/todoitem/ id Deletes an existing entity POST /tables/todoitem/ id Undelete a previously deleted entity We can take a look at each of these in turn with the Azure App Service. These can be done with Postman easily. Info The first request to a new Azure App Service will take some time, especially if the site has to set up the database. Let's start with a basic Query operation: We always get an array of elements back. These contain five system properties. We didn't mention createdAt earlier - it's optional and will be maintained for you if you don't use it. In addition, we have the fields that were in our model. If there are no elements in a table, we get an empty array. If the table does not exist, we will get a 404 Not Found error. Info Any operation can also return a 401 Unauthorized if you are not allowed to do the operation with the current authentication, 400 Bad Request if you supplied bad data and 500 Internal Server Error if the server crashed. We can also do a GET for an Id: The return is the entity serialized as an object. If the Id does not exist, then a 404 Not Found is returned. Adding an item requires a POST: Note that you do not need to provide all the fields. In particular, the system fields will be automatically filled in for you. Any fields with a default value will be similarly auto-created. It is fairly easy to generate a 400 Bad Request when updating or inserting data. For example, if you submit a string when a number is expected or submit a malformed date, you can expect a 400 Bad Request . On success, the response has a Location field in the headers: This is the URI of the entity. You can do a GET on this location to get the entity again. Updating an entity involves sending the updated fields to the Id endpoint with the changed properties: Note that you do not need to send the entire entity - just the properties that are changed. The new entity is returned on success. As with the insert operation, data format errors will result in a 400 Bad Request . Note, however, that if you do not submit a version field, no conflict handling is done and the server just accepts the record. We can fix this with server code later on by requiring a version field on updates. Deletion is fairly straight forward. The main different is that it returns a 204 No Content . The table controller does not support soft delete out of the box. If you have followed the sequence, the record we just deleted is gone. You can verify this using a SQL Browser. To enable soft delete, you need to adjust the domain manager in the TodoItemController: protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request, enableSoftDelete: true); } Adding the enableSoftDelete parameter and setting it to true will enable the appropriate logic in the domain manager. Info We haven't introduced the Domain Manager yet. Azure Mobile Apps doesn't really care what sort of data store you are using on the backend. It proxies all requests through a class that implements the IDomainManager interface. Azure Mobile Apps Server SDK supplies one such domain manager - the EntityDomainManager uses Entity Framework underneath for this purpose. Go through the same process of adding and deleting an entity. You can see the entity by using the SQL Server Object Explorer in Visual Studio: Go to Server Explorer . Expand Azure and SQL Databases . Select your database, then right-click and select Open in SQL Server Object Explorer . You will be prompted for your username and password. Enter them, then click OK . You may be prompted to update the firewall for SQL access. Select My Client IP , then OK . Expand your database node, then Tables . Right-click on dbo.TodoItems and select View Data . I find the SQL Server Object Explorer to be relatively slow when it comes to database options. However, it doesn't require any additional installs. You can also use SQL Server Management Studio if you have it installed. You will need to update the firewall for access (something the SQL Server Object Explorer will do for you). Note the third record has the Deleted column set to true. We will not see that record when we do a query. We can see the deleted records only if we use the parameter __includeDeleted=true : We can now undelete that record by POSTing to the Id endpoint: A success results in a 201 Created response, with a failure resulting in a 404 Not Found response (assuming the failure is because the Id does not exist in the table).","title":"The Data Access Protocol"},{"location":"chapter3/dataconcepts/#filtering-data","text":"If you have followed along, we have three entities in our table now. We can do searches by utilizing the OData $filter operator as a query: The $filter parameter takes an OData filter and returns the list of entities that match the search. The Azure Mobile Apps SDK supports quite a bit of the OData v3 specification that is supported by the Microsoft.Data.OData package, but not everything. There are features of the OData package that are explicitly disabled because they do not work in an offline context. OData was defined as a method of transferring data between client and server in an online context so we can expect some things to work differently. We can also select specific fields by using the $select clause:","title":"Filtering Data"},{"location":"chapter3/dataconcepts/#paging-results","text":"At some point, we are going to bump into an in-built limit of the server. You can clearly see this by inserting a lot of entities then querying the results. Once the number of entities gets above 50, paging will occur. You can adjust the paging size on the server by adding an [EnableQuery()] attribute to the class. For example, the following will set the page size at 10: namespace Chapter3.Controllers { [EnableQuery(PageSize=10)] public class TodoItemController : TableController<TodoItem> { You cannot make the page size infinite, so you should always implement paging controls in your mobile client. We can always receive the number of records that would have been sent if paging had not been in place by including $inlinecount=allpages with the query. The query response turns into an object with two properties - the results property contains the array of results. This is the same response as we received before. There is now another property called count that contains the count of the records: We can implement paging by using $top and $skip parameters. The $top parameter tells the server how many entities you want to return. The $skip parameter tells the server how many entities to skip before it starts counting. For example, let's say you wanted to receive individual entities. You could request: /tables/todoitem?$top=1&$skip=0 /tables/todoitem?$top=1&$skip=1 /tables/todoitem?$top=1&$skip=2 /tables/todoitem?$top=1&$skip=3 At this point, no entities would be returned and you would know you are at the end. Warn Although it is tempting to suggest removing the limit on the number of entities that can be returned (so you can receive all entities in one shot), it's better to implement paging. The Azure App Service will run in a smaller App Service Plan because it won't require as much memory. You will be able to support more users and your code will be more resilient to network issues that occur during transmission.","title":"Paging Results"},{"location":"chapter3/dataconcepts/#offline-synchronization","text":"One of the many reasons that developers choose the Azure Mobile Apps SDK is that it natively supports offline sync. Offline sync provides a number of benefits. It improves app responsiveness by caching server data locally on the device. It allows the app to survive network issues including little or no connectivity, and it allows the developer to decide when to synchronize, thus allowing the deferral of large updates to when there is wifi available, for example. The Azure Mobile Apps SDKs provide incremental sync (thereby ensuring the minimal amount of mobile data is used), optimistic concurrency and conflict resolution. To do this, Azure Mobile Apps provides a SQLite based backing store for data persistence on the mobile client. You don't have to use SQLite, but it's built in and there are very few reasons to not use it. If you are using iOS, the implementation is based on Core Data (which is itself based on SQLite). When you perform changes to an offline table, a Sync Context is created along side the offline table. One of the elements of this sync context is an Operation Queue . This is an ordered list of Create, Update and Delete operations against the offline table. When you PUSH the Sync Context, the list of creates, updates and Deletes are sent one by one to the Azure App Service, which then executes them as if they were done online. Nothing is sent to the Azure App Service until your call to PUSH . To retrieve entities, your mobile client will perform a PULL against a query. The query is based on the filter that we reviewed earlier. By default, all properties of all entities are pulled down. An Implicit Push happens if there are entities in the operation queue at the time of a pull request. If you specify a query name (which is just a text string) to the PullAsync() method, the mobile client will do an Incremental Sync . In this case, the latest UpdatedAt timestamp that the mobile client saw is recorded in the Sync Context (and associated with the query name). This allows the pull operation to pick up where it left off. Tip The query name must be unique within a Sync Context for incremental sync to work. The sync process implements Optimistic Concurrency . With optimistic concurrency, the mobile client assumes that its change is valid. Conflicts are handled only on push operations. If the mobile client submits a record with a version field that does not match the server version field, the server will return a 409 or 412 response code. Info What's the difference between 409 and 412? Most of the time, you will see 412 Precondition Failed. This means the ETag of the request did not match. The ETag is a header that is equivalent to the version value. 409 Conflict occurs when you don't submit an ETag but do submit a version field in the update. If no version field (or ETag header) is submitted, the client entity is used for the create or update irrespective of the value on the server.","title":"Offline synchronization"},{"location":"chapter3/domainmgr/","text":"The Domain Manager \u00b6 As a request comes in to the mobile backend, it is processed through several layers. First, ASP.NET processes the request, handling things like Authentication and Authorization. It is then processed through the Microsoft.Web.Http.OData controller, which compiles the requested query. Then it is passed to the Domain Manager, which is responsible for converting the request into a response. The response is then passed back up the stack to be finally given back to the mobile client. The Domain Manager is a central part of this process. It is a class that implements the IDomainManager interface: namespace Microsoft.Azure.Mobile.Server.Tables { public interface IDomainManager<TData> where TData : class, ITableData { IQueryable<TData> Query(); SingleResult<TData> Lookup(string id); Task<IEnumerable<TData>> QueryAsync(ODataQueryOptions query); Task<SingleResult<TData>> LookupAsync(string id); Task<TData> InsertAsync(TData data); Task<TData> UpdateAsync(string id, Delta<TData> patch); Task<TData> ReplaceAsync(string id, TData data); Task<bool> DeleteAsync(string id); } } This looks deceptively simple. Just 8 methods. In reality, this is anything but simple. The major issue that a prospective domain manager implementor has to grapple with is the translation of an IQueryable into something that the backend data source can understand. Let's take a look at a couple of domain managers that solve specific problems that crop up from time to time during development. It should be noted that NEITHER of these domain managers are recommended as a generalized solution. They both have significant caveats to their use and you should understand those caveats before embarking on integrating them. Existing Table Relationships with the MappedEntityDomainManager \u00b6 One of the key areas that is weak when using the default EntityDomainManager is handling existing tables. The generally accepted method of dealing with relationships is through loose coupling and manual relationship management in the client. Relationships are core to the SQL database world and we sometimes want to project those relationships into the mobile client, allowing the backend to preserve any relationships that have been configured while still using the standard offline client capabilities. If you have existing SQL relationships, you can use a combination of AutoMapper and the MappedEntityDomainManager . The MappedEntityDomainManager is an abstract IDomainManager implementation targetting SQL as the backend store where there is not a 1:1 mapping between the data object (DTO) exposed through the TableController and the domain model managed by Entity Framework. If there is a 1:1 mapping, use EntityDomainManager . The MappedEntityDomainManager uses AutoMapper to map between the DTO and the domain model. It assumes that AutoMapper has already been initialized with appropriate mappings that map from DTO to domain model and from the domain model to the DTO. Let's take a small example. If I am producing an enterprise mobile app that field engineers can use - the ones that, for example, visit your house to install cable. I can define an Entity Framework model map as follows: [Table(\"Customers\")] public class Customer { public Customer() { this.Jobs = new HashSet<Job>(); } [StringLength(50)] public string Id { get; set; } [StringLength(50)] public string FullName { get; set; } [StringLength(250)] public string Address { get; set; public decimal? Latitude { get; set; } public decimal? Longitude { get; set; } public ICollection<Job> Jobs { get; set; } } [Table(\"Equipment\")] public class Equipment : ITableData { public Equipment() { this.Jobs = new HashSet<Job>(); } [StringLength(50)] public string Name { get; set; } [StringLength(28)] public string Asset { get; set; } [StringLength(250)] public string Description { get; set; } #region ITableData public DateTimeOffset? CreatedAt { get; set; } public bool Deleted { get; set; } [DatabaseGenerated(DatabaseGeneratedOption.Computed)] public DateTimeOffset? UpdatedAt { get; set; } [Timestamp] public byte[] Version { get; set; } #endregion public ICollection<Job> Jobs { get; set; } } [Table(\"Jobs\")] public class Job : ITableData { public Job() { this.Equipments = new HashSet<Equipment>(); } [StringLength(50)] public string CustomerId { get; set; } [StringLength(50)] public string AgentId { get; set; } public DateTimeOffset? StartTime { get; set; } public DateTimeOffset? EndTime { get; set; } [StringLength(50)] public string Status { get; set; } [StringLength(250)] public string Description { get; set; } #region ITableData public DateTimeOffset? CreatedAt { get; set; } public bool Deleted { get; set; } [DatabaseGenerated(DatabaseGeneratedOption.Computed)] public DateTimeOffset? UpdatedAt { get; set; } [Timestamp] public byte[] Version { get; set; } #endregion public ICollection<Equipment> Equipments { get; set; } } This is the representation of the tables within the database. They don't have to map to what the client requires. We can wire up the relationships in the normal Entity Framework way , within the DbContext : public partial class ExistingDbContext : DbContext { public FieldDbContext() : base(\"name=MS_TableConnectionString\") { } public virtual DbSet<Customer> Customers { get; set; } public virtual DbSet<Equipment> Equipments { get; set; } public virtual DbSet<Job> Jobs { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.HasDefaultSchema(\"dbo\"); modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); modelBuilder.Entity<Customer>().Property(e => e.Address).IsUnicode(false); modelBuilder.Entity<Customer>().Property(e => e.FullName).IsUnicode(false); modelBuilder.Entity<Customer>().Property(e => e.Id).IsUnicode(false); modelBuilder.Entity<Customer>().Property(e => e.Latitude).HasPrecision(9, 6); modelBuilder.Entity<Customer>().Property(e => e.Longitude).HasPrecision(9, 6); modelBuilder.Entity<Equipment>().Property(e => e.Asset).IsUnicode(false); modelBuilder.Entity<Equipment>().Property(e => e.Description).IsUnicode(false); modelBuilder.Entity<Equipment>().Property(e => e.Name).IsUnicode(false); modelBuilder.Entity<Equipment>().Property(e => e.Id).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.CustomerId).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.AgentId).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.Id).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.Status).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.Description).IsUnicode(false); modelBuilder.Entity<Equipment>() .HasMany(e => e.Jobs) .WithMany(e => e.Equipments) .Map(m => m.ToTable(\"EquipmentIds\").MapLeftKey(\"EquipmentId\").MapRightKey(\"JobId\")); } } We can see the relationship (a Many:Many relationship) at the end of the modelBuilder within the DbContext. The 1:Many and 1:1 relationships are handled within the models themselves, per the normal Entity Framework methods. This is pure Entity Framework thus far - we have defined the structure of the database. To translate this into a mobile client, we need to define Data Transfer Objects. These don't have to be the same shape as the models that Entity Framework is using. For example: public class CustomerDTO { public string FullName { get; set; } public string Address { get; set; public decimal? Latitude { get; set; } public decimal? Longitude { get; set; } } public class EquipmentDTO { public string Name { get; set; } public string Asset { get; set; } public string Description { get; set; } } public class JobDTO : EntityData { public string AgentId { get; set; } public DateTimeOffset? StartTime { get; set; } public DateTimeOffset? EndTime { get; set; } public string Status { get; set; } public string Description { get; set; } public virtual CustomerDTO Customer { get; set; } public virtual List<EquipmentDTO> Equipments { get; set; } } Note that the DTOs are similar, but definitely not the same. They don't have the same relationships between the records, for example. MappedEntityDomainManager requires that AutoMapper is already configured and initialized, so that's the next step. Set up an AutoMapper configuration in the App_Start directory: using AutoMapper; using FieldEngineer.Service.DataObjects; using FieldEngineer.Service.Models; namespace FieldEngineer.Service { public class AutomapperConfiguration { public static void CreateMapping(IConfiguration cfg) { // Apply some name changes from the entity to the DTO cfg.CreateMap<Job, JobDTO>() .ForMember(jobDTO => jobDTO.Equipments, map => map.MapFrom(job => job.Equipments)); // For incoming requests, ignore the relationships cfg.CreateMap<JobDTO, Job>() .ForMember(job => job.Customer, map => map.Ignore()) .ForMember(job => job.Equipments, map => map.Ignore()); cfg.CreateMap<Customer, CustomerDTO>(); cfg.CreateMap<Equipment, EquipmentDTO>(); } } } You will also need to initialize the AutoMapper - this can be done where you also configure the Azure Mobile Apps: using System; using System.Web.Http; using AutoMapper; using Microsoft.WindowsAzure.Mobile.Service; namespace FieldEngineer.Service { public static class WebApiConfig { public static void Register() { // Use this class to set configuration options for your mobile service ConfigOptions options = new ConfigOptions(); // Use this class to set WebAPI configuration options HttpConfiguration config = ServiceConfig.Initialize(new ConfigBuilder(options)); // To display errors in the browser during development, uncomment the following // line. Comment it out again when you deploy your service for production use. config.IncludeErrorDetailPolicy = IncludeErrorDetailPolicy.Always; // This is the line that initializes AutoMapper Mapper.Initialize(cfg => { AutomapperConfiguration.CreateMapping(cfg); }); } } } Finally, we can create a controller that allows the receipt and update of jobs: namespace FieldEngineer.Service.Controllers { [Authorize] public class JobController : TableController<JobDTO> { private FieldDbContext context; protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); this.context = new FieldDbContext(); this.DomainManager = new DefaultMappedEntityDomainManager<JobDTO,Job>(this.context, Request, Services); } [ExpandProperty(\"Customer\")] [ExpandProperty(\"Equipments\")] public async Task<IQueryable<JobDTO>> GetAllJobs() { var jobs = this.context.Jobs .Include(\"Customer\") .Include(\"Equipments\") .Project().To<JobDTO>(); return jobs; } [ExpandProperty(\"Customer\")] [ExpandProperty(\"Equipments\")] public SingleResult<JobDTO> GetJob(string id) { return this.Lookup(id); } public async Task<JobDTO> PatchJob(string id, Delta<JobDTO> patch) { return await this.UpdateAsync(id, patch); } } } We are using [ExpandProperty] to expand the Customer and Equipment data so that it is transferred with the Job object. The MappedEntityDomainManager is an abstract type, so we have to create a concrete implementation. Fortunately, most of the work is done for us. There are already concrete versions of most of the methods we require (like insert, delete and lookup). The MappedEntityDomainManager needs help to deal with replacements nor optimistic concurrency - features we want. We can use the DefaultMappedEntityDomainManager to handle this for us: namespace FieldEngineerLite.Service.Helpers { public class DefaultMappedEntityDomainManager<TData, TModel> : MappedEntityDomainManager<TData, TModel> where TData : class, ITableData where TModel : class, ITableData { public DefaultMappedEntityDomainManager(DbContext context, HttpRequestMessage request, ApiServices services) : base(context, request, services) { } public override Task<bool> DeleteAsync(string id) { return this.DeleteItemAsync(id); } public override Task<TData> UpdateAsync(string id, Delta<TData> patch) { return this.UpdateEntityAsync(patch, id); } public override SingleResult<TData> Lookup(string id) { return this.LookupEntity(model => model.Id == id); } protected override void SetOriginalVersion(TModel model, byte[] version) { this.Context.Entry(model).OriginalValues[\"Version\"] = version; } } } The primary thing that the DefaultMappedEntityDomainManager does that the original doesn't is in the SetOriginalVersion method. This causes the model to be updated with a new version, allowing for conflict detection in the domain manager. If we move now to the models on the mobile client, we see some fairly standard models: public class Customer { public string Id { get; set; } public string FullName { get; set; } public string Address { get; set; public decimal? Latitude { get; set; } public decimal? Longitude { get; set; } } public class Equipment { public string Id { get; set; } public string Name { get; set; } public string Asset { get; set; } public string Description { get; set; } } public class Job { public const string CompleteStatus = \"Completed\"; public const string InProgressStatus = \"On Site\"; public const string PendingStatus = \"Not Started\"; public string Id { get; set; } public string AgentId { get; set; } public DateTimeOffset? StartTime { get; set; } public DateTimeOffSet? EndTime { get; set; } public string Status { get; set; } public string Description { get; set; } public Customer Customer { get; set; } public List<Equipment> Equipments { get; set; } [Version] public string Version { get; set; } } In this case, we only need to sync the Job table, so we can define it in the InitializeAsync() method on the client: public async Task InitializeAsync() { var store = new MobileServiceSQLiteStore(\"localdata.db\"); store.DefineTable<Job>(); await MobileService.SyncContext.InitializeAsync(store); } You can use GetSyncTable<Job>() to get a reference to the table and deal with it as you normally would. I'd expect in this case that the Customer and Equipment would be handled elsewhere - maybe a separate web application that customer service agents use, for example. So, what are the caveats? The first is that the Job, Customer and Equipment data all comes down as one record. This has a side effect of ensuring that the Customer and Equipment data is read-only. You can only update the information in the Job table. This is also a very time consuming process to set up properly. Automapper is known as a fairly picky piece of software to integrate, so extra time must be allotted to make it work correctly. In the end, I prefer handling tables individually and handling relationship management on the mobile client manually. This causes more code on the mobile client but makes the server much simpler by avoiding most of the complexity of relationships. NoSQL Storage with the StorageDomainManager \u00b6 What if you don't want to use a SQL backend for your service? Relationships between entities are not that important in the mobile client and Azure Table Storage costs significantly less than SQL Azure. There are always trade-offs between various storage providers. A Domain Manager enables you to swap out the storage for one of your own choosing. Azure Mobile Apps provides a domain manager for Azure Table Storage. Azure Table Storage is Microsoft's NOSQL key/attribute store. It has a schemaless design, which (at least theoretically) enables you to adapt your data models as the application evolves without having to worry about the schema. To see this in action, let's rework the existing data store (which has tags and todoitems as tables) to use Table Storage. First up, we need to set up a suitable environment. This involves: Create a Resource Group Create an Azure App Service Set up authentication on the Azure App Service Create a Storage Account Link the Storage Account to the Azure App Service. We've already covered the first three items in previous chapters. The important element here is that we do not create a SQL database. We are going to be using Table Storage instead so we don't need it. To create a Storage Account: Log on to the Azure portal . Click the big + NEW button in the top left corner. Click Data + Storage , then Storage account . Fill in the form: The name can only contain letters and numbers and must be unique. A GUID without the dashes is a good choice. The Deployment model should be set to Resource manager . The Account kind should be set to General purpose . The Performance should be set to Standard for this example. The Replication should be set to Locally-redundant storage (LRS) . Set the Resource group to your existing resource group. Set the Location to the same location as your App Service. Click Create . Just like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. We have selected the slowest performance and least redundant options here to keep the cost down on your service. Warn There is no \"free\" option for Azure Storage. You pay by the kilobyte depending on the performance and redundancy selected. Once the Azure Storage account is deployed, you can link the storage account to your App Service: Open your App Service in the Azure portal . Click Data Connections under the MOBILE section in the settings menu. Click + ADD In the Add data connection blade: Set the Type to Storage . Click the Storage link. In the Storage Account selector, click the storage account you just created. Click the Connection string . In the Connection string selector, make a note of the Name field. Click OK . Click OK to close the Add data connection blade. Click on the Application Settings menu option, then scroll down to the Connection Strings section. Note that the portal has created the connection string as an App Setting for you with the right value: DefaultEndpointsProtocol=https;AccountName=thebook;AccountKey=<key1> The key is the access key for the storage. When a storage account is created, two keys are also created. If you re-generate the storage access keys, remember to update your connection string. By default, the connection string is called MS_AzureStorageAccountConnectionString and we will use that throughout. Now that our resources are set up, let's look at the Backend project. This started off as a standard Azure Mobile Apps template. The template assumes you are going to use SQL Azure, so there is quite a bit of work to convert the provided template to use Azure Table Storage. Let's start with the App_Start\\Startup.MobileApp.cs file. There is no Entity Framework, so that needs to be stripped out: using System.Configuration; using System.Web.Http; using Microsoft.Azure.Mobile.Server; using Microsoft.Azure.Mobile.Server.Authentication; using Microsoft.Azure.Mobile.Server.Config; using Owin; namespace Backend { public partial class Startup { public static void ConfigureMobileApp(IAppBuilder app) { HttpConfiguration config = new HttpConfiguration(); new MobileAppConfiguration() .AddTables() .ApplyTo(config); app.UseWebApi(config); } } } We've made three changes: We've removed the database seeding. We've removed the database initializer. We've changed AddTablesWithEntityFramework() to AddTables() . There is extra work needed with Entity Framework. Since we aren't using it, we don't need the additional work. We do, however, need to create the ASP.NET routes to the table controllers. Tip You must add the Microsoft.Azure.Mobile.Server.Storage package from NuGet. Let's move onto the DataObjects. These are very similar: namespace Backend.DataObjects { public class TodoItem : StorageData { public string Text { get; set; } public bool Complete { get; set; } } } Each storage implementation will likely need their own implementation of the ITableData interface. The StorageData class performs the same duties as the EntityData class for Entity Framework based backends. Tip You can remove the Models directory and the DbContext for the project. These are only needed when working with Entity Framework. The Azure Table Storage SDK is completely async driven. Fortunately, the domain manager specification (codified in the definition of IDomainManager ) allows both async and synchronous usage. This does require a change to our controller: using System.Collections.Generic; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using System.Web.Http.OData.Query; using Backend.DataObjects; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { const string connectionString = \"MS_AzureStorageAccountConnectionString\"; const string tableName = \"TodoItem\"; protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); DomainManager = new StorageDomainManager<TodoItem>(connectionString, tableName, Request, enableSoftDelete: true); } // GET tables/TodoItem public async Task<IEnumerable<TodoItem>> GetAllTodoItemsAsync(ODataQueryOptions query) { return await QueryAsync(query); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<SingleResult<TodoItem>> GetTodoItemAsync(string id) { return await LookupAsync(id); } // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<TodoItem> PatchTodoItemAsync(string id, Delta<TodoItem> patch) { return await UpdateAsync(id, patch); } // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItemAsync(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task DeleteTodoItemAsync(string id) { await DeleteAsync(id); } } } Note how we instantiate the storage domain controller. This requires a connection string and the name of the table. We have created the connection string in the portal, but we have not exposed that connection string to our ASP.NET application. We need to edit the Web.config file as well: <connectionStrings> <add name=\"MS_AzureStorageAccountConnectionString\" connectionString=\"UseDevelopmentStorage=true\"/> </connectionStrings> This will be overwritten by the connection string in the portal. If you are running the service locally, you can use this setting with the Azure Storage Emulator. If you don't add this line to the Web.config , you will not be able to run this server locally. This backend can now be published and we can work with Postman to test it out. For instance, let's try adding a simple test of getting some data: This is exactly the same response as we got when we don't have any data from the Entity Framework version. Let's add a record: There are a couple of things to note here. Firstly, the Id must be specified. It also must be of a specific form. There are two numbers. The first is a partition key and the second is a row key. Tables are partitioned to support load balancing across storage notes. It can be anything you wish it to be. Similarly, the row key is unique within a partition. We can use this information to generate a suitable Id if one is not provided: // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItemAsync(TodoItem item) { if (item.Id == null || item.Id == \"'',''\") { item.Id = GenerateUniqueId(); } TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } private string GenerateUniqueId() { var partitionId = \"1\"; var rowId = Guid.NewGuid().ToString(\"N\"); return $\"'{partitionId}','{rowId}'\"; } The value '','' is the default value of the Id column. However, that is not good enough to get a successful store. This code generates a unique identifier in the right format. A single partition is reasonable for most applications. If you intend on storing massive amounts of data, the data partitioning scheme will require some consideration (just like any other NoSQL application). Tip You will need a copy of the GenerateUniqueId() method if you generate unique identifiers for your records within your mobile client. The partition key and row key are returned as part of the record. You can use the Cloud Explorer if you wish to see the data stored in Azure Table Storage. Expand the Storage Accounts node, then expand the appropriate nodes: your storage account, Tables , TodoItem . You can open the table editor or delete the table from there. Within the table editor, you can right-click on any row to delete or edit values. This will update the time stamp and etag, ensuring that your mobile clients are updated as well. Limitations of the Azure Storage Domain Manager \u00b6 There are, of course, caveats to working in Azure Mobile Apps with Azure Table Storage. There are three major caveats that you should be aware of. There are no relationships possible with Azure Table Storage. Only $filter , $top and $select are supported in the URI. Offline sync only supports flat objects. Let's take a look at each in turn. No relationhips You have to work at relationships and relationships between entities are severely restricted in Entity Framework. However, they are possible. Not so with Azure Table Storage. The NoSQL store has no concept of a relationship of any description. This is not a major caveat since your mobile client similarly has no notion of relationships. Just treat every table as distinct. Limited support for OData query options Only $filter , $top and $select are supported by the OData interface . Since the Azure Table Storage Domain Manager passes the incoming OData query to the Storage driver intact, this limitation is passed on to the OData interface for Azure Mobile Apps. Specifically, this means paging is handled differently. With the EntityDomainManager , paging was accomplished by using $skip and $top to get more records until zero records were returned. With the StorageDomainManager , a Link header is returned when there are more records. The Link header contains the URI that you need to retrieve to get the next page of the results. This has implications for how you receive more than 50 records. Offline sync only supports \"flat\" objects One of the common reasons for using NoSQL stores is that you can store pretty much any document you wish. You just have to have a JSON representation of the object cross the wire. If you have complex objects stored in Azure Table Storage, they won't be able to be stored in the offline cache. The offline cache is based on SQLite and inherits the limitations of that resource. In particular, this means no complex types. Using a NoSQL store seems like a great idea. However, the limitations of the platform make Azure Table Storage a poor choice for this particular function. The Azure Table Storage is ill-suited to the demands of a mobile client backend. One of the great uses of the Azure Table Storage Domain Manager is to see how you can write your own domain manager. The code for the domain manager (and the ITableData interface) is relatively simple since it passes through the OData query to Azure Storage. This allows you to see what is truly involved in writing a domain manager.","title":"The Domain Manager"},{"location":"chapter3/domainmgr/#the-domain-manager","text":"As a request comes in to the mobile backend, it is processed through several layers. First, ASP.NET processes the request, handling things like Authentication and Authorization. It is then processed through the Microsoft.Web.Http.OData controller, which compiles the requested query. Then it is passed to the Domain Manager, which is responsible for converting the request into a response. The response is then passed back up the stack to be finally given back to the mobile client. The Domain Manager is a central part of this process. It is a class that implements the IDomainManager interface: namespace Microsoft.Azure.Mobile.Server.Tables { public interface IDomainManager<TData> where TData : class, ITableData { IQueryable<TData> Query(); SingleResult<TData> Lookup(string id); Task<IEnumerable<TData>> QueryAsync(ODataQueryOptions query); Task<SingleResult<TData>> LookupAsync(string id); Task<TData> InsertAsync(TData data); Task<TData> UpdateAsync(string id, Delta<TData> patch); Task<TData> ReplaceAsync(string id, TData data); Task<bool> DeleteAsync(string id); } } This looks deceptively simple. Just 8 methods. In reality, this is anything but simple. The major issue that a prospective domain manager implementor has to grapple with is the translation of an IQueryable into something that the backend data source can understand. Let's take a look at a couple of domain managers that solve specific problems that crop up from time to time during development. It should be noted that NEITHER of these domain managers are recommended as a generalized solution. They both have significant caveats to their use and you should understand those caveats before embarking on integrating them.","title":"The Domain Manager"},{"location":"chapter3/domainmgr/#existing-table-relationships-with-the-mappedentitydomainmanager","text":"One of the key areas that is weak when using the default EntityDomainManager is handling existing tables. The generally accepted method of dealing with relationships is through loose coupling and manual relationship management in the client. Relationships are core to the SQL database world and we sometimes want to project those relationships into the mobile client, allowing the backend to preserve any relationships that have been configured while still using the standard offline client capabilities. If you have existing SQL relationships, you can use a combination of AutoMapper and the MappedEntityDomainManager . The MappedEntityDomainManager is an abstract IDomainManager implementation targetting SQL as the backend store where there is not a 1:1 mapping between the data object (DTO) exposed through the TableController and the domain model managed by Entity Framework. If there is a 1:1 mapping, use EntityDomainManager . The MappedEntityDomainManager uses AutoMapper to map between the DTO and the domain model. It assumes that AutoMapper has already been initialized with appropriate mappings that map from DTO to domain model and from the domain model to the DTO. Let's take a small example. If I am producing an enterprise mobile app that field engineers can use - the ones that, for example, visit your house to install cable. I can define an Entity Framework model map as follows: [Table(\"Customers\")] public class Customer { public Customer() { this.Jobs = new HashSet<Job>(); } [StringLength(50)] public string Id { get; set; } [StringLength(50)] public string FullName { get; set; } [StringLength(250)] public string Address { get; set; public decimal? Latitude { get; set; } public decimal? Longitude { get; set; } public ICollection<Job> Jobs { get; set; } } [Table(\"Equipment\")] public class Equipment : ITableData { public Equipment() { this.Jobs = new HashSet<Job>(); } [StringLength(50)] public string Name { get; set; } [StringLength(28)] public string Asset { get; set; } [StringLength(250)] public string Description { get; set; } #region ITableData public DateTimeOffset? CreatedAt { get; set; } public bool Deleted { get; set; } [DatabaseGenerated(DatabaseGeneratedOption.Computed)] public DateTimeOffset? UpdatedAt { get; set; } [Timestamp] public byte[] Version { get; set; } #endregion public ICollection<Job> Jobs { get; set; } } [Table(\"Jobs\")] public class Job : ITableData { public Job() { this.Equipments = new HashSet<Equipment>(); } [StringLength(50)] public string CustomerId { get; set; } [StringLength(50)] public string AgentId { get; set; } public DateTimeOffset? StartTime { get; set; } public DateTimeOffset? EndTime { get; set; } [StringLength(50)] public string Status { get; set; } [StringLength(250)] public string Description { get; set; } #region ITableData public DateTimeOffset? CreatedAt { get; set; } public bool Deleted { get; set; } [DatabaseGenerated(DatabaseGeneratedOption.Computed)] public DateTimeOffset? UpdatedAt { get; set; } [Timestamp] public byte[] Version { get; set; } #endregion public ICollection<Equipment> Equipments { get; set; } } This is the representation of the tables within the database. They don't have to map to what the client requires. We can wire up the relationships in the normal Entity Framework way , within the DbContext : public partial class ExistingDbContext : DbContext { public FieldDbContext() : base(\"name=MS_TableConnectionString\") { } public virtual DbSet<Customer> Customers { get; set; } public virtual DbSet<Equipment> Equipments { get; set; } public virtual DbSet<Job> Jobs { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.HasDefaultSchema(\"dbo\"); modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); modelBuilder.Entity<Customer>().Property(e => e.Address).IsUnicode(false); modelBuilder.Entity<Customer>().Property(e => e.FullName).IsUnicode(false); modelBuilder.Entity<Customer>().Property(e => e.Id).IsUnicode(false); modelBuilder.Entity<Customer>().Property(e => e.Latitude).HasPrecision(9, 6); modelBuilder.Entity<Customer>().Property(e => e.Longitude).HasPrecision(9, 6); modelBuilder.Entity<Equipment>().Property(e => e.Asset).IsUnicode(false); modelBuilder.Entity<Equipment>().Property(e => e.Description).IsUnicode(false); modelBuilder.Entity<Equipment>().Property(e => e.Name).IsUnicode(false); modelBuilder.Entity<Equipment>().Property(e => e.Id).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.CustomerId).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.AgentId).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.Id).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.Status).IsUnicode(false); modelBuilder.Entity<Job>().Property(e => e.Description).IsUnicode(false); modelBuilder.Entity<Equipment>() .HasMany(e => e.Jobs) .WithMany(e => e.Equipments) .Map(m => m.ToTable(\"EquipmentIds\").MapLeftKey(\"EquipmentId\").MapRightKey(\"JobId\")); } } We can see the relationship (a Many:Many relationship) at the end of the modelBuilder within the DbContext. The 1:Many and 1:1 relationships are handled within the models themselves, per the normal Entity Framework methods. This is pure Entity Framework thus far - we have defined the structure of the database. To translate this into a mobile client, we need to define Data Transfer Objects. These don't have to be the same shape as the models that Entity Framework is using. For example: public class CustomerDTO { public string FullName { get; set; } public string Address { get; set; public decimal? Latitude { get; set; } public decimal? Longitude { get; set; } } public class EquipmentDTO { public string Name { get; set; } public string Asset { get; set; } public string Description { get; set; } } public class JobDTO : EntityData { public string AgentId { get; set; } public DateTimeOffset? StartTime { get; set; } public DateTimeOffset? EndTime { get; set; } public string Status { get; set; } public string Description { get; set; } public virtual CustomerDTO Customer { get; set; } public virtual List<EquipmentDTO> Equipments { get; set; } } Note that the DTOs are similar, but definitely not the same. They don't have the same relationships between the records, for example. MappedEntityDomainManager requires that AutoMapper is already configured and initialized, so that's the next step. Set up an AutoMapper configuration in the App_Start directory: using AutoMapper; using FieldEngineer.Service.DataObjects; using FieldEngineer.Service.Models; namespace FieldEngineer.Service { public class AutomapperConfiguration { public static void CreateMapping(IConfiguration cfg) { // Apply some name changes from the entity to the DTO cfg.CreateMap<Job, JobDTO>() .ForMember(jobDTO => jobDTO.Equipments, map => map.MapFrom(job => job.Equipments)); // For incoming requests, ignore the relationships cfg.CreateMap<JobDTO, Job>() .ForMember(job => job.Customer, map => map.Ignore()) .ForMember(job => job.Equipments, map => map.Ignore()); cfg.CreateMap<Customer, CustomerDTO>(); cfg.CreateMap<Equipment, EquipmentDTO>(); } } } You will also need to initialize the AutoMapper - this can be done where you also configure the Azure Mobile Apps: using System; using System.Web.Http; using AutoMapper; using Microsoft.WindowsAzure.Mobile.Service; namespace FieldEngineer.Service { public static class WebApiConfig { public static void Register() { // Use this class to set configuration options for your mobile service ConfigOptions options = new ConfigOptions(); // Use this class to set WebAPI configuration options HttpConfiguration config = ServiceConfig.Initialize(new ConfigBuilder(options)); // To display errors in the browser during development, uncomment the following // line. Comment it out again when you deploy your service for production use. config.IncludeErrorDetailPolicy = IncludeErrorDetailPolicy.Always; // This is the line that initializes AutoMapper Mapper.Initialize(cfg => { AutomapperConfiguration.CreateMapping(cfg); }); } } } Finally, we can create a controller that allows the receipt and update of jobs: namespace FieldEngineer.Service.Controllers { [Authorize] public class JobController : TableController<JobDTO> { private FieldDbContext context; protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); this.context = new FieldDbContext(); this.DomainManager = new DefaultMappedEntityDomainManager<JobDTO,Job>(this.context, Request, Services); } [ExpandProperty(\"Customer\")] [ExpandProperty(\"Equipments\")] public async Task<IQueryable<JobDTO>> GetAllJobs() { var jobs = this.context.Jobs .Include(\"Customer\") .Include(\"Equipments\") .Project().To<JobDTO>(); return jobs; } [ExpandProperty(\"Customer\")] [ExpandProperty(\"Equipments\")] public SingleResult<JobDTO> GetJob(string id) { return this.Lookup(id); } public async Task<JobDTO> PatchJob(string id, Delta<JobDTO> patch) { return await this.UpdateAsync(id, patch); } } } We are using [ExpandProperty] to expand the Customer and Equipment data so that it is transferred with the Job object. The MappedEntityDomainManager is an abstract type, so we have to create a concrete implementation. Fortunately, most of the work is done for us. There are already concrete versions of most of the methods we require (like insert, delete and lookup). The MappedEntityDomainManager needs help to deal with replacements nor optimistic concurrency - features we want. We can use the DefaultMappedEntityDomainManager to handle this for us: namespace FieldEngineerLite.Service.Helpers { public class DefaultMappedEntityDomainManager<TData, TModel> : MappedEntityDomainManager<TData, TModel> where TData : class, ITableData where TModel : class, ITableData { public DefaultMappedEntityDomainManager(DbContext context, HttpRequestMessage request, ApiServices services) : base(context, request, services) { } public override Task<bool> DeleteAsync(string id) { return this.DeleteItemAsync(id); } public override Task<TData> UpdateAsync(string id, Delta<TData> patch) { return this.UpdateEntityAsync(patch, id); } public override SingleResult<TData> Lookup(string id) { return this.LookupEntity(model => model.Id == id); } protected override void SetOriginalVersion(TModel model, byte[] version) { this.Context.Entry(model).OriginalValues[\"Version\"] = version; } } } The primary thing that the DefaultMappedEntityDomainManager does that the original doesn't is in the SetOriginalVersion method. This causes the model to be updated with a new version, allowing for conflict detection in the domain manager. If we move now to the models on the mobile client, we see some fairly standard models: public class Customer { public string Id { get; set; } public string FullName { get; set; } public string Address { get; set; public decimal? Latitude { get; set; } public decimal? Longitude { get; set; } } public class Equipment { public string Id { get; set; } public string Name { get; set; } public string Asset { get; set; } public string Description { get; set; } } public class Job { public const string CompleteStatus = \"Completed\"; public const string InProgressStatus = \"On Site\"; public const string PendingStatus = \"Not Started\"; public string Id { get; set; } public string AgentId { get; set; } public DateTimeOffset? StartTime { get; set; } public DateTimeOffSet? EndTime { get; set; } public string Status { get; set; } public string Description { get; set; } public Customer Customer { get; set; } public List<Equipment> Equipments { get; set; } [Version] public string Version { get; set; } } In this case, we only need to sync the Job table, so we can define it in the InitializeAsync() method on the client: public async Task InitializeAsync() { var store = new MobileServiceSQLiteStore(\"localdata.db\"); store.DefineTable<Job>(); await MobileService.SyncContext.InitializeAsync(store); } You can use GetSyncTable<Job>() to get a reference to the table and deal with it as you normally would. I'd expect in this case that the Customer and Equipment would be handled elsewhere - maybe a separate web application that customer service agents use, for example. So, what are the caveats? The first is that the Job, Customer and Equipment data all comes down as one record. This has a side effect of ensuring that the Customer and Equipment data is read-only. You can only update the information in the Job table. This is also a very time consuming process to set up properly. Automapper is known as a fairly picky piece of software to integrate, so extra time must be allotted to make it work correctly. In the end, I prefer handling tables individually and handling relationship management on the mobile client manually. This causes more code on the mobile client but makes the server much simpler by avoiding most of the complexity of relationships.","title":"Existing Table Relationships with the MappedEntityDomainManager"},{"location":"chapter3/domainmgr/#nosql-storage-with-the-storagedomainmanager","text":"What if you don't want to use a SQL backend for your service? Relationships between entities are not that important in the mobile client and Azure Table Storage costs significantly less than SQL Azure. There are always trade-offs between various storage providers. A Domain Manager enables you to swap out the storage for one of your own choosing. Azure Mobile Apps provides a domain manager for Azure Table Storage. Azure Table Storage is Microsoft's NOSQL key/attribute store. It has a schemaless design, which (at least theoretically) enables you to adapt your data models as the application evolves without having to worry about the schema. To see this in action, let's rework the existing data store (which has tags and todoitems as tables) to use Table Storage. First up, we need to set up a suitable environment. This involves: Create a Resource Group Create an Azure App Service Set up authentication on the Azure App Service Create a Storage Account Link the Storage Account to the Azure App Service. We've already covered the first three items in previous chapters. The important element here is that we do not create a SQL database. We are going to be using Table Storage instead so we don't need it. To create a Storage Account: Log on to the Azure portal . Click the big + NEW button in the top left corner. Click Data + Storage , then Storage account . Fill in the form: The name can only contain letters and numbers and must be unique. A GUID without the dashes is a good choice. The Deployment model should be set to Resource manager . The Account kind should be set to General purpose . The Performance should be set to Standard for this example. The Replication should be set to Locally-redundant storage (LRS) . Set the Resource group to your existing resource group. Set the Location to the same location as your App Service. Click Create . Just like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. We have selected the slowest performance and least redundant options here to keep the cost down on your service. Warn There is no \"free\" option for Azure Storage. You pay by the kilobyte depending on the performance and redundancy selected. Once the Azure Storage account is deployed, you can link the storage account to your App Service: Open your App Service in the Azure portal . Click Data Connections under the MOBILE section in the settings menu. Click + ADD In the Add data connection blade: Set the Type to Storage . Click the Storage link. In the Storage Account selector, click the storage account you just created. Click the Connection string . In the Connection string selector, make a note of the Name field. Click OK . Click OK to close the Add data connection blade. Click on the Application Settings menu option, then scroll down to the Connection Strings section. Note that the portal has created the connection string as an App Setting for you with the right value: DefaultEndpointsProtocol=https;AccountName=thebook;AccountKey=<key1> The key is the access key for the storage. When a storage account is created, two keys are also created. If you re-generate the storage access keys, remember to update your connection string. By default, the connection string is called MS_AzureStorageAccountConnectionString and we will use that throughout. Now that our resources are set up, let's look at the Backend project. This started off as a standard Azure Mobile Apps template. The template assumes you are going to use SQL Azure, so there is quite a bit of work to convert the provided template to use Azure Table Storage. Let's start with the App_Start\\Startup.MobileApp.cs file. There is no Entity Framework, so that needs to be stripped out: using System.Configuration; using System.Web.Http; using Microsoft.Azure.Mobile.Server; using Microsoft.Azure.Mobile.Server.Authentication; using Microsoft.Azure.Mobile.Server.Config; using Owin; namespace Backend { public partial class Startup { public static void ConfigureMobileApp(IAppBuilder app) { HttpConfiguration config = new HttpConfiguration(); new MobileAppConfiguration() .AddTables() .ApplyTo(config); app.UseWebApi(config); } } } We've made three changes: We've removed the database seeding. We've removed the database initializer. We've changed AddTablesWithEntityFramework() to AddTables() . There is extra work needed with Entity Framework. Since we aren't using it, we don't need the additional work. We do, however, need to create the ASP.NET routes to the table controllers. Tip You must add the Microsoft.Azure.Mobile.Server.Storage package from NuGet. Let's move onto the DataObjects. These are very similar: namespace Backend.DataObjects { public class TodoItem : StorageData { public string Text { get; set; } public bool Complete { get; set; } } } Each storage implementation will likely need their own implementation of the ITableData interface. The StorageData class performs the same duties as the EntityData class for Entity Framework based backends. Tip You can remove the Models directory and the DbContext for the project. These are only needed when working with Entity Framework. The Azure Table Storage SDK is completely async driven. Fortunately, the domain manager specification (codified in the definition of IDomainManager ) allows both async and synchronous usage. This does require a change to our controller: using System.Collections.Generic; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using System.Web.Http.OData.Query; using Backend.DataObjects; using Microsoft.Azure.Mobile.Server; namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { const string connectionString = \"MS_AzureStorageAccountConnectionString\"; const string tableName = \"TodoItem\"; protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); DomainManager = new StorageDomainManager<TodoItem>(connectionString, tableName, Request, enableSoftDelete: true); } // GET tables/TodoItem public async Task<IEnumerable<TodoItem>> GetAllTodoItemsAsync(ODataQueryOptions query) { return await QueryAsync(query); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<SingleResult<TodoItem>> GetTodoItemAsync(string id) { return await LookupAsync(id); } // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<TodoItem> PatchTodoItemAsync(string id, Delta<TodoItem> patch) { return await UpdateAsync(id, patch); } // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItemAsync(TodoItem item) { TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task DeleteTodoItemAsync(string id) { await DeleteAsync(id); } } } Note how we instantiate the storage domain controller. This requires a connection string and the name of the table. We have created the connection string in the portal, but we have not exposed that connection string to our ASP.NET application. We need to edit the Web.config file as well: <connectionStrings> <add name=\"MS_AzureStorageAccountConnectionString\" connectionString=\"UseDevelopmentStorage=true\"/> </connectionStrings> This will be overwritten by the connection string in the portal. If you are running the service locally, you can use this setting with the Azure Storage Emulator. If you don't add this line to the Web.config , you will not be able to run this server locally. This backend can now be published and we can work with Postman to test it out. For instance, let's try adding a simple test of getting some data: This is exactly the same response as we got when we don't have any data from the Entity Framework version. Let's add a record: There are a couple of things to note here. Firstly, the Id must be specified. It also must be of a specific form. There are two numbers. The first is a partition key and the second is a row key. Tables are partitioned to support load balancing across storage notes. It can be anything you wish it to be. Similarly, the row key is unique within a partition. We can use this information to generate a suitable Id if one is not provided: // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItemAsync(TodoItem item) { if (item.Id == null || item.Id == \"'',''\") { item.Id = GenerateUniqueId(); } TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } private string GenerateUniqueId() { var partitionId = \"1\"; var rowId = Guid.NewGuid().ToString(\"N\"); return $\"'{partitionId}','{rowId}'\"; } The value '','' is the default value of the Id column. However, that is not good enough to get a successful store. This code generates a unique identifier in the right format. A single partition is reasonable for most applications. If you intend on storing massive amounts of data, the data partitioning scheme will require some consideration (just like any other NoSQL application). Tip You will need a copy of the GenerateUniqueId() method if you generate unique identifiers for your records within your mobile client. The partition key and row key are returned as part of the record. You can use the Cloud Explorer if you wish to see the data stored in Azure Table Storage. Expand the Storage Accounts node, then expand the appropriate nodes: your storage account, Tables , TodoItem . You can open the table editor or delete the table from there. Within the table editor, you can right-click on any row to delete or edit values. This will update the time stamp and etag, ensuring that your mobile clients are updated as well.","title":"NoSQL Storage with the StorageDomainManager"},{"location":"chapter3/domainmgr/#limitations-of-the-azure-storage-domain-manager","text":"There are, of course, caveats to working in Azure Mobile Apps with Azure Table Storage. There are three major caveats that you should be aware of. There are no relationships possible with Azure Table Storage. Only $filter , $top and $select are supported in the URI. Offline sync only supports flat objects. Let's take a look at each in turn. No relationhips You have to work at relationships and relationships between entities are severely restricted in Entity Framework. However, they are possible. Not so with Azure Table Storage. The NoSQL store has no concept of a relationship of any description. This is not a major caveat since your mobile client similarly has no notion of relationships. Just treat every table as distinct. Limited support for OData query options Only $filter , $top and $select are supported by the OData interface . Since the Azure Table Storage Domain Manager passes the incoming OData query to the Storage driver intact, this limitation is passed on to the OData interface for Azure Mobile Apps. Specifically, this means paging is handled differently. With the EntityDomainManager , paging was accomplished by using $skip and $top to get more records until zero records were returned. With the StorageDomainManager , a Link header is returned when there are more records. The Link header contains the URI that you need to retrieve to get the next page of the results. This has implications for how you receive more than 50 records. Offline sync only supports \"flat\" objects One of the common reasons for using NoSQL stores is that you can store pretty much any document you wish. You just have to have a JSON representation of the object cross the wire. If you have complex objects stored in Azure Table Storage, they won't be able to be stored in the offline cache. The offline cache is based on SQLite and inherits the limitations of that resource. In particular, this means no complex types. Using a NoSQL store seems like a great idea. However, the limitations of the platform make Azure Table Storage a poor choice for this particular function. The Azure Table Storage is ill-suited to the demands of a mobile client backend. One of the great uses of the Azure Table Storage Domain Manager is to see how you can write your own domain manager. The code for the domain manager (and the ITableData interface) is relatively simple since it passes through the OData query to Azure Storage. This allows you to see what is truly involved in writing a domain manager.","title":"Limitations of the Azure Storage Domain Manager"},{"location":"chapter3/projection/","text":"Projecting a Data Set \u00b6 We have thus far looked at what it takes to project a whole SQL database table into the mobile world. We can easily do both pre-existing and greenfield databases with code-first and database-first methodologies. The next logical thing is to see what we can do to adjust the transfer of data. How do we filter and transform the data as requests are sent to the server. There are two places where adjustment of the transfer is accomplished. I recommend spending time on the server adjusting the table controller so that security policies are assured. The set of data that a mobile client can see should be the complete set of data that the user of that mobile device is allowed to see. We can then adjust the view of that data at the client. For example, let's say that the user is a sales person. They are allowed to see the information for their accounts, but only wants to see the records for the accounts that have planned to visit within the next week. We would place the limitation on what records they can see on the server, but place the date range manipulation on the client. In this section, we will look at all the things one can do on in the table controller on the server. Basics of Projection \u00b6 There are four basic things we will want to do with table controllers: Filters adjust the data that the requesting user can see. We would normally apply a filter to all methods EXCEPT the Create or Insert method. This is the most common adjustment that is coded in the table controller as filtering is the key to enforcing security policy. Transforms adjust the data that is being sent to the table controller before it is stored. It is used in two areas. First, it is used to automatically inject necessary fields for ensuring the security filters can be applied. For instance, if we wish to have a per-user data store (where a user can only see their own records), then we will need to store the user ID of the requesting user. Secondly, it is used to insert point-in-time lookups into a record. For instance, if we wish to record the current price of an item at the time the record was inserted into the table, we would do this with a transform. Validations do not adjust the data. Validations ensure that the data is correct according to the server model. Your data may, for example, store an age indirectly by storing the year of birth. It's highly unlikely that you will want to support the entire range of possible years. You definitely don't want to support years in the future. Finally, Hooks allow another piece of code to be triggered either before or after the request has been processed. For example, we may wish to send a push notification on a valid insert, or kick off an order processing work flow when a record is updated with an approval to ship. We won't be covering hooks in this chapter as we have a whole chapter on customized requests later on. Projection Recipes \u00b6 There are a few \"standard\" projects we see all the time and these are great ways to learn how to do projections. Per-User Data \u00b6 The first projection that pretty much everyone implements is the Per-User Data projection. In this recipe, we want the user to only see records that they have inserted. For example, let's update our TodoItem table to support per-user data. This involves three parts: A Filter that limits data to only the logged in user. A Transform that updates an inserted record with the logged in user. A Validation that ensures an updated or deleted record is owned by the user. The logged in user is available as the User object, but you have to cast it to a ClaimsPrinicipal to access the claims that are sent inside the identity token. I tend to use a public property as an implementation: public string UserId { get { var principal = this.User as ClaimsPrincipal; return principal.FindFirst(ClaimTypes.NameIdentifier).Value; } } Tip It's generally a good idea to use the SID as the user ID for the authenticated user in security applications. The user can change the email address or username associated with the account, but the SID never changes. We need an extra property in the DataObjects\\TodoItem.cs class (in the Backend project) to hold the extra security claim that we will be adding later: using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class TodoItem : EntityData { public string UserId { get; set; } public string Text { get; set; } public bool Complete { get; set; } } } Remember to do a code-first migration if you are doing this on an existing service. Let's take a look at the PostTodoItem() first. This requires the Transform to ensure the UserId field is filled in. We've already defined the UserId field, so we can inject that in the inbound object: // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { item.UserId = UserId; TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } Transforms tend to be short. This is deliberate. We don't want any of our code in a table controller to do too much work. The heavy lifting is done by the database, with the ASP.NET table controller being a conduit for translating requests into responses. This allows us to support more users on less virtual hardware. The Filter is a relatively simple affair in this case. We ensure that the only records returned are those that belong to the user. For example, here is a simplistic filter applied to the GetAll method: // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query().Where(item => item.UserId.Equals(UserId)); } The Query() and Lookup(id).Queryable methods return IQueryable objects. The IQueryable is used to represent a query, so we can alter it with LINQ. A filter is merely a LINQ expression to limit the records being returned. There might be another filter sent by the client, in which case this filter will be tacked on the end of the request. For instance, let's say that the client requests only records where Complete == False . When this comes through the GetAllTodoItems() method, the resulting SQL code will look something like this: SELECT * FROM [dbo].[TodoItems] WHERE (Complete = false) AND (UserId = @0); The @0 parameter will be replaced by the users SID. Warn If a user is not logged in (i.e. you forgot to add the [Authorize] attribute), the User object will be null and the server will produce a 500 Internal Server Error back to the client. This can get a little unwieldy for complex filters, however. Since the filters are applied in two different places (and are generally used for validation as well), I like to abstract them into a LINQ extension method . Create a class in Extensions\\TodoItemExtensions.cs (you will have to create the Extensions directory) with the following contents: using System.Linq; using Chapter3.DataObjects; namespace Chapter3.Extensions { public static class TodoItemExtensions { public static IQueryable<TodoItem> PerUserFilter(this IQueryable<TodoItem> query, string userid) { return query.Where(item => item.UserId.Equals(userid)); } } } We can use this to simplify our filters and make them more readable: // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query().PerUserFilter(UserId); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) { return new SingleResult<TodoItem>(Lookup(id).Queryable.PerUserFilter(UserId)); } Note that we need to apply the filter we have written to both the Get methods. When we look at the Delete and Patch methods, we only have to validate that the UserId owns the id we are updating. For that, I write a custom validation method. This method is in the table controller: private void ValidateOwner(string id) { var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault<TodoItem>(); if (result == null) { throw new HttpResponseException(HttpStatusCode.NotFound); } } The validation method must throw an appropriate HttpResponseException if the validation fails. It's common to return a 404 Not Found error rather than a 403 Forbidden error for security reasons. Returning a 403 Forbidden error confirms that the Id exists, which is a data leakage. Returning a 404 Not Found error means that a rogue client cannot tell the difference between \"I can't access the record\" and \"the record doesn't exist\". We can use this validation method in each method that requires it: // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) { ValidateOwner(id); return UpdateAsync(id, patch); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) { ValidateOwner(id); return DeleteAsync(id); } Although this is a very basic example of a filtered table, we can see three different techniques: A Filter implemented as a LINQ query in an IQueryable extension method, applied to both Get methods. A Transform implemented inside the Post method. A Validation implemented using the filter as a method in the table controller, applied to Patch and Delete methods. I set up a copy of the TaskList from Chapter 2 with Azure Active Directory Client Flow for authentication. We can look at the SQL table contents after we log in with this client and add a new task: Note that the UserId is the security ID of the authenticated user. The security ID is a stable ID for the user. The security ID does not change if the user changes their username or email address. Per-Group Data \u00b6 Let's say we have a mobile client that a sales person uses to enter data about sales. He might be able to pick from a list of industries that the account is in. The enterprise may further organize those industries as groups, with several people within the organization able to view the accounts associated with a specific industry. In this case: We want to limit the mobile client to only view accounts for groups to which he belongs. We want to allow the mobile client to submit new accounts for any group to which he belongs. Updates and Deletes should not adjust the group field. The limit will be implemented as a filter. The update and insert methods will require a validation method (comparing the submitted group with the list of groups to which the user belongs). We have another table in DataObjects\\Example.cs . Let's extend it to support a GroupId: using System; using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class Example : EntityData { public string GroupId { get; set; } public string StringField { get; set; } public int IntField { get; set; } public double DoubleField { get; set; } public DateTimeOffset DateTimeField { get; set; } } } We'll store the group ID in the additional field. Don't forget to use a code-first migration to update the database. Each of the validations and filters requires a list of the groups the user belongs to. This can be achieved using a claim lookup: /// <summary> /// Get the list of groups from the claims /// </summary> /// <returns>The list of groups</returns> public async Task<List<string>> GetGroups() { var creds = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); return creds.UserClaims .Where(claim => claim.Type.Equals(\"groups\")) .Select(claim => claim.Value) .ToList(); } Tip If you are using claims as part of your security model, you should add the claims that you are using to the identity token that is used for authentication. You can do this with custom authentication by calling LoginAsync() twice - once for the standard login method and the second time to adjust the token through the custom auth. Our filter is defined as a LINQ extension (just like in the per-user filter) in Extensions\\ExampleExtensions.cs : using System.Collections.Generic; using System.Linq; using Chapter3.DataObjects; namespace Chapter3.Extensions { public static class ExampleExtensions { public static IQueryable<Example> PerGroupFilter(this IQueryable<Example> query, List<string> groups) { return query.Where(item => groups.Contains(item.GroupId)); } } } We can use this LINQ extension on the retrieval methods: // GET tables/Example public async Task<IQueryable<Example>> GetAllExample() { var groups = await GetGroups(); return Query().PerGroupFilter(groups); } // GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<SingleResult<Example>> GetExample(string id) { var groups = await GetGroups(); return new SingleResult<Example>(Lookup(id).Queryable.PerGroupFilter(groups)); } We have to convert each of the methods to a async method so that we can check the groups. Retrieving the group list for a user is an async method and this trickles down to the method being called. The validation method is also an async method (for the same reason). The Post and Patch methods look like this: /// <summary> /// Validator to determine if the provided group is in the list of groups /// </summary> /// <param name=\"group\">The group name</param> public async Task ValidateGroup(string group) { var groups = await GetGroups(); if (!groups.Contains(group)) { throw new HttpResponseException(HttpStatusCode.BadRequest); } } // PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<Example> PatchExample(string id, Delta<Example> patch) { await ValidateGroup(patch.GetEntity().GroupId); return await UpdateAsync(id, patch); } // POST tables/Example public async Task<IHttpActionResult> PostExample(Example item) { await ValidateGroup(item.GroupId); Example current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } It's appropriate to throw a 400 Bad Request for a validation error in this case as the user is authenticated at this point. The user is not being exposed by the response. There is no transform in this recipe as the group ID is being sent on each update request. Friends Data \u00b6 One of the common social patterns is a \"friends feed\". We can post to our feed and we can see both our messages and our friends messages. In this recipe, we will have three tables. The first is the Users table with the following model: using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class User : EntityData { public string EmailAddress { get; set; } public string Name { get; set; } } } Tip Don't forget to add a DbSet<> for each table to the MobileServiceContext to add the table. In our application, we will update the Users table via a custom authentication controller . After we have logged in via Azure Active Directory, we call the InvokeApiAsync() method to call the custom authentication controller and get a new token with some extra information in it. We'll cover custom authentication controllers in a later chapter. using System; using System.Data.Entity.Migrations; using System.IdentityModel.Tokens; using System.Linq; using System.Security.Claims; using System.Security.Principal; using System.Threading.Tasks; using System.Web.Http; using Chapter3.DataObjects; using Chapter3.Models; using Microsoft.Azure.Mobile.Server.Authentication; using Microsoft.Azure.Mobile.Server.Login; using Newtonsoft.Json; namespace Chapter3.Controllers { [Authorize] [Route(\"auth/login/custom\")] public class CustomAuthController : ApiController { MobileServiceContext dbContext; public CustomAuthController() { dbContext = new MobileServiceContext(); string website = Environment.GetEnvironmentVariable(\"WEBSITE_HOSTNAME\"); Audience = $\"https://{website}/\"; Issuer = $\"https://{website}/\"; SigningKey = Environment.GetEnvironmentVariable(\"WEBSITE_AUTH_SIGNING_KEY\"); } public string Audience { get; set; } public string Issuer { get; set; } public string SigningKey { get; set; } [HttpPost] public async Task<IHttpActionResult> Post() { var creds = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); var sid = ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; var email = creds.UserClaims .FirstOrDefault(claim => claim.Type.EndsWith(\"emailaddress\")) .Value; var name = creds.UserClaims .FirstOrDefault(claim => claim.Type.EndsWith(\"name\")) .Value; // Insert the record information into the database User user = new User() { Id = sid, Name = name, EmailAddress = email }; dbContext.Users.AddOrUpdate(user); dbContext.SaveChanges(); // Mind a new token based on the old one plus the new information var newClaims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, sid), new Claim(JwtRegisteredClaimNames.Email, email), new Claim(\"name\", name) }; JwtSecurityToken token = AppServiceLoginHandler.CreateToken( newClaims, SigningKey, Audience, Issuer, TimeSpan.FromDays(30)); // Return the token and user ID to the client return Ok(new LoginResult() { AuthenticationToken = token.RawData, UserId = sid }); } } public class LoginResult { [JsonProperty(PropertyName = \"authenticationToken\")] public string AuthenticationToken { get; set; } [JsonProperty(PropertyName = \"user_id\")] public string UserId { get; set; } } } The actual database update is done by Entity Framework. The ASP.NET service is just a regular ASP.NET service using Entity Framework, so all the same facilities are available as that configuration. In this case, we take the provided token (which is the same token that Azure Active Directory client-flow returns) and return a modified token that includes a couple of extra fields. During this process, we update the database by adding or inserting (also known as upserting) a record with the Id field set to the security ID and the additional fields we need. I need a table to implement the \"friends\" relationship. My friends are not stored in Azure Active Directory. If I were using a social provider, I could use the friends feed from that social provider by doing a Graph API lookup. In this case, I'm going to use the following model: namespace Chapter3.DataObjects { public class Friend { public string UserId { get; set; } public string FriendId { get; set; } } } This is not based on EntityData because I am not going to expose this table to the mobile client. It's purely for determining what records I am going to show to the mobile client. In this example, I will maintain the data in this table manually. A \"real\" application would have some sort of custom workflow to add friends and get the friends to approve the connection. To get the list of \"friends I can see\", I will request a list of the FriendId field where the UserId is my UserId. The final table in the trio is the Messages table. This will be downloaded to the mobile client so it has to be based on the EntityData base class. In addition, the inserts for this table are going to look a lot like a per-user table. I need the UserId field to properly maintain the security model. using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class Message : EntityData { public string UserId { get; set; } public string Text { get; set; } } } Let's think about the security model we want to implement in the Messages table controller: A filter will allow the viewing of the users own data or any data that has an association in the Friends table. A transform will set the owner of the record to my UserId We will remove the ability to update or delete records since this is a write-once read-many table. Let's look at the PostMessage() method for the MessageController first. This is practically identical to the PostTodoItem() method in the per-user recipe: public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; // POST tables/Message public async Task<IHttpActionResult> PostMessage(Message item) { item.UserId = UserId; Message current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } The filter is a little harder. We are going to use the Fluent Syntax for LINQ to provide the right logic. The Fluent Syntax is also known as \"Query Syntax\" or the \"declarative syntax\", depending on the author. The extension method looks like this: using Chapter3.DataObjects; using System.Data.Entity; using System.Linq; namespace Chapter3.Extensions { public static class MessageExtensions { public static IQueryable<Message> OwnedByFriends(this IQueryable<Message> query, DbSet<Friend> friends, string userId) { var myPosts = from m in query let fr = (from f in friends where f.FriendId == userId select f.UserId) where m.UserId == userId || fr.Contains(m.UserId) select m; return myPosts; } } } The LINQ query selects the messages from the Messages table where the author is either the mobile client user or the mobile client user is listed as a friend of the author. My query methods in the table controller now look similar to the per-user data: public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; // GET tables/Message public IQueryable<Message> GetAllMessage() { return Query().OwnedByFriends(context.Friends, UserId); } // GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<Message> GetMessage(string id) { return new SingleResult<Message>(Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId)); } Using LINQPad to test LINQ Queries \u00b6 I tend to struggle with LINQ queries. Fortunately, there are a plethora of blogs, tutorials and tools out there to assist. One of my favorite tools is LINQPad . LINQPad gives you an interactive playground for testing your LINQ queries. In this case, I created a new query with the following contents: public class Message { public string UserId; public string Text; } public class Friend { public string UserId; public string FollowerId; } void Main() { List<Friend> friends = new List<Friend>() { new Friend() { UserId = \"adrian\", FollowerId = \"donna\" }, new Friend() { UserId = \"fabio\", FollowerId = \"donna\" }, new Friend() { UserId = \"fabio\", FollowerId = \"adrian\" } }; List<Message> messages = new List<Message>() { new Message() { UserId = \"adrian\", Text = \"message 1\" }, new Message() { UserId = \"donna\", Text = \"message 2\" }, new Message() { UserId = \"fabio\", Text = \"message 3\" } }; var user = \"fabio\"; var q = from m in messages let fr = (from f in friends where f.FollowerId == user select f.UserId) where m.UserId == user || fr.Contains(m.UserId) select m; q.Dump(); } I have a friends table and a messages table. User \"donna\" should be able to see three messages, user \"adrian\" should be able to see two messages and user \"fabio\" should be able to see just one message. By changing the value of the user variable, I can test the query I am writing. I don't need to set up a database (although LINQPad supports that as well). Best Practices \u00b6 There are a number of best practices that I think are important in developing table controllers: Optimize Operations You should always optimize the CRUD operations that are implemented in a table controller. This means limiting the code so that only filters , transforms and validators are used. You can use hooks as an asynchronous way to handle custom code if something else needs to happen when a mobile client inserts, updates or deletes a record. (We will be delving into hooks during the custom code chapter later on). You should NOT insert custom code into a table controller that runs synchronously. Implement Security Policy with Filters The mobile backend should be concerned with security. What can the connecting user see? Use filters to ensure that the connecting user can only see the data that they are allowed to see. There are several examples of bad filters. For example, if a user normally wants to see the last 7 days worth of messages, but is allowed to see all messages. I would implement this particular case as a client-side filter as it has nothing to do with security. Use LINQ Extension Methods LINQ extension methods can be used to great effect to make your CRUD methods more readable. I love readable code. For example, consider the following two code snippets from the last recipe: public IQueryable<Message> GetAllMessage() { // Recipe #1 return Query().OwnedByFriends(context.Friends, UserId); // Recipe #2 return from m in Query() let fr = (from f in context.Friends where f.FriendId == UserId select f.UserId) where m.UserId == UserId || fr.Contains(m.UserId) select m; } The first recipe makes the intent of the filter very clear. I have to work at understanding the specific implementation of the second method.","title":"Data Projection and Queries"},{"location":"chapter3/projection/#projecting-a-data-set","text":"We have thus far looked at what it takes to project a whole SQL database table into the mobile world. We can easily do both pre-existing and greenfield databases with code-first and database-first methodologies. The next logical thing is to see what we can do to adjust the transfer of data. How do we filter and transform the data as requests are sent to the server. There are two places where adjustment of the transfer is accomplished. I recommend spending time on the server adjusting the table controller so that security policies are assured. The set of data that a mobile client can see should be the complete set of data that the user of that mobile device is allowed to see. We can then adjust the view of that data at the client. For example, let's say that the user is a sales person. They are allowed to see the information for their accounts, but only wants to see the records for the accounts that have planned to visit within the next week. We would place the limitation on what records they can see on the server, but place the date range manipulation on the client. In this section, we will look at all the things one can do on in the table controller on the server.","title":"Projecting a Data Set"},{"location":"chapter3/projection/#basics-of-projection","text":"There are four basic things we will want to do with table controllers: Filters adjust the data that the requesting user can see. We would normally apply a filter to all methods EXCEPT the Create or Insert method. This is the most common adjustment that is coded in the table controller as filtering is the key to enforcing security policy. Transforms adjust the data that is being sent to the table controller before it is stored. It is used in two areas. First, it is used to automatically inject necessary fields for ensuring the security filters can be applied. For instance, if we wish to have a per-user data store (where a user can only see their own records), then we will need to store the user ID of the requesting user. Secondly, it is used to insert point-in-time lookups into a record. For instance, if we wish to record the current price of an item at the time the record was inserted into the table, we would do this with a transform. Validations do not adjust the data. Validations ensure that the data is correct according to the server model. Your data may, for example, store an age indirectly by storing the year of birth. It's highly unlikely that you will want to support the entire range of possible years. You definitely don't want to support years in the future. Finally, Hooks allow another piece of code to be triggered either before or after the request has been processed. For example, we may wish to send a push notification on a valid insert, or kick off an order processing work flow when a record is updated with an approval to ship. We won't be covering hooks in this chapter as we have a whole chapter on customized requests later on.","title":"Basics of Projection"},{"location":"chapter3/projection/#projection-recipes","text":"There are a few \"standard\" projects we see all the time and these are great ways to learn how to do projections.","title":"Projection Recipes"},{"location":"chapter3/projection/#per-user-data","text":"The first projection that pretty much everyone implements is the Per-User Data projection. In this recipe, we want the user to only see records that they have inserted. For example, let's update our TodoItem table to support per-user data. This involves three parts: A Filter that limits data to only the logged in user. A Transform that updates an inserted record with the logged in user. A Validation that ensures an updated or deleted record is owned by the user. The logged in user is available as the User object, but you have to cast it to a ClaimsPrinicipal to access the claims that are sent inside the identity token. I tend to use a public property as an implementation: public string UserId { get { var principal = this.User as ClaimsPrincipal; return principal.FindFirst(ClaimTypes.NameIdentifier).Value; } } Tip It's generally a good idea to use the SID as the user ID for the authenticated user in security applications. The user can change the email address or username associated with the account, but the SID never changes. We need an extra property in the DataObjects\\TodoItem.cs class (in the Backend project) to hold the extra security claim that we will be adding later: using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class TodoItem : EntityData { public string UserId { get; set; } public string Text { get; set; } public bool Complete { get; set; } } } Remember to do a code-first migration if you are doing this on an existing service. Let's take a look at the PostTodoItem() first. This requires the Transform to ensure the UserId field is filled in. We've already defined the UserId field, so we can inject that in the inbound object: // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { item.UserId = UserId; TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } Transforms tend to be short. This is deliberate. We don't want any of our code in a table controller to do too much work. The heavy lifting is done by the database, with the ASP.NET table controller being a conduit for translating requests into responses. This allows us to support more users on less virtual hardware. The Filter is a relatively simple affair in this case. We ensure that the only records returned are those that belong to the user. For example, here is a simplistic filter applied to the GetAll method: // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query().Where(item => item.UserId.Equals(UserId)); } The Query() and Lookup(id).Queryable methods return IQueryable objects. The IQueryable is used to represent a query, so we can alter it with LINQ. A filter is merely a LINQ expression to limit the records being returned. There might be another filter sent by the client, in which case this filter will be tacked on the end of the request. For instance, let's say that the client requests only records where Complete == False . When this comes through the GetAllTodoItems() method, the resulting SQL code will look something like this: SELECT * FROM [dbo].[TodoItems] WHERE (Complete = false) AND (UserId = @0); The @0 parameter will be replaced by the users SID. Warn If a user is not logged in (i.e. you forgot to add the [Authorize] attribute), the User object will be null and the server will produce a 500 Internal Server Error back to the client. This can get a little unwieldy for complex filters, however. Since the filters are applied in two different places (and are generally used for validation as well), I like to abstract them into a LINQ extension method . Create a class in Extensions\\TodoItemExtensions.cs (you will have to create the Extensions directory) with the following contents: using System.Linq; using Chapter3.DataObjects; namespace Chapter3.Extensions { public static class TodoItemExtensions { public static IQueryable<TodoItem> PerUserFilter(this IQueryable<TodoItem> query, string userid) { return query.Where(item => item.UserId.Equals(userid)); } } } We can use this to simplify our filters and make them more readable: // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query().PerUserFilter(UserId); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) { return new SingleResult<TodoItem>(Lookup(id).Queryable.PerUserFilter(UserId)); } Note that we need to apply the filter we have written to both the Get methods. When we look at the Delete and Patch methods, we only have to validate that the UserId owns the id we are updating. For that, I write a custom validation method. This method is in the table controller: private void ValidateOwner(string id) { var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault<TodoItem>(); if (result == null) { throw new HttpResponseException(HttpStatusCode.NotFound); } } The validation method must throw an appropriate HttpResponseException if the validation fails. It's common to return a 404 Not Found error rather than a 403 Forbidden error for security reasons. Returning a 403 Forbidden error confirms that the Id exists, which is a data leakage. Returning a 404 Not Found error means that a rogue client cannot tell the difference between \"I can't access the record\" and \"the record doesn't exist\". We can use this validation method in each method that requires it: // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) { ValidateOwner(id); return UpdateAsync(id, patch); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) { ValidateOwner(id); return DeleteAsync(id); } Although this is a very basic example of a filtered table, we can see three different techniques: A Filter implemented as a LINQ query in an IQueryable extension method, applied to both Get methods. A Transform implemented inside the Post method. A Validation implemented using the filter as a method in the table controller, applied to Patch and Delete methods. I set up a copy of the TaskList from Chapter 2 with Azure Active Directory Client Flow for authentication. We can look at the SQL table contents after we log in with this client and add a new task: Note that the UserId is the security ID of the authenticated user. The security ID is a stable ID for the user. The security ID does not change if the user changes their username or email address.","title":"Per-User Data"},{"location":"chapter3/projection/#per-group-data","text":"Let's say we have a mobile client that a sales person uses to enter data about sales. He might be able to pick from a list of industries that the account is in. The enterprise may further organize those industries as groups, with several people within the organization able to view the accounts associated with a specific industry. In this case: We want to limit the mobile client to only view accounts for groups to which he belongs. We want to allow the mobile client to submit new accounts for any group to which he belongs. Updates and Deletes should not adjust the group field. The limit will be implemented as a filter. The update and insert methods will require a validation method (comparing the submitted group with the list of groups to which the user belongs). We have another table in DataObjects\\Example.cs . Let's extend it to support a GroupId: using System; using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class Example : EntityData { public string GroupId { get; set; } public string StringField { get; set; } public int IntField { get; set; } public double DoubleField { get; set; } public DateTimeOffset DateTimeField { get; set; } } } We'll store the group ID in the additional field. Don't forget to use a code-first migration to update the database. Each of the validations and filters requires a list of the groups the user belongs to. This can be achieved using a claim lookup: /// <summary> /// Get the list of groups from the claims /// </summary> /// <returns>The list of groups</returns> public async Task<List<string>> GetGroups() { var creds = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); return creds.UserClaims .Where(claim => claim.Type.Equals(\"groups\")) .Select(claim => claim.Value) .ToList(); } Tip If you are using claims as part of your security model, you should add the claims that you are using to the identity token that is used for authentication. You can do this with custom authentication by calling LoginAsync() twice - once for the standard login method and the second time to adjust the token through the custom auth. Our filter is defined as a LINQ extension (just like in the per-user filter) in Extensions\\ExampleExtensions.cs : using System.Collections.Generic; using System.Linq; using Chapter3.DataObjects; namespace Chapter3.Extensions { public static class ExampleExtensions { public static IQueryable<Example> PerGroupFilter(this IQueryable<Example> query, List<string> groups) { return query.Where(item => groups.Contains(item.GroupId)); } } } We can use this LINQ extension on the retrieval methods: // GET tables/Example public async Task<IQueryable<Example>> GetAllExample() { var groups = await GetGroups(); return Query().PerGroupFilter(groups); } // GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<SingleResult<Example>> GetExample(string id) { var groups = await GetGroups(); return new SingleResult<Example>(Lookup(id).Queryable.PerGroupFilter(groups)); } We have to convert each of the methods to a async method so that we can check the groups. Retrieving the group list for a user is an async method and this trickles down to the method being called. The validation method is also an async method (for the same reason). The Post and Patch methods look like this: /// <summary> /// Validator to determine if the provided group is in the list of groups /// </summary> /// <param name=\"group\">The group name</param> public async Task ValidateGroup(string group) { var groups = await GetGroups(); if (!groups.Contains(group)) { throw new HttpResponseException(HttpStatusCode.BadRequest); } } // PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<Example> PatchExample(string id, Delta<Example> patch) { await ValidateGroup(patch.GetEntity().GroupId); return await UpdateAsync(id, patch); } // POST tables/Example public async Task<IHttpActionResult> PostExample(Example item) { await ValidateGroup(item.GroupId); Example current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } It's appropriate to throw a 400 Bad Request for a validation error in this case as the user is authenticated at this point. The user is not being exposed by the response. There is no transform in this recipe as the group ID is being sent on each update request.","title":"Per-Group Data"},{"location":"chapter3/projection/#friends-data","text":"One of the common social patterns is a \"friends feed\". We can post to our feed and we can see both our messages and our friends messages. In this recipe, we will have three tables. The first is the Users table with the following model: using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class User : EntityData { public string EmailAddress { get; set; } public string Name { get; set; } } } Tip Don't forget to add a DbSet<> for each table to the MobileServiceContext to add the table. In our application, we will update the Users table via a custom authentication controller . After we have logged in via Azure Active Directory, we call the InvokeApiAsync() method to call the custom authentication controller and get a new token with some extra information in it. We'll cover custom authentication controllers in a later chapter. using System; using System.Data.Entity.Migrations; using System.IdentityModel.Tokens; using System.Linq; using System.Security.Claims; using System.Security.Principal; using System.Threading.Tasks; using System.Web.Http; using Chapter3.DataObjects; using Chapter3.Models; using Microsoft.Azure.Mobile.Server.Authentication; using Microsoft.Azure.Mobile.Server.Login; using Newtonsoft.Json; namespace Chapter3.Controllers { [Authorize] [Route(\"auth/login/custom\")] public class CustomAuthController : ApiController { MobileServiceContext dbContext; public CustomAuthController() { dbContext = new MobileServiceContext(); string website = Environment.GetEnvironmentVariable(\"WEBSITE_HOSTNAME\"); Audience = $\"https://{website}/\"; Issuer = $\"https://{website}/\"; SigningKey = Environment.GetEnvironmentVariable(\"WEBSITE_AUTH_SIGNING_KEY\"); } public string Audience { get; set; } public string Issuer { get; set; } public string SigningKey { get; set; } [HttpPost] public async Task<IHttpActionResult> Post() { var creds = await User.GetAppServiceIdentityAsync<AzureActiveDirectoryCredentials>(Request); var sid = ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; var email = creds.UserClaims .FirstOrDefault(claim => claim.Type.EndsWith(\"emailaddress\")) .Value; var name = creds.UserClaims .FirstOrDefault(claim => claim.Type.EndsWith(\"name\")) .Value; // Insert the record information into the database User user = new User() { Id = sid, Name = name, EmailAddress = email }; dbContext.Users.AddOrUpdate(user); dbContext.SaveChanges(); // Mind a new token based on the old one plus the new information var newClaims = new Claim[] { new Claim(JwtRegisteredClaimNames.Sub, sid), new Claim(JwtRegisteredClaimNames.Email, email), new Claim(\"name\", name) }; JwtSecurityToken token = AppServiceLoginHandler.CreateToken( newClaims, SigningKey, Audience, Issuer, TimeSpan.FromDays(30)); // Return the token and user ID to the client return Ok(new LoginResult() { AuthenticationToken = token.RawData, UserId = sid }); } } public class LoginResult { [JsonProperty(PropertyName = \"authenticationToken\")] public string AuthenticationToken { get; set; } [JsonProperty(PropertyName = \"user_id\")] public string UserId { get; set; } } } The actual database update is done by Entity Framework. The ASP.NET service is just a regular ASP.NET service using Entity Framework, so all the same facilities are available as that configuration. In this case, we take the provided token (which is the same token that Azure Active Directory client-flow returns) and return a modified token that includes a couple of extra fields. During this process, we update the database by adding or inserting (also known as upserting) a record with the Id field set to the security ID and the additional fields we need. I need a table to implement the \"friends\" relationship. My friends are not stored in Azure Active Directory. If I were using a social provider, I could use the friends feed from that social provider by doing a Graph API lookup. In this case, I'm going to use the following model: namespace Chapter3.DataObjects { public class Friend { public string UserId { get; set; } public string FriendId { get; set; } } } This is not based on EntityData because I am not going to expose this table to the mobile client. It's purely for determining what records I am going to show to the mobile client. In this example, I will maintain the data in this table manually. A \"real\" application would have some sort of custom workflow to add friends and get the friends to approve the connection. To get the list of \"friends I can see\", I will request a list of the FriendId field where the UserId is my UserId. The final table in the trio is the Messages table. This will be downloaded to the mobile client so it has to be based on the EntityData base class. In addition, the inserts for this table are going to look a lot like a per-user table. I need the UserId field to properly maintain the security model. using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class Message : EntityData { public string UserId { get; set; } public string Text { get; set; } } } Let's think about the security model we want to implement in the Messages table controller: A filter will allow the viewing of the users own data or any data that has an association in the Friends table. A transform will set the owner of the record to my UserId We will remove the ability to update or delete records since this is a write-once read-many table. Let's look at the PostMessage() method for the MessageController first. This is practically identical to the PostTodoItem() method in the per-user recipe: public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; // POST tables/Message public async Task<IHttpActionResult> PostMessage(Message item) { item.UserId = UserId; Message current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } The filter is a little harder. We are going to use the Fluent Syntax for LINQ to provide the right logic. The Fluent Syntax is also known as \"Query Syntax\" or the \"declarative syntax\", depending on the author. The extension method looks like this: using Chapter3.DataObjects; using System.Data.Entity; using System.Linq; namespace Chapter3.Extensions { public static class MessageExtensions { public static IQueryable<Message> OwnedByFriends(this IQueryable<Message> query, DbSet<Friend> friends, string userId) { var myPosts = from m in query let fr = (from f in friends where f.FriendId == userId select f.UserId) where m.UserId == userId || fr.Contains(m.UserId) select m; return myPosts; } } } The LINQ query selects the messages from the Messages table where the author is either the mobile client user or the mobile client user is listed as a friend of the author. My query methods in the table controller now look similar to the per-user data: public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; // GET tables/Message public IQueryable<Message> GetAllMessage() { return Query().OwnedByFriends(context.Friends, UserId); } // GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<Message> GetMessage(string id) { return new SingleResult<Message>(Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId)); }","title":"Friends Data"},{"location":"chapter3/projection/#using-linqpad-to-test-linq-queries","text":"I tend to struggle with LINQ queries. Fortunately, there are a plethora of blogs, tutorials and tools out there to assist. One of my favorite tools is LINQPad . LINQPad gives you an interactive playground for testing your LINQ queries. In this case, I created a new query with the following contents: public class Message { public string UserId; public string Text; } public class Friend { public string UserId; public string FollowerId; } void Main() { List<Friend> friends = new List<Friend>() { new Friend() { UserId = \"adrian\", FollowerId = \"donna\" }, new Friend() { UserId = \"fabio\", FollowerId = \"donna\" }, new Friend() { UserId = \"fabio\", FollowerId = \"adrian\" } }; List<Message> messages = new List<Message>() { new Message() { UserId = \"adrian\", Text = \"message 1\" }, new Message() { UserId = \"donna\", Text = \"message 2\" }, new Message() { UserId = \"fabio\", Text = \"message 3\" } }; var user = \"fabio\"; var q = from m in messages let fr = (from f in friends where f.FollowerId == user select f.UserId) where m.UserId == user || fr.Contains(m.UserId) select m; q.Dump(); } I have a friends table and a messages table. User \"donna\" should be able to see three messages, user \"adrian\" should be able to see two messages and user \"fabio\" should be able to see just one message. By changing the value of the user variable, I can test the query I am writing. I don't need to set up a database (although LINQPad supports that as well).","title":"Using LINQPad to test LINQ Queries"},{"location":"chapter3/projection/#best-practices","text":"There are a number of best practices that I think are important in developing table controllers: Optimize Operations You should always optimize the CRUD operations that are implemented in a table controller. This means limiting the code so that only filters , transforms and validators are used. You can use hooks as an asynchronous way to handle custom code if something else needs to happen when a mobile client inserts, updates or deletes a record. (We will be delving into hooks during the custom code chapter later on). You should NOT insert custom code into a table controller that runs synchronously. Implement Security Policy with Filters The mobile backend should be concerned with security. What can the connecting user see? Use filters to ensure that the connecting user can only see the data that they are allowed to see. There are several examples of bad filters. For example, if a user normally wants to see the last 7 days worth of messages, but is allowed to see all messages. I would implement this particular case as a client-side filter as it has nothing to do with security. Use LINQ Extension Methods LINQ extension methods can be used to great effect to make your CRUD methods more readable. I love readable code. For example, consider the following two code snippets from the last recipe: public IQueryable<Message> GetAllMessage() { // Recipe #1 return Query().OwnedByFriends(context.Friends, UserId); // Recipe #2 return from m in Query() let fr = (from f in context.Friends where f.FriendId == UserId select f.UserId) where m.UserId == UserId || fr.Contains(m.UserId) select m; } The first recipe makes the intent of the filter very clear. I have to work at understanding the specific implementation of the second method.","title":"Best Practices"},{"location":"chapter3/relationships/","text":"Relationships \u00b6 One of the biggest benefits to using a SQL database over a NoSQL store is relationships between entities. Relationships provide the ability to normalize the data, allowing you to store the minimal amount of data for a specific use case on the mobile device. This reduces bandwidth usage and memory usage on the device. Relationships are a good thing. Unfortunately, relationships between tables are hard when one is working within an offline context. This is primarily caused by the need for resilience. Because we can do many updates to the tables on the offline client, the transactions that update the tables need to be co-ordinated. This is practically impossible in an offline context where one of the goals in bandwidth performance. Azure Mobile Apps, when used in an offline context, has an operations table. As you do each operation against a table, an entry is made in the operations table. The operations table is then replayed in order to the mobile backend to effect changes in the remote database. However, this also has the effect that we do not have transactions to allow the updating of multiple tables within the database at the same time. Each record in each table is updated individually. The push process that offline sync uses has major ramifications for how relationships between tables work. Specifically, only 1-way relationships will work in an offline sync world. 1-Way Relationships You can define relationships in Entity Framework with or without a virtual back-reference. Relationships with the virtual back-reference are known as 2-way relationships (because you can get back to the original model). Relationships with only a forward reference (and no knowledge of the original model) are said to have a 1-way relationship. A database model with only 1-way relationships can generally be represented with a tree structure. Let's take a quick example. We've been using the \"task list\" scenario for our testing thus far. Let's say that each task could be assigned a tag from a list of tags. We can use a 1-way 1:1 relationship between the tasks and the tags. To do that, we would store the Id of the tag in the task model. If, however, we could attach many tags to a single task, that would be a 1:Many relationship. 1:1 Relationships \u00b6 Let's take a look at our task list example, from the perspective of the models on the server side: using Microsoft.Azure.Mobile.Server; using System.ComponentModel.DataAnnotations.Schema; namespace ComplexTypes.DataObjects { public class Tag : EntityData { public string TagName { get; set; } } public class TodoItem : EntityData { public string Text { get; set; } public bool Complete { get; set; } #region Relationships public string TagId { get; set; } [ForeignKey(\"TagId\")] public Tag Tag { get; set; } #endregion } } 1:1 relationships are defined using a foreign key in the SQL database. We can use Entity Framework to define the foreign key relationship easily. In this case, our TodoItem model will, have a TagId that contains the Id field of the tag. I also created a pair of table controllers for these models in the normal manner. Finally, I've created some records using the Seed() method within the App_Start\\Startup.MobileApp.cs file to give us some test data. If we take a look at records through Postman, we will get the following: Note that the first item has a reference to a tag, by virtue of the TagId. The second item does not have a tag assigned, so the value of TagId is null. When we implement the client, we are going to download these tables independently. The linkage and relationships between the tables is lost when going from the backend to the offline client. We have to link them together ourselves. This is why the \"1-way\" relationship is necessary. In a 2-way relationship, a tag and task would have to be created at the same time as part of an SQL transaction. In a 1-way relationship, the tag can be created THEN the task that has the relationship is created Tip When you think of all the mobile applications you own, you will realize that 1-way relationships are the normal state of affairs. Very few data models for mobile apps actually require a two-way relationship. When you are developing the mobile client, the Tag is removed from the model: using TaskList.Helpers; namespace TaskList.Models { public class Tag : TableData { public string TagName { get; set; } } public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } public string TagId { get; set; } } } One can easily retrieve the tag information with a LINQ query on the Tag table: var tag = tagTable.FirstOrDefault(tag => tag.Id.Equals(task.TagId)).Value; There are a couple of rules you must follow within your client code: You need to ensure that you create a tag before associating that tag with a task. You need to store the TagId with the task, not the Tag object (as you would normally do within Entity Framework). The former is generally handled for you. If you InsertAsync a tag in offline mode, it will be placed into the operations queue prior to anything that uses it. Since the operations queue is processed in order, the tag will be sent to the backend prior to any record updates that would use it. 1:Many Relationships \u00b6 What if we had a list of messages and wanted to assign more than one tag to each record? In this case, we would need a 1:Many relationship. Setting up a 1:Many relationship on the backend again relies on Entity Framework syntax: using Microsoft.Azure.Mobile.Server; using System.Collections.Generic; namespace TaskList.Models { public class Tag : EntityData { public string TagName { get; set; } } public class Message : EntityData { public string UserId { get; set; } public string Text { get; set; } public virtual ICollection<Tag> Tags { get; set; } } } We can seed some information into this database to simulate relationships with a standard initializer: public class MobileServiceInitializer : CreateDatabaseIfNotExists<MobileServiceContext> { protected override void Seed(MobileServiceContext context) { List<Tag> tags = new List<Tag> { new Tag { Id = NewGuid(), TagName = \"Tag-1\" }, new Tag { Id = NewGuid(), TagName = \"Tag-2\" }, new Tag { Id = NewGuid(), TagName = \"Tag-3\" } }; context.Set<Tag>().AddRange(tags); List<Message> messages = new List<Message> { new Message { Id = NewGuid(), Text = \"Message-1\", Tags = tags }, new Message { Id = NewGuid(), Text = \"message-2\", Tags = new List<Tag>() } }; context.Set<Message>().AddRange(messages); base.Seed(context); } private string NewGuid() { return Guid.NewGuid().ToString(); } } If we use Postman to do a GET /tables/Message, we get the following: Note that the tags field is not even produced. The Azure Mobile Apps Server SDK depends on multiple frameworks. It uses Entity Framework for data access, for instance. It also uses the standard Microsoft OData server to translate OData queries into results. This has a side effect that the rules of the Microsoft OData server must be followed. Specifically, this means that collections, such as the list of tags, will not be produced unless you explicitly expand them. You can do this on the URL by using the $expand parameter: GET /tables/Message?$expand=tags This will result in the following output: Note that the tags are fully expanded and embedded in the object. This has some serious consequences that must be considered: The table becomes read-only through the controller. Inserts, Updates and Deletes must have special code handling. The data takes up more space on the client as the tags will be duplicated whenever used. You cannot use offline sync since the data model is no longer flat. In addition to these three points, you need to be able to add the $expand property to the request. This is not done by the SDK. You can automatically add the $expand property on the server-side by using an attribute for the purpose: using System; using System.Linq; using System.Web.Http.Controllers; using System.Web.Http.Filters; namespace Chapter3.Extensions { [AttributeUsage(AttributeTargets.Method, AllowMultiple = true)] public class ExpandPropertyAttribute : ActionFilterAttribute { string propertyName; public ExpandPropertyAttribute(string propertyName) { this.propertyName = propertyName; } public override void OnActionExecuting(HttpActionContext actionContext) { base.OnActionExecuting(actionContext); var uriBuilder = new UriBuilder(actionContext.Request.RequestUri); var queryParams = uriBuilder.Query.TrimStart('?').Split(new[] { '&' }, StringSplitOptions.RemoveEmptyEntries).ToList(); int expandIndex = -1; for (var i = 0; i < queryParams.Count; i++) { if (queryParams[i].StartsWith(\"$expand\", StringComparison.Ordinal)) { expandIndex = i; break; } } if (expandIndex < 0) { queryParams.Add(\"$expand=\" + this.propertyName); } else { queryParams[expandIndex] = queryParams[expandIndex] + \",\" + propertyName; } uriBuilder.Query = string.Join(\"&\", queryParams); actionContext.Request.RequestUri = uriBuilder.Uri; } } } This is used in the controller: public class MessageController : TableController<Message> { private MobileServiceContext context; protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); context = new MobileServiceContext(); DomainManager = new EntityDomainManager<Message>(context, Request); } public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; // GET tables/Message [ExpandProperty(\"tags\")] public IQueryable<Message> GetAllMessage() { return Query(); //return Query().OwnedByFriends(context.Friends, UserId); } // GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959 [ExpandProperty(\"tags\")] public SingleResult<Message> GetMessage(string id) { return new SingleResult<Message>(Lookup(id).Queryable); //return new SingleResult<Message>(Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId)); } // POST tables/Message public async Task<IHttpActionResult> PostMessageAsync(Message item) { item.UserId = UserId; Message current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } } Given this is not recommended, what are the alternatives? There are two alternatives. The first alternative is to use a third table that joins the Messages and Tags together in a loose manner: using Microsoft.Azure.Mobile.Server; using System.Collections.Generic; namespace TaskList.Models { public class Tag : EntityData { public string TagName { get; set; } } public class Message : EntityData { public string UserId { get; set; } public string Text { get; set; } } public class MessageTag : EntityData { public string MessageId { get; set; } public string TagId { get; set; } } } We can now create a MessageTagController to retrieve the information. This data can be stored offline since the model is now flat. Obtaining the list of tags for a message is a single LINQ query: var tags = from tag in tagTable let tl = (from mt in messageTags where mt.MessageId == messageId select mt.TagId) where tl.Contains(tag.Id) select tag; You may recognize this LINQ query as it is very similar to the LINQ query used for the friends filter. A similar query can be used to find the messages associated with a tag: var msgs = from message in messageTable let tl = (from mt in messageTags where mt.TagId == tagId select mt.MessageId) where tl.Contains(message.Id) orderby message.CreatedAtRoute select message; The second alternative is to use an alternative DomainManager that implements the relationships for you. We will cover this in the next section.","title":"Relationships"},{"location":"chapter3/relationships/#relationships","text":"One of the biggest benefits to using a SQL database over a NoSQL store is relationships between entities. Relationships provide the ability to normalize the data, allowing you to store the minimal amount of data for a specific use case on the mobile device. This reduces bandwidth usage and memory usage on the device. Relationships are a good thing. Unfortunately, relationships between tables are hard when one is working within an offline context. This is primarily caused by the need for resilience. Because we can do many updates to the tables on the offline client, the transactions that update the tables need to be co-ordinated. This is practically impossible in an offline context where one of the goals in bandwidth performance. Azure Mobile Apps, when used in an offline context, has an operations table. As you do each operation against a table, an entry is made in the operations table. The operations table is then replayed in order to the mobile backend to effect changes in the remote database. However, this also has the effect that we do not have transactions to allow the updating of multiple tables within the database at the same time. Each record in each table is updated individually. The push process that offline sync uses has major ramifications for how relationships between tables work. Specifically, only 1-way relationships will work in an offline sync world. 1-Way Relationships You can define relationships in Entity Framework with or without a virtual back-reference. Relationships with the virtual back-reference are known as 2-way relationships (because you can get back to the original model). Relationships with only a forward reference (and no knowledge of the original model) are said to have a 1-way relationship. A database model with only 1-way relationships can generally be represented with a tree structure. Let's take a quick example. We've been using the \"task list\" scenario for our testing thus far. Let's say that each task could be assigned a tag from a list of tags. We can use a 1-way 1:1 relationship between the tasks and the tags. To do that, we would store the Id of the tag in the task model. If, however, we could attach many tags to a single task, that would be a 1:Many relationship.","title":"Relationships"},{"location":"chapter3/relationships/#11-relationships","text":"Let's take a look at our task list example, from the perspective of the models on the server side: using Microsoft.Azure.Mobile.Server; using System.ComponentModel.DataAnnotations.Schema; namespace ComplexTypes.DataObjects { public class Tag : EntityData { public string TagName { get; set; } } public class TodoItem : EntityData { public string Text { get; set; } public bool Complete { get; set; } #region Relationships public string TagId { get; set; } [ForeignKey(\"TagId\")] public Tag Tag { get; set; } #endregion } } 1:1 relationships are defined using a foreign key in the SQL database. We can use Entity Framework to define the foreign key relationship easily. In this case, our TodoItem model will, have a TagId that contains the Id field of the tag. I also created a pair of table controllers for these models in the normal manner. Finally, I've created some records using the Seed() method within the App_Start\\Startup.MobileApp.cs file to give us some test data. If we take a look at records through Postman, we will get the following: Note that the first item has a reference to a tag, by virtue of the TagId. The second item does not have a tag assigned, so the value of TagId is null. When we implement the client, we are going to download these tables independently. The linkage and relationships between the tables is lost when going from the backend to the offline client. We have to link them together ourselves. This is why the \"1-way\" relationship is necessary. In a 2-way relationship, a tag and task would have to be created at the same time as part of an SQL transaction. In a 1-way relationship, the tag can be created THEN the task that has the relationship is created Tip When you think of all the mobile applications you own, you will realize that 1-way relationships are the normal state of affairs. Very few data models for mobile apps actually require a two-way relationship. When you are developing the mobile client, the Tag is removed from the model: using TaskList.Helpers; namespace TaskList.Models { public class Tag : TableData { public string TagName { get; set; } } public class TodoItem : TableData { public string Text { get; set; } public bool Complete { get; set; } public string TagId { get; set; } } } One can easily retrieve the tag information with a LINQ query on the Tag table: var tag = tagTable.FirstOrDefault(tag => tag.Id.Equals(task.TagId)).Value; There are a couple of rules you must follow within your client code: You need to ensure that you create a tag before associating that tag with a task. You need to store the TagId with the task, not the Tag object (as you would normally do within Entity Framework). The former is generally handled for you. If you InsertAsync a tag in offline mode, it will be placed into the operations queue prior to anything that uses it. Since the operations queue is processed in order, the tag will be sent to the backend prior to any record updates that would use it.","title":"1:1 Relationships"},{"location":"chapter3/relationships/#1many-relationships","text":"What if we had a list of messages and wanted to assign more than one tag to each record? In this case, we would need a 1:Many relationship. Setting up a 1:Many relationship on the backend again relies on Entity Framework syntax: using Microsoft.Azure.Mobile.Server; using System.Collections.Generic; namespace TaskList.Models { public class Tag : EntityData { public string TagName { get; set; } } public class Message : EntityData { public string UserId { get; set; } public string Text { get; set; } public virtual ICollection<Tag> Tags { get; set; } } } We can seed some information into this database to simulate relationships with a standard initializer: public class MobileServiceInitializer : CreateDatabaseIfNotExists<MobileServiceContext> { protected override void Seed(MobileServiceContext context) { List<Tag> tags = new List<Tag> { new Tag { Id = NewGuid(), TagName = \"Tag-1\" }, new Tag { Id = NewGuid(), TagName = \"Tag-2\" }, new Tag { Id = NewGuid(), TagName = \"Tag-3\" } }; context.Set<Tag>().AddRange(tags); List<Message> messages = new List<Message> { new Message { Id = NewGuid(), Text = \"Message-1\", Tags = tags }, new Message { Id = NewGuid(), Text = \"message-2\", Tags = new List<Tag>() } }; context.Set<Message>().AddRange(messages); base.Seed(context); } private string NewGuid() { return Guid.NewGuid().ToString(); } } If we use Postman to do a GET /tables/Message, we get the following: Note that the tags field is not even produced. The Azure Mobile Apps Server SDK depends on multiple frameworks. It uses Entity Framework for data access, for instance. It also uses the standard Microsoft OData server to translate OData queries into results. This has a side effect that the rules of the Microsoft OData server must be followed. Specifically, this means that collections, such as the list of tags, will not be produced unless you explicitly expand them. You can do this on the URL by using the $expand parameter: GET /tables/Message?$expand=tags This will result in the following output: Note that the tags are fully expanded and embedded in the object. This has some serious consequences that must be considered: The table becomes read-only through the controller. Inserts, Updates and Deletes must have special code handling. The data takes up more space on the client as the tags will be duplicated whenever used. You cannot use offline sync since the data model is no longer flat. In addition to these three points, you need to be able to add the $expand property to the request. This is not done by the SDK. You can automatically add the $expand property on the server-side by using an attribute for the purpose: using System; using System.Linq; using System.Web.Http.Controllers; using System.Web.Http.Filters; namespace Chapter3.Extensions { [AttributeUsage(AttributeTargets.Method, AllowMultiple = true)] public class ExpandPropertyAttribute : ActionFilterAttribute { string propertyName; public ExpandPropertyAttribute(string propertyName) { this.propertyName = propertyName; } public override void OnActionExecuting(HttpActionContext actionContext) { base.OnActionExecuting(actionContext); var uriBuilder = new UriBuilder(actionContext.Request.RequestUri); var queryParams = uriBuilder.Query.TrimStart('?').Split(new[] { '&' }, StringSplitOptions.RemoveEmptyEntries).ToList(); int expandIndex = -1; for (var i = 0; i < queryParams.Count; i++) { if (queryParams[i].StartsWith(\"$expand\", StringComparison.Ordinal)) { expandIndex = i; break; } } if (expandIndex < 0) { queryParams.Add(\"$expand=\" + this.propertyName); } else { queryParams[expandIndex] = queryParams[expandIndex] + \",\" + propertyName; } uriBuilder.Query = string.Join(\"&\", queryParams); actionContext.Request.RequestUri = uriBuilder.Uri; } } } This is used in the controller: public class MessageController : TableController<Message> { private MobileServiceContext context; protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); context = new MobileServiceContext(); DomainManager = new EntityDomainManager<Message>(context, Request); } public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; // GET tables/Message [ExpandProperty(\"tags\")] public IQueryable<Message> GetAllMessage() { return Query(); //return Query().OwnedByFriends(context.Friends, UserId); } // GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959 [ExpandProperty(\"tags\")] public SingleResult<Message> GetMessage(string id) { return new SingleResult<Message>(Lookup(id).Queryable); //return new SingleResult<Message>(Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId)); } // POST tables/Message public async Task<IHttpActionResult> PostMessageAsync(Message item) { item.UserId = UserId; Message current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } } Given this is not recommended, what are the alternatives? There are two alternatives. The first alternative is to use a third table that joins the Messages and Tags together in a loose manner: using Microsoft.Azure.Mobile.Server; using System.Collections.Generic; namespace TaskList.Models { public class Tag : EntityData { public string TagName { get; set; } } public class Message : EntityData { public string UserId { get; set; } public string Text { get; set; } } public class MessageTag : EntityData { public string MessageId { get; set; } public string TagId { get; set; } } } We can now create a MessageTagController to retrieve the information. This data can be stored offline since the model is now flat. Obtaining the list of tags for a message is a single LINQ query: var tags = from tag in tagTable let tl = (from mt in messageTags where mt.MessageId == messageId select mt.TagId) where tl.Contains(tag.Id) select tag; You may recognize this LINQ query as it is very similar to the LINQ query used for the friends filter. A similar query can be used to find the messages associated with a tag: var msgs = from message in messageTable let tl = (from mt in messageTags where mt.TagId == tagId select mt.MessageId) where tl.Contains(message.Id) orderby message.CreatedAtRoute select message; The second alternative is to use an alternative DomainManager that implements the relationships for you. We will cover this in the next section.","title":"1:Many Relationships"},{"location":"chapter3/server/","text":"Implementing Table Controllers \u00b6 The central component for providing a table endpoint on the Azure App Service side of things occurs in your backend project. You must implement a Table Controller. This is a specialized version of an ApiController that has some similarities with an ODataController. However, it has its own base class and the Azure Mobile Apps SDK simplifies the process of making them. Implementing Code First Migrations \u00b6 Before we can get started with adding another table controller, we have to deal with modifications to our database schema. The default code for Azure Mobile Apps will deploy the configured schema only if the database is empty . If the database is not empty, you have to do extra work. This extra work involves updating the schema of your database. There are two methods of doing this. The first we will consider is Code First Migrations. With code first migrations, we construct a code file whenever we need to change the database. Don't worry - it's done for us. Later on, we will consider Database First. With database first, we adjust the database schema manually, then write C# models to reflect that change. Nothing causes more headaches in an Azure Mobile Apps backend than code-first migrations . A code-first migration is simply a set of configuration commands that updates the database to support the new database model. If you try to publish this application, you will see an InvalidOperationException and your service will likely crash. If you manage to trap the error, it will say The model backing the 'MobileServiceContext' context has changed since the database was created. Consider using Code First Migrations to update the database. That's fairly specific and is common to all applications based on Entity Framework that use code-first models. Tip There is an alternative here called Database First Models . In this alternative, you create the database first then create the models to match. However, Azure Mobile Apps requires specific configuration of mobile tables that you will need to take care of. See the section on using existing SQL tables later on for details. The first step is to enable migrations. Go to View -> Other Windows -> Package Manager Console . This window will generally appear in the same place as your Output and Error List windows at the bottom of the screen. Type enable-migrations in it: Check out the Migrations folder that has just been created to hold the code-first migrations. An initial Configuration.cs object will be added to this folder that describes the basic configuration. We also need to create an Initial migration that represents the current state of the database. We can do this with the command add-migration Initial . Tip If you have multiple projects in your solution, you may need to add -Project _projectname_ . For example, add-migration -project Chapter3 Initial . This applies to all the migration commands you execute in the Package Manager Console. The initial migration creates a few files in the Migrations folder that represent the current state of affairs for the database. These easily recognized by a combination of the current date and the name of the migration. Code First Migrations can be applied manually or automatically. I personally prefer the automatic method. To implement automated Code First Migrations, edit the App_Start\\Startup.MobileApp.cs file: public static void ConfigureMobileApp(IAppBuilder app) { var httpConfig = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(httpConfig); // Automatic Code First Migrations var migrator = new DbMigrator(new Migrations.Configuration()); migrator.Update(); app.UseWebApi(httpConfig); } We have replaced the DbInitializer() (which was the method that created the database for us) with the automatic database migrator code. There is one issue that will cause some problems. We are no longer using a database initializer. The database initializer is the single line of code we just replaced that creates the database tables with the appropriate triggers. This means that the special system columns will no longer be wired up to update their values automatically. We can fix that by configuring the SqlGenerator in Migrations\\Configuration.cs : public Configuration() { AutomaticMigrationsEnabled = false; SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator()); } Since we are not using a database initializer, our seed data has also gone. You may as well delete the MobileServiceInitializer class in the App_Start\\Startup.MobileApp.cs as it isn't doing anything any more. You can move the seed data to the Migrations\\Configuration.cs file though: namespace Chapter3.Migrations { using System; using System.Collections.Generic; using System.Data.Entity.Migrations; using DataObjects; using Microsoft.Azure.Mobile.Server.Tables; internal sealed class Configuration : DbMigrationsConfiguration<Chapter3.Models.MobileServiceContext> { public Configuration() { AutomaticMigrationsEnabled = false; SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator()); } protected override void Seed(Chapter3.Models.MobileServiceContext context) { List<TodoItem> todoItems = new List<TodoItem> { new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"First item\", Complete = false }, new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"Second item\", Complete = false } }; foreach (TodoItem todoItem in todoItems) { context.Set<TodoItem>().Add(todoItem); } base.Seed(context); } } } The contents of the Seed method are a cut-and-paste from the MobileServiceInitializer version. If all goes well, you can clear your database (or delete it and re-create it), then publish this project and do a GET of the /tables/todoitem endpoint. You should still see your data. You should do a little more investigation however. Open the database in the SQL Server Object Explorer . Expand the Tables node. Right-click on the dbo.__MigrationHistory table and select View Data . There should be one row with a name that indicates it is the initial migration. The final step is to apply the migration to the local system. In the Package Manager Console, enter the command update-database to apply the existing migration. Adding a SQL Table Controller \u00b6 Before we can use a table controller, we need to add one. This has three steps: Create a Data Transfer Object (DTO) Create a Table Controller Create a Code-First Migration Creating a Data Transfer Object \u00b6 A Data Transfer Object (or DTO as it is commonly known) is the wire representation of the model for your table. It must inherit from a concrete implementation of ITableData . The Azure Mobile Apps SDK includes EntityData for this reason. EntityData is a concrete implementation that works with Entity Framework. Warn You can't just assume EntityData will work with other data stores. There are Entity Framework specific attributes decorating the properties for EntityData that will likely be different for other stores. The default Azure Mobile Apps project that is supplied with the Azure SDK provides a folder for storing DTOs called DataObjects . Let's create a DTO by right-clicking on the DataObjects folder, then using Add -> Class... : using System; using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class Example : EntityData { public string StringField { get; set; } public int IntField { get; set; } public double DoubleField { get; set; } public DateTimeOffset DateTimeField { get; set; } } } Tip Don't call your model SomethingDTO . This ends up as a /tables/somethingDTO endpoint and a SomethingDTO table in your database. Just call it Something . All the names will then line up properly. I've included several field types, including a complex type. The basic requirement for a field is that it must be serialized into a simple JSON type during transfer between the server and the mobile client. Complex types (that is, any type that can be serialized to an object or array) will always require special handling and may not be able to be used at all. Create a Table Controller \u00b6 Visual Studio for Windows (with the Azure SDK) provides some help in creating a table controller. Right-click on the Controllers node and select Add -> Controller... . Visual Studio provides scaffolding for a new table controller. Select it and then click on Add . The dialog asks for the model (which is actually a DTO) and the data context (which is already created). Once you select the model, the controller name is created for you. You can change it if you like, but it's common practice to not do this. Once the scaffolding is finished, you can look at your newly created table controller. We do want to do one change. We want to enable soft delete so that our table controller supports offline sync scenarios properly. To do this, go into the Initialize() method and change the constructor of the EntityDomainManager . The completed table controller looks like this: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Microsoft.Azure.Mobile.Server; using Chapter3.DataObjects; using Chapter3.Models; namespace Chapter3.Controllers { public class ExampleController : TableController<Example> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<Example>(context, Request, enableSoftDelete: true); } // GET tables/Example public IQueryable<Example> GetAllExample() { return Query(); } // GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<Example> GetExample(string id) { return Lookup(id); } // PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<Example> PatchExample(string id, Delta<Example> patch) { return UpdateAsync(id, patch); } // POST tables/Example public async Task<IHttpActionResult> PostExample(Example item) { Example current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteExample(string id) { return DeleteAsync(id); } } } Creating a Code-First Migration \u00b6 You must add a code first migration to update the database when it is published. Use the add-migration command in the Package Manager Console. The add-migration command will request a name - it just has to be unique, but it's a good idea to make the name descriptive: You should also use update-database to apply the change to the local database (if any): Once this is done, you can publish the project. Right-click on the project and select Publish... . Once the project is published, you should be able to send a query to the /tables/example endpoint using Postman and get an empty array. You should also be able to insert, update and delete entities as you can with the TodoItem table. Handling Publish Failures \u00b6 Sometimes, the publish fails. It seems that whenever I start with code-first migrations, my publish fails. I get a nice error screen, but no actual error. At least half the time, the problem is not my code-first migration, but something else. For instance, one of the things I tend to do is update my NuGet packages. This inevitably breaks something. Fortunately, once the error message is known, it's generally trivial to correct the error. You can turn custom error messages off (and thus expose the original error message) by editing the Web.config file. Locate the <system.web> section and add the <customErrors mode=\"Off\"/> line: <system.web> <httpRuntime targetFramework=\"4.6\" /> <compilation debug=\"true\" targetFramework=\"4.6\" /> <customErrors mode=\"Off\" /> </system.web> Then republish your project and the response from the server is much more informative. Turning on Diagnostic Logs \u00b6 You can log all the SQL statements that Entity Framework executes on your behalf by adding a Database Log. Edit the Models\\MobileServiceContext.cs file: public class MobileServiceContext : DbContext { private const string connectionStringName = \"Name=MS_TableConnectionString\"; public MobileServiceContext() : base(connectionStringName) { Database.Log = s => WriteLog(s); } public void WriteLog(string msg) { System.Diagnostics.Debug.WriteLine(msg); } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); } public DbSet<DataObjects.TodoItem> TodoItems { get; set; } public DbSet<DataObjects.Example> Examples { get; set; } } You have to use a real method. System.Diagnostics.Debug is removed from the context when DEBUG is not defined, so you can't just use it directly. Using an interim method works around that problem. Azure App Service captures the output from the console and places it into the log viewer for you. To turn on diagnostic logging: Log in to the Azure Portal . Click on App Services then your App Service. Find Diagnostic Logs in the list of settings (you can use the search box). Turn on Application Logging (Filesystem) with a level of Verbose . Click on Save . To view the diagnostic logs in the portal, find Log Stream in the list of settings (again, you can use the search box). You can also get the diagnostic logs within Visual Studio. Open the Server Explorer . Expand Azure , App Service , your resource group. Right-click on the your App Service and select View Streaming Logs . Using an existing SQL Table \u00b6 There are times when a \"Database First\" approach is needed. If you are trying to expose an existing SQL database table, for example, you want to use a \"Database First\" approach. You may also like the separation of the database table from the mobile system columns. Warn The Database First and Code First approaches to Entity Framework are mutually exclusive. You need to decide which one you want to use and stick with it. To use \"Database First\", you first set up the database. Then you create a Model and update the DbContext object. For example, our Example model from before can be represented by the following database schema: CREATE TABLE [dbo].[Examples] ( -- This must be a string suitable for a GUID [Id] NVARCHAR (128) NOT NULL, -- These are the system properties [Version] ROWVERSION NOT NULL, [CreatedAt] DATETIMEOFFSET (7) NOT NULL, [UpdatedAt] DATETIMEOFFSET (7) NULL, [Deleted] BIT NOT NULL -- These are the properties of our DTO not included in EntityFramework [StringField] NVARCHAR (MAX) NULL, [IntField] INT NOT NULL, [DoubleField] FLOAT (53) NOT NULL, [DateTimeField] DATETIMEOFFSET (7) NOT NULL, ); CREATE CLUSTERED INDEX [IX_CreatedAt] ON [dbo].[Examples]([CreatedAt] ASC); ALTER TABLE [dbo].[Examples] ADD CONSTRAINT [PK_dbo.Examples] PRIMARY KEY NONCLUSTERED ([Id] ASC); CREATE TRIGGER [TR_dbo_Examples_InsertUpdateDelete] ON [dbo].[Examples] AFTER INSERT, UPDATE, DELETE AS BEGIN UPDATE [dbo].[Examples] SET [dbo].[Examples].[UpdatedAt] = CONVERT(DATETIMEOFFSET, SYSUTCDATETIME()) FROM INSERTED WHERE inserted.[Id] = [dbo].[Examples].[Id] END; The system properties are added to the schema. We can (and do) use a trigger to update the UpdatedAt column when the data is updated. Placing this logic within the SQL Server schema definition means we can use this SQL table outside of the mobile context and it will still work for the mobile application. Info We use the CreatedAt field to create a clustered index. In older versions of SQL Server, this was required to increase performance on the table as a whole. It may not be required now and may be removed in future versions of Azure Mobile Apps. If you can update an existing table to match this schema, then you should do so. Note that the Id field is not set by default. If you want to set a default, set it to NEWID() : ALTER TABLE [dbo].[Examples] ALTER COLUMN [Id] SET DEFAULT CONVERT(NVARCHAR(128), NEWID()); Once you have set up the database and created the models, you must also turn off the database initializer. This is done in App_Start\\Startup.MobileApp.cs : public static void ConfigureMobileApp(IAppBuilder app) { var httpConfig = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(httpConfig); // Automatic Code First Migrations // var migrator = new DbMigrator(new Migrations.Configuration()); // migrator.Update(); // Database First Database.SetInitializer<MobileDbContext>(null); app.UseWebApi(httpConfig); } There are a lot of times when the existing table is not suitable for this sort of schema adjustment. The primary reason will be that the existing table has an auto-incrementing integer Id column and you can't change that. Quite often, I see developers needing to integrate the SQL schema of an existing application like a Customer Relationship Management system with this constraint, for example. Let's take an example. Suppose you have a table called [dbo].[TodoItems]. This is fairly basic and defined like this: CREATE TABLE [dbo].[TodoItems] ( [id] BIGINT NOT NULL IDENTITY(1,1) PRIMARY KEY, [UserId] NVARCHAR(255) NOT NULL, [Title] NVARCHAR(255) NOT NULL, [Complete] BIT ); This is the sort of SQL table that is very common within existing web applications, for instance. We have an auto-incrementing id column and some fields. It isn't complex (no relationships, for example), so we don't have to worry about referential integrity of the table. Tip I recommend placing the \"mobile views\" that we will discuss below in a separate schema (for example, [mobile] ) so that they don't interfere with your existing database schema. Let's take a look at creating a VIEW of the data that is \"mobile ready\". This is an Entity Relationship Diagram of what we are going to do: we are going to create a separate table for the system properties, called [mobile].[SysProps_TodoItems] . This will hold the five fields that Azure Mobile Apps requires, plus a reference to the TodoItem id: CREATE TABLE [mobile].[SysProps_TodoItems] ( [Id] NVARCHAR(128) CONSTRAINT [DF_todoitem_id] DEFAULT (CONVERT([NVARCHAR](255),NEWID(),(0))) NOT NULL, [CreatedAt] DATETIMEOFFSET(7) CONSTRAINT [DF_todoitem_createdAt] DEFAULT (CONVERT([DATETIMEOFFSET](7),SYSUTCDATETIME(),(0))) NOT NULL, [UpdatedAt] DATETIMEOFFSET(7) NULL, [Version] ROWVERSION NOT NULL, [Deleted] BIT DEFAULT ((0)) NOT NULL, [Item_Id] BIGINT NOT NULL PRIMARY KEY NONCLUSTERED ([id] ASC) ); Then we will create a SQL VIEW that maps to this: CREATE VIEW [mobile].[TodoItems] AS SELECT [mobile].[SysProps_TodoItems].[Id], [mobile].[SysProps_TodoItems].[CreatedAt], [mobile].[SysProps_TodoItems].[UpdatedAt], [mobile].[SysProps_TodoItems].[Version], [mobile].[SysProps_TodoItems].[Deleted], [mobile].[SysProps_TodoItems].[Item_Id], [dbo].[TodoItems].[UserId], [dbo].[TodoItems].[Title], [dbo].[TodoItems].[Complete] FROM [dbo].[TodoItems], [mobile].[SysProps_TodoItems] WHERE [dbo].[TodoItems].[id] = [mobile].[SysProps_TodoItems].[Item_Id]; This view is still only a combination of the two tables. I want specific logic so that when a row is changed (inserted, updated or deleted) in the [dbo].[TodoItems] table, a similar change is made to the [mobile].[SysProps_TodoItems] table. This is achieved through the use of triggers: CREATE TRIGGER [dbo].[TRG_TodoItem_Insert] ON [dbo].[TodoItems] AFTER INSERT AS BEGIN INSERT INTO [mobile].[SysProps_TodoItems] ([Item_Id], [UpdatedAt]) SELECT inserted.id, CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME()) FROM inserted END GO CREATE TRIGGER [dbo].[TRG_TodoItem_Update] ON [dbo].[TodoItems] AFTER UPDATE AS BEGIN UPDATE [mobile].[SysProps_TodoItems] SET [UpdatedAt] = CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME()) FROM INSERTED WHERE INSERTED.id = [mobile].[SysProps_TodoItems].[Item_Id] END GO CREATE TRIGGER [dbo].[TRG_TodoItem_Delete] ON [dbo].[TodoItems] AFTER DELETE AS BEGIN DELETE FROM [mobile].[SysProps_TodoItems] WHERE [Item_Id] IN (select deleted.id from deleted) END GO Tip You may want to set the Deleted flag to 1 (or true) in the Delete trigger. This will enable soft delete on the system properties table, ensuring that mobile clients remove the row from their offline cache. Similarly, when changes are made by the mobile backend to the VIEW, we need to propagate those changes to the underlying tables. This is also done with triggers: CREATE TRIGGER [mobile].[TRG_Mobile_TodoItem_Insert] ON [mobile].[TodoItems] INSTEAD OF INSERT AS BEGIN DECLARE @userid AS NVARCHAR(255) SELECT @userid = inserted.UserId FROM inserted DECLARE @title AS NVARCHAR(255) SELECT @title = inserted.Title FROM inserted DECLARE @complete AS BIT SELECT @complete = inserted.Complete FROM inserted INSERT INTO [dbo].[TodoItems] ([UserId], [Title], [Complete]) VALUES (@userid, @title, @complete) IF UPDATE(Id) BEGIN DECLARE @itemid AS BIGINT SELECT @itemid = @@identity DECLARE @id AS NVARCHAR(255) SELECT @id = inserted.Id FROM inserted UPDATE [mobile].[SysProps_TodoItems] SET [Id] = @id WHERE [item_id] = @itemid END END; GO CREATE TRIGGER [mobile].[TRG_Mobile_TodoItem_Update] ON [mobile].[TodoItems] INSTEAD OF UPDATE AS BEGIN DECLARE @id AS NVARCHAR(255) SELECT @id = inserted.id FROM inserted DECLARE @itemid AS BIGINT SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id IF UPDATE(UserId) BEGIN DECLARE @userid AS NVARCHAR(255) SELECT @userid = inserted.UserId FROM inserted UPDATE [dbo].[TodoItems] SET [UserId] = @userid WHERE [id] = @itemid END IF UPDATE(Title) BEGIN DECLARE @title AS NVARCHAR(255) SELECT @title = inserted.Title FROM inserted UPDATE [dbo].[TodoItems] SET [Title] = @title WHERE [id] = @itemid END IF UPDATE(Complete) BEGIN DECLARE @complete AS BIT SELECT @complete = inserted.Complete FROM inserted UPDATE [dbo].[TodoItems] SET [Complete] = @complete WHERE [id] = @itemid END IF UPDATE(deleted) BEGIN DECLARE @deleted AS BIT SELECT @deleted = inserted.deleted FROM inserted UPDATE [mobile].[SysProps_TodoItems] SET [deleted] = @deleted WHERE [item_id] = @itemid END END GO CREATE TRIGGER [mobile].[TRG_Mobile_TodoItem_Delete] ON [mobile].[TodoItems] INSTEAD OF DELETE AS BEGIN DECLARE @id AS NVARCHAR(255) SELECT @id = deleted.id FROM deleted DECLARE @itemid AS BIGINT SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id DELETE FROM [dbo].[TodoItems] WHERE [id] = @itemid DELETE FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id END GO A standard SQL view is read-only. We made the view read/write by using triggers to trap the call and replace it with two calls instead. Warn This version does not support soft delete. If you wish to support soft delete, set the Deleted flag in the `[mobile].[SysProps_TodoItems] table instead of deleting the row. Changing the Mobile Schema \u00b6 We stored our mobile representation of the table in a different schema. Azure Mobile Apps looks for tables (and views) in the [dbo] schema by default. There are two places you can modify the schema that Entity Framework uses to access the database. The first is by changing the default schema that Entity Framework uses. This will affect all the tables that we are exposing via a table controller. This is done in the Models\\MobileDbContext.cs class: protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.HasDefaultSchema(\"mobile\"); modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString() ) ); } We can also do this on the model as an annotation. For example, here is the TodoItem model suitably adjusted: using Microsoft.Azure.Mobile.Server; using Newtonsoft.Json; using System.ComponentModel.DataAnnotations.Schema; namespace Backend.DataObjects { [Table(\"TodoItems\", Schema=\"mobile\")] public class TodoItem : EntityData { public string UserId { get; set; } public string Title { get; set; } public bool Complete { get; set; } } } Info Entity Framework adds an s onto the end of the model to create the table name. If you have a model named TodoItem , the table is called TodoItems in the database. You can use this same annotation to adjust the table name if it is not to your liking. Best Practices \u00b6 Here is a summary of what I consider best practices for table controllers: Use Code First when you are in a green-field database situation. Use Database First when you have to integrate an existing database. Only expose the data that you need for the mobile client. Set up Automatic Code First Migrations as early as possible. Think about what happens to existing clients when you update the schema. This last point is an important one. Let's say you have a v1 mobile client of your awesome mobile application. This works with v1 mobile backend. You've set up the schema and ensured that everything works. Then you want to release v2 mobile client. It has a new feature and needs some extra data. So you update to v2 mobile backend that provides that data. You need to ensure that the schema changes provided by the v2 mobile backend are optional. In other words, the v1 mobile client can continue to work against the v2 mobile backend. You should test this case. Users do not upgrade instantly. In fact, many users don't upgrade at all. At every release of your app, you are going to have to ensure that ALL versions of your mobile client work against your mobile backend.","title":"Implementing Table Controllers"},{"location":"chapter3/server/#implementing-table-controllers","text":"The central component for providing a table endpoint on the Azure App Service side of things occurs in your backend project. You must implement a Table Controller. This is a specialized version of an ApiController that has some similarities with an ODataController. However, it has its own base class and the Azure Mobile Apps SDK simplifies the process of making them.","title":"Implementing Table Controllers"},{"location":"chapter3/server/#implementing-code-first-migrations","text":"Before we can get started with adding another table controller, we have to deal with modifications to our database schema. The default code for Azure Mobile Apps will deploy the configured schema only if the database is empty . If the database is not empty, you have to do extra work. This extra work involves updating the schema of your database. There are two methods of doing this. The first we will consider is Code First Migrations. With code first migrations, we construct a code file whenever we need to change the database. Don't worry - it's done for us. Later on, we will consider Database First. With database first, we adjust the database schema manually, then write C# models to reflect that change. Nothing causes more headaches in an Azure Mobile Apps backend than code-first migrations . A code-first migration is simply a set of configuration commands that updates the database to support the new database model. If you try to publish this application, you will see an InvalidOperationException and your service will likely crash. If you manage to trap the error, it will say The model backing the 'MobileServiceContext' context has changed since the database was created. Consider using Code First Migrations to update the database. That's fairly specific and is common to all applications based on Entity Framework that use code-first models. Tip There is an alternative here called Database First Models . In this alternative, you create the database first then create the models to match. However, Azure Mobile Apps requires specific configuration of mobile tables that you will need to take care of. See the section on using existing SQL tables later on for details. The first step is to enable migrations. Go to View -> Other Windows -> Package Manager Console . This window will generally appear in the same place as your Output and Error List windows at the bottom of the screen. Type enable-migrations in it: Check out the Migrations folder that has just been created to hold the code-first migrations. An initial Configuration.cs object will be added to this folder that describes the basic configuration. We also need to create an Initial migration that represents the current state of the database. We can do this with the command add-migration Initial . Tip If you have multiple projects in your solution, you may need to add -Project _projectname_ . For example, add-migration -project Chapter3 Initial . This applies to all the migration commands you execute in the Package Manager Console. The initial migration creates a few files in the Migrations folder that represent the current state of affairs for the database. These easily recognized by a combination of the current date and the name of the migration. Code First Migrations can be applied manually or automatically. I personally prefer the automatic method. To implement automated Code First Migrations, edit the App_Start\\Startup.MobileApp.cs file: public static void ConfigureMobileApp(IAppBuilder app) { var httpConfig = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(httpConfig); // Automatic Code First Migrations var migrator = new DbMigrator(new Migrations.Configuration()); migrator.Update(); app.UseWebApi(httpConfig); } We have replaced the DbInitializer() (which was the method that created the database for us) with the automatic database migrator code. There is one issue that will cause some problems. We are no longer using a database initializer. The database initializer is the single line of code we just replaced that creates the database tables with the appropriate triggers. This means that the special system columns will no longer be wired up to update their values automatically. We can fix that by configuring the SqlGenerator in Migrations\\Configuration.cs : public Configuration() { AutomaticMigrationsEnabled = false; SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator()); } Since we are not using a database initializer, our seed data has also gone. You may as well delete the MobileServiceInitializer class in the App_Start\\Startup.MobileApp.cs as it isn't doing anything any more. You can move the seed data to the Migrations\\Configuration.cs file though: namespace Chapter3.Migrations { using System; using System.Collections.Generic; using System.Data.Entity.Migrations; using DataObjects; using Microsoft.Azure.Mobile.Server.Tables; internal sealed class Configuration : DbMigrationsConfiguration<Chapter3.Models.MobileServiceContext> { public Configuration() { AutomaticMigrationsEnabled = false; SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator()); } protected override void Seed(Chapter3.Models.MobileServiceContext context) { List<TodoItem> todoItems = new List<TodoItem> { new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"First item\", Complete = false }, new TodoItem { Id = Guid.NewGuid().ToString(), Text = \"Second item\", Complete = false } }; foreach (TodoItem todoItem in todoItems) { context.Set<TodoItem>().Add(todoItem); } base.Seed(context); } } } The contents of the Seed method are a cut-and-paste from the MobileServiceInitializer version. If all goes well, you can clear your database (or delete it and re-create it), then publish this project and do a GET of the /tables/todoitem endpoint. You should still see your data. You should do a little more investigation however. Open the database in the SQL Server Object Explorer . Expand the Tables node. Right-click on the dbo.__MigrationHistory table and select View Data . There should be one row with a name that indicates it is the initial migration. The final step is to apply the migration to the local system. In the Package Manager Console, enter the command update-database to apply the existing migration.","title":"Implementing Code First Migrations"},{"location":"chapter3/server/#adding-a-sql-table-controller","text":"Before we can use a table controller, we need to add one. This has three steps: Create a Data Transfer Object (DTO) Create a Table Controller Create a Code-First Migration","title":"Adding a SQL Table Controller"},{"location":"chapter3/server/#creating-a-data-transfer-object","text":"A Data Transfer Object (or DTO as it is commonly known) is the wire representation of the model for your table. It must inherit from a concrete implementation of ITableData . The Azure Mobile Apps SDK includes EntityData for this reason. EntityData is a concrete implementation that works with Entity Framework. Warn You can't just assume EntityData will work with other data stores. There are Entity Framework specific attributes decorating the properties for EntityData that will likely be different for other stores. The default Azure Mobile Apps project that is supplied with the Azure SDK provides a folder for storing DTOs called DataObjects . Let's create a DTO by right-clicking on the DataObjects folder, then using Add -> Class... : using System; using Microsoft.Azure.Mobile.Server; namespace Chapter3.DataObjects { public class Example : EntityData { public string StringField { get; set; } public int IntField { get; set; } public double DoubleField { get; set; } public DateTimeOffset DateTimeField { get; set; } } } Tip Don't call your model SomethingDTO . This ends up as a /tables/somethingDTO endpoint and a SomethingDTO table in your database. Just call it Something . All the names will then line up properly. I've included several field types, including a complex type. The basic requirement for a field is that it must be serialized into a simple JSON type during transfer between the server and the mobile client. Complex types (that is, any type that can be serialized to an object or array) will always require special handling and may not be able to be used at all.","title":"Creating a Data Transfer Object"},{"location":"chapter3/server/#create-a-table-controller","text":"Visual Studio for Windows (with the Azure SDK) provides some help in creating a table controller. Right-click on the Controllers node and select Add -> Controller... . Visual Studio provides scaffolding for a new table controller. Select it and then click on Add . The dialog asks for the model (which is actually a DTO) and the data context (which is already created). Once you select the model, the controller name is created for you. You can change it if you like, but it's common practice to not do this. Once the scaffolding is finished, you can look at your newly created table controller. We do want to do one change. We want to enable soft delete so that our table controller supports offline sync scenarios properly. To do this, go into the Initialize() method and change the constructor of the EntityDomainManager . The completed table controller looks like this: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Microsoft.Azure.Mobile.Server; using Chapter3.DataObjects; using Chapter3.Models; namespace Chapter3.Controllers { public class ExampleController : TableController<Example> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<Example>(context, Request, enableSoftDelete: true); } // GET tables/Example public IQueryable<Example> GetAllExample() { return Query(); } // GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<Example> GetExample(string id) { return Lookup(id); } // PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<Example> PatchExample(string id, Delta<Example> patch) { return UpdateAsync(id, patch); } // POST tables/Example public async Task<IHttpActionResult> PostExample(Example item) { Example current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteExample(string id) { return DeleteAsync(id); } } }","title":"Create a Table Controller"},{"location":"chapter3/server/#creating-a-code-first-migration","text":"You must add a code first migration to update the database when it is published. Use the add-migration command in the Package Manager Console. The add-migration command will request a name - it just has to be unique, but it's a good idea to make the name descriptive: You should also use update-database to apply the change to the local database (if any): Once this is done, you can publish the project. Right-click on the project and select Publish... . Once the project is published, you should be able to send a query to the /tables/example endpoint using Postman and get an empty array. You should also be able to insert, update and delete entities as you can with the TodoItem table.","title":"Creating a Code-First Migration"},{"location":"chapter3/server/#handling-publish-failures","text":"Sometimes, the publish fails. It seems that whenever I start with code-first migrations, my publish fails. I get a nice error screen, but no actual error. At least half the time, the problem is not my code-first migration, but something else. For instance, one of the things I tend to do is update my NuGet packages. This inevitably breaks something. Fortunately, once the error message is known, it's generally trivial to correct the error. You can turn custom error messages off (and thus expose the original error message) by editing the Web.config file. Locate the <system.web> section and add the <customErrors mode=\"Off\"/> line: <system.web> <httpRuntime targetFramework=\"4.6\" /> <compilation debug=\"true\" targetFramework=\"4.6\" /> <customErrors mode=\"Off\" /> </system.web> Then republish your project and the response from the server is much more informative.","title":"Handling Publish Failures"},{"location":"chapter3/server/#turning-on-diagnostic-logs","text":"You can log all the SQL statements that Entity Framework executes on your behalf by adding a Database Log. Edit the Models\\MobileServiceContext.cs file: public class MobileServiceContext : DbContext { private const string connectionStringName = \"Name=MS_TableConnectionString\"; public MobileServiceContext() : base(connectionStringName) { Database.Log = s => WriteLog(s); } public void WriteLog(string msg) { System.Diagnostics.Debug.WriteLine(msg); } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString())); } public DbSet<DataObjects.TodoItem> TodoItems { get; set; } public DbSet<DataObjects.Example> Examples { get; set; } } You have to use a real method. System.Diagnostics.Debug is removed from the context when DEBUG is not defined, so you can't just use it directly. Using an interim method works around that problem. Azure App Service captures the output from the console and places it into the log viewer for you. To turn on diagnostic logging: Log in to the Azure Portal . Click on App Services then your App Service. Find Diagnostic Logs in the list of settings (you can use the search box). Turn on Application Logging (Filesystem) with a level of Verbose . Click on Save . To view the diagnostic logs in the portal, find Log Stream in the list of settings (again, you can use the search box). You can also get the diagnostic logs within Visual Studio. Open the Server Explorer . Expand Azure , App Service , your resource group. Right-click on the your App Service and select View Streaming Logs .","title":"Turning on Diagnostic Logs"},{"location":"chapter3/server/#using-an-existing-sql-table","text":"There are times when a \"Database First\" approach is needed. If you are trying to expose an existing SQL database table, for example, you want to use a \"Database First\" approach. You may also like the separation of the database table from the mobile system columns. Warn The Database First and Code First approaches to Entity Framework are mutually exclusive. You need to decide which one you want to use and stick with it. To use \"Database First\", you first set up the database. Then you create a Model and update the DbContext object. For example, our Example model from before can be represented by the following database schema: CREATE TABLE [dbo].[Examples] ( -- This must be a string suitable for a GUID [Id] NVARCHAR (128) NOT NULL, -- These are the system properties [Version] ROWVERSION NOT NULL, [CreatedAt] DATETIMEOFFSET (7) NOT NULL, [UpdatedAt] DATETIMEOFFSET (7) NULL, [Deleted] BIT NOT NULL -- These are the properties of our DTO not included in EntityFramework [StringField] NVARCHAR (MAX) NULL, [IntField] INT NOT NULL, [DoubleField] FLOAT (53) NOT NULL, [DateTimeField] DATETIMEOFFSET (7) NOT NULL, ); CREATE CLUSTERED INDEX [IX_CreatedAt] ON [dbo].[Examples]([CreatedAt] ASC); ALTER TABLE [dbo].[Examples] ADD CONSTRAINT [PK_dbo.Examples] PRIMARY KEY NONCLUSTERED ([Id] ASC); CREATE TRIGGER [TR_dbo_Examples_InsertUpdateDelete] ON [dbo].[Examples] AFTER INSERT, UPDATE, DELETE AS BEGIN UPDATE [dbo].[Examples] SET [dbo].[Examples].[UpdatedAt] = CONVERT(DATETIMEOFFSET, SYSUTCDATETIME()) FROM INSERTED WHERE inserted.[Id] = [dbo].[Examples].[Id] END; The system properties are added to the schema. We can (and do) use a trigger to update the UpdatedAt column when the data is updated. Placing this logic within the SQL Server schema definition means we can use this SQL table outside of the mobile context and it will still work for the mobile application. Info We use the CreatedAt field to create a clustered index. In older versions of SQL Server, this was required to increase performance on the table as a whole. It may not be required now and may be removed in future versions of Azure Mobile Apps. If you can update an existing table to match this schema, then you should do so. Note that the Id field is not set by default. If you want to set a default, set it to NEWID() : ALTER TABLE [dbo].[Examples] ALTER COLUMN [Id] SET DEFAULT CONVERT(NVARCHAR(128), NEWID()); Once you have set up the database and created the models, you must also turn off the database initializer. This is done in App_Start\\Startup.MobileApp.cs : public static void ConfigureMobileApp(IAppBuilder app) { var httpConfig = new HttpConfiguration(); var mobileConfig = new MobileAppConfiguration(); mobileConfig .AddTablesWithEntityFramework() .ApplyTo(httpConfig); // Automatic Code First Migrations // var migrator = new DbMigrator(new Migrations.Configuration()); // migrator.Update(); // Database First Database.SetInitializer<MobileDbContext>(null); app.UseWebApi(httpConfig); } There are a lot of times when the existing table is not suitable for this sort of schema adjustment. The primary reason will be that the existing table has an auto-incrementing integer Id column and you can't change that. Quite often, I see developers needing to integrate the SQL schema of an existing application like a Customer Relationship Management system with this constraint, for example. Let's take an example. Suppose you have a table called [dbo].[TodoItems]. This is fairly basic and defined like this: CREATE TABLE [dbo].[TodoItems] ( [id] BIGINT NOT NULL IDENTITY(1,1) PRIMARY KEY, [UserId] NVARCHAR(255) NOT NULL, [Title] NVARCHAR(255) NOT NULL, [Complete] BIT ); This is the sort of SQL table that is very common within existing web applications, for instance. We have an auto-incrementing id column and some fields. It isn't complex (no relationships, for example), so we don't have to worry about referential integrity of the table. Tip I recommend placing the \"mobile views\" that we will discuss below in a separate schema (for example, [mobile] ) so that they don't interfere with your existing database schema. Let's take a look at creating a VIEW of the data that is \"mobile ready\". This is an Entity Relationship Diagram of what we are going to do: we are going to create a separate table for the system properties, called [mobile].[SysProps_TodoItems] . This will hold the five fields that Azure Mobile Apps requires, plus a reference to the TodoItem id: CREATE TABLE [mobile].[SysProps_TodoItems] ( [Id] NVARCHAR(128) CONSTRAINT [DF_todoitem_id] DEFAULT (CONVERT([NVARCHAR](255),NEWID(),(0))) NOT NULL, [CreatedAt] DATETIMEOFFSET(7) CONSTRAINT [DF_todoitem_createdAt] DEFAULT (CONVERT([DATETIMEOFFSET](7),SYSUTCDATETIME(),(0))) NOT NULL, [UpdatedAt] DATETIMEOFFSET(7) NULL, [Version] ROWVERSION NOT NULL, [Deleted] BIT DEFAULT ((0)) NOT NULL, [Item_Id] BIGINT NOT NULL PRIMARY KEY NONCLUSTERED ([id] ASC) ); Then we will create a SQL VIEW that maps to this: CREATE VIEW [mobile].[TodoItems] AS SELECT [mobile].[SysProps_TodoItems].[Id], [mobile].[SysProps_TodoItems].[CreatedAt], [mobile].[SysProps_TodoItems].[UpdatedAt], [mobile].[SysProps_TodoItems].[Version], [mobile].[SysProps_TodoItems].[Deleted], [mobile].[SysProps_TodoItems].[Item_Id], [dbo].[TodoItems].[UserId], [dbo].[TodoItems].[Title], [dbo].[TodoItems].[Complete] FROM [dbo].[TodoItems], [mobile].[SysProps_TodoItems] WHERE [dbo].[TodoItems].[id] = [mobile].[SysProps_TodoItems].[Item_Id]; This view is still only a combination of the two tables. I want specific logic so that when a row is changed (inserted, updated or deleted) in the [dbo].[TodoItems] table, a similar change is made to the [mobile].[SysProps_TodoItems] table. This is achieved through the use of triggers: CREATE TRIGGER [dbo].[TRG_TodoItem_Insert] ON [dbo].[TodoItems] AFTER INSERT AS BEGIN INSERT INTO [mobile].[SysProps_TodoItems] ([Item_Id], [UpdatedAt]) SELECT inserted.id, CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME()) FROM inserted END GO CREATE TRIGGER [dbo].[TRG_TodoItem_Update] ON [dbo].[TodoItems] AFTER UPDATE AS BEGIN UPDATE [mobile].[SysProps_TodoItems] SET [UpdatedAt] = CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME()) FROM INSERTED WHERE INSERTED.id = [mobile].[SysProps_TodoItems].[Item_Id] END GO CREATE TRIGGER [dbo].[TRG_TodoItem_Delete] ON [dbo].[TodoItems] AFTER DELETE AS BEGIN DELETE FROM [mobile].[SysProps_TodoItems] WHERE [Item_Id] IN (select deleted.id from deleted) END GO Tip You may want to set the Deleted flag to 1 (or true) in the Delete trigger. This will enable soft delete on the system properties table, ensuring that mobile clients remove the row from their offline cache. Similarly, when changes are made by the mobile backend to the VIEW, we need to propagate those changes to the underlying tables. This is also done with triggers: CREATE TRIGGER [mobile].[TRG_Mobile_TodoItem_Insert] ON [mobile].[TodoItems] INSTEAD OF INSERT AS BEGIN DECLARE @userid AS NVARCHAR(255) SELECT @userid = inserted.UserId FROM inserted DECLARE @title AS NVARCHAR(255) SELECT @title = inserted.Title FROM inserted DECLARE @complete AS BIT SELECT @complete = inserted.Complete FROM inserted INSERT INTO [dbo].[TodoItems] ([UserId], [Title], [Complete]) VALUES (@userid, @title, @complete) IF UPDATE(Id) BEGIN DECLARE @itemid AS BIGINT SELECT @itemid = @@identity DECLARE @id AS NVARCHAR(255) SELECT @id = inserted.Id FROM inserted UPDATE [mobile].[SysProps_TodoItems] SET [Id] = @id WHERE [item_id] = @itemid END END; GO CREATE TRIGGER [mobile].[TRG_Mobile_TodoItem_Update] ON [mobile].[TodoItems] INSTEAD OF UPDATE AS BEGIN DECLARE @id AS NVARCHAR(255) SELECT @id = inserted.id FROM inserted DECLARE @itemid AS BIGINT SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id IF UPDATE(UserId) BEGIN DECLARE @userid AS NVARCHAR(255) SELECT @userid = inserted.UserId FROM inserted UPDATE [dbo].[TodoItems] SET [UserId] = @userid WHERE [id] = @itemid END IF UPDATE(Title) BEGIN DECLARE @title AS NVARCHAR(255) SELECT @title = inserted.Title FROM inserted UPDATE [dbo].[TodoItems] SET [Title] = @title WHERE [id] = @itemid END IF UPDATE(Complete) BEGIN DECLARE @complete AS BIT SELECT @complete = inserted.Complete FROM inserted UPDATE [dbo].[TodoItems] SET [Complete] = @complete WHERE [id] = @itemid END IF UPDATE(deleted) BEGIN DECLARE @deleted AS BIT SELECT @deleted = inserted.deleted FROM inserted UPDATE [mobile].[SysProps_TodoItems] SET [deleted] = @deleted WHERE [item_id] = @itemid END END GO CREATE TRIGGER [mobile].[TRG_Mobile_TodoItem_Delete] ON [mobile].[TodoItems] INSTEAD OF DELETE AS BEGIN DECLARE @id AS NVARCHAR(255) SELECT @id = deleted.id FROM deleted DECLARE @itemid AS BIGINT SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id DELETE FROM [dbo].[TodoItems] WHERE [id] = @itemid DELETE FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id END GO A standard SQL view is read-only. We made the view read/write by using triggers to trap the call and replace it with two calls instead. Warn This version does not support soft delete. If you wish to support soft delete, set the Deleted flag in the `[mobile].[SysProps_TodoItems] table instead of deleting the row.","title":"Using an existing SQL Table"},{"location":"chapter3/server/#changing-the-mobile-schema","text":"We stored our mobile representation of the table in a different schema. Azure Mobile Apps looks for tables (and views) in the [dbo] schema by default. There are two places you can modify the schema that Entity Framework uses to access the database. The first is by changing the default schema that Entity Framework uses. This will affect all the tables that we are exposing via a table controller. This is done in the Models\\MobileDbContext.cs class: protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.HasDefaultSchema(\"mobile\"); modelBuilder.Conventions.Add( new AttributeToColumnAnnotationConvention<TableColumnAttribute, string>( \"ServiceTableColumn\", (property, attributes) => attributes.Single().ColumnType.ToString() ) ); } We can also do this on the model as an annotation. For example, here is the TodoItem model suitably adjusted: using Microsoft.Azure.Mobile.Server; using Newtonsoft.Json; using System.ComponentModel.DataAnnotations.Schema; namespace Backend.DataObjects { [Table(\"TodoItems\", Schema=\"mobile\")] public class TodoItem : EntityData { public string UserId { get; set; } public string Title { get; set; } public bool Complete { get; set; } } } Info Entity Framework adds an s onto the end of the model to create the table name. If you have a model named TodoItem , the table is called TodoItems in the database. You can use this same annotation to adjust the table name if it is not to your liking.","title":"Changing the Mobile Schema"},{"location":"chapter3/server/#best-practices","text":"Here is a summary of what I consider best practices for table controllers: Use Code First when you are in a green-field database situation. Use Database First when you have to integrate an existing database. Only expose the data that you need for the mobile client. Set up Automatic Code First Migrations as early as possible. Think about what happens to existing clients when you update the schema. This last point is an important one. Let's say you have a v1 mobile client of your awesome mobile application. This works with v1 mobile backend. You've set up the schema and ensured that everything works. Then you want to release v2 mobile client. It has a new feature and needs some extra data. So you update to v2 mobile backend that provides that data. You need to ensure that the schema changes provided by the v2 mobile backend are optional. In other words, the v1 mobile client can continue to work against the v2 mobile backend. You should test this case. Users do not upgrade instantly. In fact, many users don't upgrade at all. At every release of your app, you are going to have to ensure that ALL versions of your mobile client work against your mobile backend.","title":"Best Practices"},{"location":"chapter4/custom/","text":"Custom HTTP Endpoints \u00b6 Azure Mobile Apps makes it really easy to develop basic APIs that can be used in mobile clients. Most custom APIs can be simply invoked. Azure Mobile Apps takes care of most of the scaffolding for you. The server SDK will: Ensure the ZUMO-API-VERSION is present and valid. Handle serialization and deserialization of JSON. Ensure the API is given an appropriate URL. All the custom APIs will appear under the /api endpoint. For example, if you created a controller called FooController , it would be invoked by sending messages to /api/Foo . This is case-insensitive, so you could also reference this API as /api/foo . Configuring Custom APIs \u00b6 Before anything happens, you must add the MapApiControllers() method to the MobileAppConfiguration() call. This is done in the ConfigureMobileApp() method in App_Start\\Startup.MobileApp.cs file: new MobileAppConfiguration() .AddTablesWithEntityFramework() /* /tables endpoints */ .MapApiControllers() /* /api endpoints */ .ApplyTo(config); The MapApiControllers() extension method does the actual work of looking for custom APIs and mapping them onto the /api endpoint. Creating a Basic Custom API \u00b6 You might remember that the original Azure Mobile Apps project within Visual Studio comes with a sample custom API called the ValuesController . This controller did not do anything useful. Let's re-create it from scratch. Right-click the Controllers node in your backend project and use Add -> Controller... . Select the Azure Mobile Apps Custom Controller , then click Add . Enter the name for the controller, for example, ValuesController . Click Add . The new controller will be scaffolded for you. When you are done, it looks like this: using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; namespace Backend.Controllers { [MobileAppController] public class ValuesController : ApiController { // GET api/Default public string Get() { return \"Hello from custom controller!\"; } } } If you have not done anything that requires a backend, then you can press F5 to run the backend and use Postman to interact with your new custom API: We still have to submit the ZUMO-API-VERSION header for this to work. Whatever my method returns will be returned as JSON. This one is not exactly exciting. One of the things I do quite often is provide a configuration endpoint called /api/config which returns a JSON object that I can use to configure the mobile client. using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; using System.Collections.Generic; using System; namespace Backend.Controllers { [MobileAppController] public class ConfigController : ApiController { private ConfigViewModel configuration; public ConfigController() { Dictionary<string, ProviderInformation> providers = new Dictionary<string, ProviderInformation>(); AddToProviders(providers, \"aad\", \"MOBILE_AAD_CLIENT_ID\"); AddToProviders(providers, \"facebook\", \"MOBILE_FB_CLIENT_ID\"); AddToProviders(providers, \"google\", \"MOBILE_GOOGLE_CLIENT_ID\"); AddToProviders(providers, \"microsoftaccount\", \"MOBILE_MSA_CLIENT_ID\"); AddToProviders(providers, \"twitter\", \"MOBILE_TWITTER_CLIENT_ID\"); configuration = new ConfigViewModel { AuthProviders = providers }; } private void AddToProviders(Dictionary<string, ProviderInformation> providers, string provider, string envVar) { string envVal = Environment.GetEnvironmentVariable(envVar); if (envVal != null && envVal?.Length > 0) { providers.Add(provider, new ProviderInformation { ClientId = envVal }); } } [HttpGet] public ConfigViewModel Get() { return configuration; } } public class ProviderInformation { public string ClientId { get; set; } } public class ConfigViewModel { public Dictionary<string, ProviderInformation> AuthProviders { get; set; } } } The constructor produces a ConfigViewModel for me. This describes the configuration object I want to send. In this case, I want to send the client ID for each authentication provider. If the authentication provider is not configured, then the client ID is not sent. I use the application settings to determine what is configured. The primary idea behind this is to integrate all the client flows within my mobile client. When the user wishes to log in, they are presented with a menu of options and can pick which social provider they wish to use. The client-flow authentication libraries may use different client IDs than the ones that are configured into the authentication service. For example, AAD uses two client IDs - one for server-flow and one for client-flow. As a result, this controller uses Application Settings (which appear as environment variables to the backend) to set the client IDs. The result of calling this API from Postman looks like this: Warn Only expose information that you would normally and reasonably embed in a mobile client. Never transmit secrets this way. It is insecure and can put your entire authentication system at risk of hijack. You can read this information using the Azure Mobile Apps Client SDK once you have a client reference, using the same model classes: var configuration = await client.InvokeApiAsync<ConfigViewModel>(\"config\", HttpMethod.Get, null); You must specify a class that deserializes the JSON that is produced by your API. If you use the same classes, that is practically guaranteed. The other methods in call are the HTTP Method (GET, POST, PATCH, DELETE, etc.) and the query parameters. Handling Parameters to a Custom API \u00b6 The /api/config endpoint didn't require any information that is extra. Sometimes, we need to provide extra information so that the right thing can be produced. For example, consider the case of uploading or downloading a file to Azure Storage. We may want to provide some extra information - the filename of the file we want to upload and the permissions for the file. Uploading and downloading files is discussed more fully later in the book and offers a fuller example of this concept. To illustrate the concept clearly, let's create an API that adds two numbers together. We would call this API through HTTP like this: GET /api/addition?first=1&second=2 . The first number gets added to the second number and we will return the result. If the first or the second number doesn't exist, we want to produce a 400 Bad Request response rather than crashing the server. Here is the code: using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; using System.Net; namespace Backend.Controllers { [MobileAppController] public class AdditionController : ApiController { // GET api/Addition public ResultViewModel Get(int? first, int? second) { if (first == null || second == null) { throw new HttpResponseException(HttpStatusCode.BadRequest); } ResultViewModel results = new ResultViewModel { First = first.GetValueOrDefault(), Second = second.GetValueOrDefault() }; results.Result = results.First + results.Second; return results; } } public class ResultViewModel { public int First { get; set; } public int Second { get; set; } public int Result { get; set; } } } If you try this out, you will notice something rather odd. Try doing the following URL: /api/addition?first=1&second=2 . You will note it works as expected. However, if you try doing the following URL: /api/addition?first=1 , then you will note that you get a 404 Not Found . This makes the API easy to write because you don't have to worry about your code receiving bad input (most of the time). However, you may not get the API surface that you want. In this case, I want to return a 400 Bad Request instead of the normal 404 response. I have to do a lot more work to support this case: public class AdditionController : ApiController { // GET api/Addition public ResultViewModel Get() { int? first = GetParameter(Request, \"first\"), second = GetParameter(Request, \"second\"); ResultViewModel results = new ResultViewModel { First = first.GetValueOrDefault(), Second = second.GetValueOrDefault() }; results.Result = results.First + results.Second; return results; } private int? GetParameter(HttpRequestMessage request, string name) { var queryParams = request.GetQueryNameValuePairs().Where(kv => kv.Key == name).ToList(); if (queryParams.Count == 0) { throw new HttpResponseException(HttpStatusCode.BadRequest); } int rv; if (!Int32.TryParse(queryParams[0].Value, out rv)) { throw new HttpResponseException(HttpStatusCode.BadRequest); } return rv; } } When our Get() routine does not take parameters, no particular pattern is required. We can use the Request object to access the parameters we send. In this case, the GetParameter() routine checks to see if there is a named parameter and converts it to an integer. If the named parameter is not there or it is not numeric, then a Bad Request response is sent. Handling POST Requests \u00b6 GET and DELETE requests take parameters on the URI. These can be dealt with via the automatic conversion to method parameters or they can be handled via LINQ queries on the request object, as we observed in the prior section. POST requests, by contrast, allow you to submit a JSON body for processing. This is useful when we want to submit multiple JSON objects for processing. For example, one of the common requirements we have is for transactions. When we want to submit two objects that are joined by a foreign key, we can submit them both and construct a transaction in the backend with Entity Framework. Let's take an example. We want to produce a music trakcing mobile app. When we add an album to our music database, we also want to add the tracks for that database. This can be modeled with code first Entity Framework: public class Track : EntityData { public Track() {} public string Title { get; set; } public int Length { get; set; } public virtual Album Album { get; set; } } public class Album : EntityData { public Album() { Tracks = new List<Track>(); } public string Title { get; set; } public virtual ICollection<Track> Tracks { get; set; } } This will generate a foreign key relationship in the tables of our database. However, as we learned in chapter 3 , relationships are hard to implement and come with some serious caveats. We could decouple the tracks from the albums, but let's instead try to insert the data for an album all in one go. We do this by submitting the JSON for an Album to a custom controller: [MobileAppController] public class AlbumCustomController : ApiController { MobileServiceContext context; public AlbumCustomController() : base() { context = new MobileServiceContext(); } [HttpPost] public async Task<AlbumCustomResponse> PostAsync([FromBody] Album newAlbum) { // Use a transaction to update the database using (DbContextTransaction transaction = context.Database.BeginTransaction()) { try { context.Albums.Add(newAlbum); await context.SaveChangesAsync(); transaction.Commit(); } catch (Exception ex) { transaction.Rollback(); } } // Now generate whatever output we want. AlbumCustomResponse response = new AlbumCustomResponse { Status = 200 }; return response; } } In this particular scenario, we would construct an album object on the mobile client side that directly corresponded to the JSON we want to push to the database, including the track information. Let's take a look at a typical call: var response = client.InvokeApiAsync<Album,AlbumCustomResponse>( \"AlbumController\", // The name of the API newAlbum, // The body of the POST HttpMethod.Post, // The HTTP Method null, // Request Headers null); // Parameters There are quite a few signatures for the InvokeApiAsync<>() method. This one sends an Album object as JSON as the body of the request, and returns an AlbumCustomResponse , decoding the response as it goes. The Downside of Custom APIs \u00b6 Given that we can do transaction processing within a custom API, one might be forgiven for wondering why we don't use custom APIs for processing data within our normalized SQL schema. The problem, of course, is that custom APIs (including WebAPIs) can only be executed while connected to the Internet. When the mobile device is offline, the custom API cannot be executed, thus breaking our offline model. Offline just doesn't mix with SQL relationships. When I first encountered this, I thought a great idea may be to instantiate a queue, much like the Operations Queue that is used within the offline sync process. When I want to queue up a transaction, I insert it into my own operations queue. My early research used a light-weight queuing mechanism ( DotNetMQ , for those interested) to implement the queue. Transactions were inserted into the queue at the appropriate time in the client. During the sync process, the transactions were pushed, then the tables were pulled. The problem is that the tables in the offline sync were not maintained until a pull, resulting in \"old data\". If I updated the data in the offline cache as well, I produced many conflicts and inconsistent data within the offline cache. In the end, I concluded that it was better to loosely couple the tables that were being used (as we discussed in chapter 3 ).","title":"Custom HTTP Endpoints"},{"location":"chapter4/custom/#custom-http-endpoints","text":"Azure Mobile Apps makes it really easy to develop basic APIs that can be used in mobile clients. Most custom APIs can be simply invoked. Azure Mobile Apps takes care of most of the scaffolding for you. The server SDK will: Ensure the ZUMO-API-VERSION is present and valid. Handle serialization and deserialization of JSON. Ensure the API is given an appropriate URL. All the custom APIs will appear under the /api endpoint. For example, if you created a controller called FooController , it would be invoked by sending messages to /api/Foo . This is case-insensitive, so you could also reference this API as /api/foo .","title":"Custom HTTP Endpoints"},{"location":"chapter4/custom/#configuring-custom-apis","text":"Before anything happens, you must add the MapApiControllers() method to the MobileAppConfiguration() call. This is done in the ConfigureMobileApp() method in App_Start\\Startup.MobileApp.cs file: new MobileAppConfiguration() .AddTablesWithEntityFramework() /* /tables endpoints */ .MapApiControllers() /* /api endpoints */ .ApplyTo(config); The MapApiControllers() extension method does the actual work of looking for custom APIs and mapping them onto the /api endpoint.","title":"Configuring Custom APIs"},{"location":"chapter4/custom/#creating-a-basic-custom-api","text":"You might remember that the original Azure Mobile Apps project within Visual Studio comes with a sample custom API called the ValuesController . This controller did not do anything useful. Let's re-create it from scratch. Right-click the Controllers node in your backend project and use Add -> Controller... . Select the Azure Mobile Apps Custom Controller , then click Add . Enter the name for the controller, for example, ValuesController . Click Add . The new controller will be scaffolded for you. When you are done, it looks like this: using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; namespace Backend.Controllers { [MobileAppController] public class ValuesController : ApiController { // GET api/Default public string Get() { return \"Hello from custom controller!\"; } } } If you have not done anything that requires a backend, then you can press F5 to run the backend and use Postman to interact with your new custom API: We still have to submit the ZUMO-API-VERSION header for this to work. Whatever my method returns will be returned as JSON. This one is not exactly exciting. One of the things I do quite often is provide a configuration endpoint called /api/config which returns a JSON object that I can use to configure the mobile client. using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; using System.Collections.Generic; using System; namespace Backend.Controllers { [MobileAppController] public class ConfigController : ApiController { private ConfigViewModel configuration; public ConfigController() { Dictionary<string, ProviderInformation> providers = new Dictionary<string, ProviderInformation>(); AddToProviders(providers, \"aad\", \"MOBILE_AAD_CLIENT_ID\"); AddToProviders(providers, \"facebook\", \"MOBILE_FB_CLIENT_ID\"); AddToProviders(providers, \"google\", \"MOBILE_GOOGLE_CLIENT_ID\"); AddToProviders(providers, \"microsoftaccount\", \"MOBILE_MSA_CLIENT_ID\"); AddToProviders(providers, \"twitter\", \"MOBILE_TWITTER_CLIENT_ID\"); configuration = new ConfigViewModel { AuthProviders = providers }; } private void AddToProviders(Dictionary<string, ProviderInformation> providers, string provider, string envVar) { string envVal = Environment.GetEnvironmentVariable(envVar); if (envVal != null && envVal?.Length > 0) { providers.Add(provider, new ProviderInformation { ClientId = envVal }); } } [HttpGet] public ConfigViewModel Get() { return configuration; } } public class ProviderInformation { public string ClientId { get; set; } } public class ConfigViewModel { public Dictionary<string, ProviderInformation> AuthProviders { get; set; } } } The constructor produces a ConfigViewModel for me. This describes the configuration object I want to send. In this case, I want to send the client ID for each authentication provider. If the authentication provider is not configured, then the client ID is not sent. I use the application settings to determine what is configured. The primary idea behind this is to integrate all the client flows within my mobile client. When the user wishes to log in, they are presented with a menu of options and can pick which social provider they wish to use. The client-flow authentication libraries may use different client IDs than the ones that are configured into the authentication service. For example, AAD uses two client IDs - one for server-flow and one for client-flow. As a result, this controller uses Application Settings (which appear as environment variables to the backend) to set the client IDs. The result of calling this API from Postman looks like this: Warn Only expose information that you would normally and reasonably embed in a mobile client. Never transmit secrets this way. It is insecure and can put your entire authentication system at risk of hijack. You can read this information using the Azure Mobile Apps Client SDK once you have a client reference, using the same model classes: var configuration = await client.InvokeApiAsync<ConfigViewModel>(\"config\", HttpMethod.Get, null); You must specify a class that deserializes the JSON that is produced by your API. If you use the same classes, that is practically guaranteed. The other methods in call are the HTTP Method (GET, POST, PATCH, DELETE, etc.) and the query parameters.","title":"Creating a Basic Custom API"},{"location":"chapter4/custom/#handling-parameters-to-a-custom-api","text":"The /api/config endpoint didn't require any information that is extra. Sometimes, we need to provide extra information so that the right thing can be produced. For example, consider the case of uploading or downloading a file to Azure Storage. We may want to provide some extra information - the filename of the file we want to upload and the permissions for the file. Uploading and downloading files is discussed more fully later in the book and offers a fuller example of this concept. To illustrate the concept clearly, let's create an API that adds two numbers together. We would call this API through HTTP like this: GET /api/addition?first=1&second=2 . The first number gets added to the second number and we will return the result. If the first or the second number doesn't exist, we want to produce a 400 Bad Request response rather than crashing the server. Here is the code: using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; using System.Net; namespace Backend.Controllers { [MobileAppController] public class AdditionController : ApiController { // GET api/Addition public ResultViewModel Get(int? first, int? second) { if (first == null || second == null) { throw new HttpResponseException(HttpStatusCode.BadRequest); } ResultViewModel results = new ResultViewModel { First = first.GetValueOrDefault(), Second = second.GetValueOrDefault() }; results.Result = results.First + results.Second; return results; } } public class ResultViewModel { public int First { get; set; } public int Second { get; set; } public int Result { get; set; } } } If you try this out, you will notice something rather odd. Try doing the following URL: /api/addition?first=1&second=2 . You will note it works as expected. However, if you try doing the following URL: /api/addition?first=1 , then you will note that you get a 404 Not Found . This makes the API easy to write because you don't have to worry about your code receiving bad input (most of the time). However, you may not get the API surface that you want. In this case, I want to return a 400 Bad Request instead of the normal 404 response. I have to do a lot more work to support this case: public class AdditionController : ApiController { // GET api/Addition public ResultViewModel Get() { int? first = GetParameter(Request, \"first\"), second = GetParameter(Request, \"second\"); ResultViewModel results = new ResultViewModel { First = first.GetValueOrDefault(), Second = second.GetValueOrDefault() }; results.Result = results.First + results.Second; return results; } private int? GetParameter(HttpRequestMessage request, string name) { var queryParams = request.GetQueryNameValuePairs().Where(kv => kv.Key == name).ToList(); if (queryParams.Count == 0) { throw new HttpResponseException(HttpStatusCode.BadRequest); } int rv; if (!Int32.TryParse(queryParams[0].Value, out rv)) { throw new HttpResponseException(HttpStatusCode.BadRequest); } return rv; } } When our Get() routine does not take parameters, no particular pattern is required. We can use the Request object to access the parameters we send. In this case, the GetParameter() routine checks to see if there is a named parameter and converts it to an integer. If the named parameter is not there or it is not numeric, then a Bad Request response is sent.","title":"Handling Parameters to a Custom API"},{"location":"chapter4/custom/#handling-post-requests","text":"GET and DELETE requests take parameters on the URI. These can be dealt with via the automatic conversion to method parameters or they can be handled via LINQ queries on the request object, as we observed in the prior section. POST requests, by contrast, allow you to submit a JSON body for processing. This is useful when we want to submit multiple JSON objects for processing. For example, one of the common requirements we have is for transactions. When we want to submit two objects that are joined by a foreign key, we can submit them both and construct a transaction in the backend with Entity Framework. Let's take an example. We want to produce a music trakcing mobile app. When we add an album to our music database, we also want to add the tracks for that database. This can be modeled with code first Entity Framework: public class Track : EntityData { public Track() {} public string Title { get; set; } public int Length { get; set; } public virtual Album Album { get; set; } } public class Album : EntityData { public Album() { Tracks = new List<Track>(); } public string Title { get; set; } public virtual ICollection<Track> Tracks { get; set; } } This will generate a foreign key relationship in the tables of our database. However, as we learned in chapter 3 , relationships are hard to implement and come with some serious caveats. We could decouple the tracks from the albums, but let's instead try to insert the data for an album all in one go. We do this by submitting the JSON for an Album to a custom controller: [MobileAppController] public class AlbumCustomController : ApiController { MobileServiceContext context; public AlbumCustomController() : base() { context = new MobileServiceContext(); } [HttpPost] public async Task<AlbumCustomResponse> PostAsync([FromBody] Album newAlbum) { // Use a transaction to update the database using (DbContextTransaction transaction = context.Database.BeginTransaction()) { try { context.Albums.Add(newAlbum); await context.SaveChangesAsync(); transaction.Commit(); } catch (Exception ex) { transaction.Rollback(); } } // Now generate whatever output we want. AlbumCustomResponse response = new AlbumCustomResponse { Status = 200 }; return response; } } In this particular scenario, we would construct an album object on the mobile client side that directly corresponded to the JSON we want to push to the database, including the track information. Let's take a look at a typical call: var response = client.InvokeApiAsync<Album,AlbumCustomResponse>( \"AlbumController\", // The name of the API newAlbum, // The body of the POST HttpMethod.Post, // The HTTP Method null, // Request Headers null); // Parameters There are quite a few signatures for the InvokeApiAsync<>() method. This one sends an Album object as JSON as the body of the request, and returns an AlbumCustomResponse , decoding the response as it goes.","title":"Handling POST Requests"},{"location":"chapter4/custom/#the-downside-of-custom-apis","text":"Given that we can do transaction processing within a custom API, one might be forgiven for wondering why we don't use custom APIs for processing data within our normalized SQL schema. The problem, of course, is that custom APIs (including WebAPIs) can only be executed while connected to the Internet. When the mobile device is offline, the custom API cannot be executed, thus breaking our offline model. Offline just doesn't mix with SQL relationships. When I first encountered this, I thought a great idea may be to instantiate a queue, much like the Operations Queue that is used within the offline sync process. When I want to queue up a transaction, I insert it into my own operations queue. My early research used a light-weight queuing mechanism ( DotNetMQ , for those interested) to implement the queue. Transactions were inserted into the queue at the appropriate time in the client. During the sync process, the transactions were pushed, then the tables were pulled. The problem is that the tables in the offline sync were not maintained until a pull, resulting in \"old data\". If I updated the data in the offline cache as well, I produced many conflicts and inconsistent data within the offline cache. In the end, I concluded that it was better to loosely couple the tables that were being used (as we discussed in chapter 3 ).","title":"The Downside of Custom APIs"},{"location":"chapter4/functions/","text":"WebJobs run in the context of your App Service Plan, which means they inherit the scaling capabilities of that plan, and may cause the site to scale unnecessarily. What should you do if you don't want this to happen? Enter Azure Functions . Azure Functions are a technology for running WebJobs in a dynamic compute context. Dynamic Compute is a relative newcomer to the cloud computing stage and part of a new paradigm known as \" Serverless \". Let's take a tour through the history of cloud. In the beginning, cloud providers provided virtual machines, networking and storage, also known as \" Infrastructure as a Service \" or IaaS. You built a cloud service in much the same way as you built an on-premise solution. You cede control of the hardware to the cloud provider, but you are responsible for the maintenance of the platform. \" Platform as a Service \" or PaaS is a step up from this. With PaaS, you cede control of the operating system, security patching and platform maintenance to the cloud provider. You are responsible for the code that is running your app. With PaaS, however, you are still somewhat responsible and aware of the underlying infrastructure. You generally are making decisions on when to add a new virtual machine to the pool for scaling, for example. The virtual machine is present. \" Software as a Service \" (or SaaS) is the opposite end of the cloud services to IaaS. You are not responsible for anything in the platform. You just use the software. A good analogy is how to get a meal. IaaS is akin to going to the grocery store, picking all the ingredients you need, preparing the ingredients, cooking the meal, and serving the meal to you and your guest. SaaS is akin to going out to a restaurant and telling the waiter what you want. PaaS is similar to food box delivery services - they provide the ingredients and the recipe, but you do the cooking. In both of these cases, you are trading off management convenience for management control. IaaS has lots of control, but you have to do pretty much all the management yourself. SaaS is run for you, but you have no control. PaaS is in between these two extremes. But what about when you want even more management convenience than PaaS can offer, but you still want to run your application? Something inbetween PaaS and Saas, where scaling issues are taken care of for you. This is where Serverless technologies and dynamic compute come in. With Serverless, you still manage your code. However, they are infinitely scalable. That comes at a cost in terms of flexibility. Serverless does not mean \"without servers\". There are still servers involved. You just don't need to manage them in any way. Azure Functions are an implementation of Serverless technology to allow WebJobs to be written that happen in dynamic compute. You pay for the number of executions. (Technically, pricing is more complex than this simplification, but you will notice that your price goes up as the number of executions goes up). You can consider Azure Functions as \"WebJobs as a Service\". This isn't the only Serverless technology on the Azure platform. Azure Logic Apps is also a Serverless technology, covering Workflow. Function execution is also not the only serverless technology. You can find examples in authentication, message queuing, edge caching, search and others. It's quite possible to write a mobile backend entirely in Azure Functions. However, this is undesirable mostly because some processes (most notably SQL database access) require a relatively lengthy cold-start process. Functions are short-lived processes and may change the server that they are running on frequently. The mobile backend can provide efficiencies in this scenario by keeping a pool of connections open to the SQL database. This efficiency is not possible in Azure Functions. Building an Azure Function \u00b6 The first thing to note about Azure Functions is that they are a completely separate resource in the Azure Portal. This means that they are built separately, charged separately and scale independently to your mobile backend. This is in contrast to WebJobs, where the WebJob shares infrastructure with the mobile backend and scales with the mobile backend. Start by logging in to the Azure Portal. Click on the + NEW button, or the + Add button on a resource group. Search for Function App , then click Create . Enter a unique name for the Function App. The Function App is still an App Service, so you cannot name the Function App the same as your mobile backend. Set the Hosting Plan to be Consumption Plan . Pick your storage account that you created for the WebJobs demos (or skip to create a new one). Click on Create . Warn Dynamic Function Apps are not allowed in the same Region + Resource Group combination as non-Dynamic apps. That generally means you have to create another Resource Group to hold your Function Apps. Functions have a \"Hosting Plan\" instead of an App Service Plan. The choices are either \"Consumption Plan\", which uses dynamic compute, or \"App Service Plan\", which uses the virtual machines that host your App Services. One could argue there isn't much difference between WebJobs and Functions running on an App Service Plan. Once the deployment is complete, you will notice two new resources. The lightning bolt type icon is the Function App and you will spend most of your time there. You might also have an additional storage account if you chose to create one (or just clicked create). Just like WebJobs, Azure Functions needs a storage account to store runtime state and logs. Now that you have a Function App, you can start creating functions. Click on your Function App to open the Functions Console. Let's start by creating the same two WebJobs that we created in the last section, but using Azure Functions. Database Cleanup with Azure Functions \u00b6 Our first WebJob was a database cleanup process that was run on a schedule. If this is your first Function, then click on Create your own custom function . If not, you can click on the + New Function link in the side bar. Your next task is to select a template: We want a TimerTrigger for this example. Note that we can write in multiple languages. C#, F# and JavaScript are supported out of the box. You can also bring other languages. Other supported languages include Python, PowerShell, bash, and PHP. Click on the TimerTrigger - CSharp template, which is near the end of the list. You will need to name your function (I called mine DatabaseCleanup ) and configure a schedule for the trigger. The schedule is in a simplified cron-style expression. 3am is written as 0 0 3 * . Once you have set the two fields, click on Create . Tip You can create as many functions as you want inside of the Function App. They will all run independently, so there is no reason to need more than one Function App per resource group. At this point you have a fully functional Azure Function. In the Develop tab, you can click on Run to run your function. The Logs panel will show you the logs for running the function. If there is any output, you can see it in the Output panel. You can edit your code in-line. There is just one method - the Run() method. It looks quite like the WebJob. The trigger comes first, the output binding second and the TraceWriter comes last for logging. In a timer job, there is no output binding. Replace the code within the editor with the following: #r \"System.Data\" using System; using System.Configuration; using System.Data.SqlClient; public static void Run(TimerInfo myTimer, TraceWriter log) { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_TableConnectionString\"].ConnectionString; log.Info($\"Using Connection String {connectionString}\"); using (var sqlConnection = new SqlConnection(connectionString)) { using (var sqlCommand = sqlConnection.CreateCommand()) { log.Info(\"Initiating SQL Connection\"); sqlConnection.Open(); log.Info(\"Executing SQL Statement\"); sqlCommand.CommandText = \"DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] < DATEADD(day, -7, SYSDATETIMEOFFSET())\"; var rowsAffected = sqlCommand.ExecuteNonQuery(); log.Info($\"{rowsAffected} rows deleted.\"); sqlConnection.Close(); } } } Notice the #r directive. Azure Functions comes with some built-in references. They are listed in the C# Reference . System.Data is not one of those references, so you have to bring it in yourself. The #r directive brings in the reference. Other than that, this looks remarkably like the WebJob that does the same thing. This is normal. In fact, it's ok to develop the functionality in WebJobs and then translate the WebJob to a function once you have it working. Aside from the #r , you will notice some other things: Functions doesn't use Console . It provides a TraceWriter for logging instead. The signature of the Run() method is different than WebJobs. Once you save the file, the function is automatically compiled. If there are any compilation errors, they will show up in the Logs panel. If you click Run right now, you will see an error. That's because the connection string is not defined. In WebJobs, you defined this connection string in the App.config file. In Azure Functions, you just have to set the connection string up: Click Function app settings in the lower left corner. Click gfo to App Service Settings . Find and click Data Connections . Now connect your database to the function app in the same way that you did for the mobile backend. This reinforces, for me anyway, that the Function App is an App Service. It uses the same menu structure under the covers. You can also set additional app settings, link storage, and so on in the same way as on App Services. When you click on Run in your function now, you will see the log output. Image Resize with Azure Functions \u00b6 Let's create another C# Function. Before you start, link your storage account to the Function App using the Data Connections in the same way as you did your SQL database. Then click on the + New Function button and select the BlobTrigger - CSharp template. Warn Your storage account must be in the same region as your Function App. In general, resources that talk to one another should be colocated in the same region. However, this is a requirement for Azure Functions. Name your function, enter userdata within the Path field. Click on new next to the Storage account creation box. This will prompt you for a valid storage account and create a connection string for you. Click Create to create the blob. This will create the following Code: using System; public static void Run(string myBlob, TraceWriter log) { log.Info($\"C# Blob trigger function processed: {myBlob}\"); } I don't want to load each blob into a string, so this is definitely the wrong code. However, you will note that any image I have stored within the userdata area is displayed. Let's take a look at the code I use for the image resizer: #r \"System.Drawing\" #r \"Microsoft.WindowsAzure.Storage\" using System; using System.Drawing; using System.Drawing.Drawing2D; using System.Drawing.Imaging; using System.IO; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; static int requiredWidth = 800; static int requiredHeight = 600; public static async Task Run(CloudBlockBlob inputBlob, ICollector<string> outputQueueItem, TraceWriter log) { log.Info($\"processing File {inputBlob.Name}\"); var ext = Path.GetExtension(inputBlob.Name); log.Info($\"Ext = {ext}\"); if (!ext.ToLowerInvariant().Equals(\".png\")) { log.Info($\"Path {inputBlob.Name} is not a PNG file (skipping)\"); outputQueueItem.Add(inputBlob.Name); return; } var input = await inputBlob.OpenReadAsync(); // From WebJobs var image = Image.FromStream(input); if (image.Height > requiredHeight || image.Width > requiredWidth) { log.Info($\"Processing image {image.Height} x {image.Width}\"); var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight); var destImage = new Bitmap(requiredWidth, requiredHeight); destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution); using (var graphics = Graphics.FromImage(destImage)) { graphics.CompositingMode = CompositingMode.SourceCopy; graphics.CompositingQuality = CompositingQuality.Default; graphics.InterpolationMode = InterpolationMode.Bicubic; graphics.SmoothingMode = SmoothingMode.Default; graphics.PixelOffsetMode = PixelOffsetMode.Default; using (var wrapMode = new ImageAttributes()) { wrapMode.SetWrapMode(WrapMode.TileFlipXY); graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode); } } image = destImage; } log.Info(\"Writing new Image to publicdata area\"); using (var stream = new MemoryStream()) { image.Save(stream, ImageFormat.Png); stream.Position = 0; var connectionString = Environment.GetEnvironmentVariable(\"zumobook_STORAGE\"); CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer publicdata = blobClient.GetContainerReference(\"publicdata\"); CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(inputBlob.Name); blockBlob.UploadFromStream(stream); } outputQueueItem.Add(inputBlob.Name); } While this will work, we need to work on the integrations. Save the function, then click on Integrate . This is where you can set triggers (the things that cause the function to run), inputs (what is fed into the function) and outputs (what is generated by the function). The first step is to change the Blob parameter name to match what is included in the function call - in this case, my parameter is inputBlob . I could have called the trigger (which also becomes an input) myBlob and saved myself the trouble. I am against prefixing variables with my , so I like to change this. My original WebJob put the source file into a queue called delete , which I've already pre-created. I want to create an Output to do this. Click + New Output and choose Azure Storage Queue . Click on Select . You can fill in the details of the queue as follows: I'm selecting the same storage account as the rest of my application is using. The Message parameter name must match the signature of the static method in my function. Note that I'm using a ICollector<> isntead of a string. Async methods cannot use out types (or ref types, for that matter). As a result, I need a type that allows me to add things to the queue. The ICollector<> does that for me. Warn Don't forget to disable the ImageResizer WebJob in your mobile backend before testing the function. You can find selected WebJobs in the Properties\\webjobs-list.json file. You can test this function in the same way that you tested the WebJob. Use the Cloud Explorer in Visual Studio to upload a file into the userdata area of your Storage Account. If things go well, you will see the Info logs in the Logs area of the Function App. You will also see an entry in the delete queue and a file of the same name in the publicdata area when you refresh them. Handling File Deletion \u00b6 You can do a function to delete files using the delete queue as well. Create a new function with the QueueTrigger - CSharp template. Replace the code with the following: #r \"Microsoft.WindowsAzure.Storage\" using System; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; public static void Run(string myQueueItem, TraceWriter log) { log.Info($\"Processing File for Deletion {myQueueItem}\"); var connectionString = Environment.GetEnvironmentVariable(\"zumobook_STORAGE\"); CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer userdata = blobClient.GetContainerReference(\"userdata\"); CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(myQueueItem); blockBlob.DeleteIfExists(); } With this in place, when you upload an image to userdata, it will be processed and appear in the publicdata area. The original file will be deleted. You will be able to see the logs for all the functions in the Logs. Benefits and Drawbacks \u00b6 There are a few things that stand out as drawbacks: A separate resource group is required, which makes managing all resources together difficult. Tooling is limited right now and comprises of a command line (for Node.js development) and a plugin for Visual Studio 2015. That means no Intellisense, which is a primary problem. It uses a separate pricing model, so you are paying extra (at high volumes) for Functions. However, there are distinct advantages: You don't have to worry about scaling - it happens automatically. They are even simpler to debug and deploy than WebJobs (and that wasn't exactly tough). Integrated Functions monitoring is better than WebJobs. In all, I believe there is room in my toolbox for both WebJobs and Functions. The fact that you can translate between them means that you don't have to pick - you can be very situational about the choice depending on need.","title":"Functions"},{"location":"chapter4/functions/#building-an-azure-function","text":"The first thing to note about Azure Functions is that they are a completely separate resource in the Azure Portal. This means that they are built separately, charged separately and scale independently to your mobile backend. This is in contrast to WebJobs, where the WebJob shares infrastructure with the mobile backend and scales with the mobile backend. Start by logging in to the Azure Portal. Click on the + NEW button, or the + Add button on a resource group. Search for Function App , then click Create . Enter a unique name for the Function App. The Function App is still an App Service, so you cannot name the Function App the same as your mobile backend. Set the Hosting Plan to be Consumption Plan . Pick your storage account that you created for the WebJobs demos (or skip to create a new one). Click on Create . Warn Dynamic Function Apps are not allowed in the same Region + Resource Group combination as non-Dynamic apps. That generally means you have to create another Resource Group to hold your Function Apps. Functions have a \"Hosting Plan\" instead of an App Service Plan. The choices are either \"Consumption Plan\", which uses dynamic compute, or \"App Service Plan\", which uses the virtual machines that host your App Services. One could argue there isn't much difference between WebJobs and Functions running on an App Service Plan. Once the deployment is complete, you will notice two new resources. The lightning bolt type icon is the Function App and you will spend most of your time there. You might also have an additional storage account if you chose to create one (or just clicked create). Just like WebJobs, Azure Functions needs a storage account to store runtime state and logs. Now that you have a Function App, you can start creating functions. Click on your Function App to open the Functions Console. Let's start by creating the same two WebJobs that we created in the last section, but using Azure Functions.","title":"Building an Azure Function"},{"location":"chapter4/functions/#database-cleanup-with-azure-functions","text":"Our first WebJob was a database cleanup process that was run on a schedule. If this is your first Function, then click on Create your own custom function . If not, you can click on the + New Function link in the side bar. Your next task is to select a template: We want a TimerTrigger for this example. Note that we can write in multiple languages. C#, F# and JavaScript are supported out of the box. You can also bring other languages. Other supported languages include Python, PowerShell, bash, and PHP. Click on the TimerTrigger - CSharp template, which is near the end of the list. You will need to name your function (I called mine DatabaseCleanup ) and configure a schedule for the trigger. The schedule is in a simplified cron-style expression. 3am is written as 0 0 3 * . Once you have set the two fields, click on Create . Tip You can create as many functions as you want inside of the Function App. They will all run independently, so there is no reason to need more than one Function App per resource group. At this point you have a fully functional Azure Function. In the Develop tab, you can click on Run to run your function. The Logs panel will show you the logs for running the function. If there is any output, you can see it in the Output panel. You can edit your code in-line. There is just one method - the Run() method. It looks quite like the WebJob. The trigger comes first, the output binding second and the TraceWriter comes last for logging. In a timer job, there is no output binding. Replace the code within the editor with the following: #r \"System.Data\" using System; using System.Configuration; using System.Data.SqlClient; public static void Run(TimerInfo myTimer, TraceWriter log) { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_TableConnectionString\"].ConnectionString; log.Info($\"Using Connection String {connectionString}\"); using (var sqlConnection = new SqlConnection(connectionString)) { using (var sqlCommand = sqlConnection.CreateCommand()) { log.Info(\"Initiating SQL Connection\"); sqlConnection.Open(); log.Info(\"Executing SQL Statement\"); sqlCommand.CommandText = \"DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] < DATEADD(day, -7, SYSDATETIMEOFFSET())\"; var rowsAffected = sqlCommand.ExecuteNonQuery(); log.Info($\"{rowsAffected} rows deleted.\"); sqlConnection.Close(); } } } Notice the #r directive. Azure Functions comes with some built-in references. They are listed in the C# Reference . System.Data is not one of those references, so you have to bring it in yourself. The #r directive brings in the reference. Other than that, this looks remarkably like the WebJob that does the same thing. This is normal. In fact, it's ok to develop the functionality in WebJobs and then translate the WebJob to a function once you have it working. Aside from the #r , you will notice some other things: Functions doesn't use Console . It provides a TraceWriter for logging instead. The signature of the Run() method is different than WebJobs. Once you save the file, the function is automatically compiled. If there are any compilation errors, they will show up in the Logs panel. If you click Run right now, you will see an error. That's because the connection string is not defined. In WebJobs, you defined this connection string in the App.config file. In Azure Functions, you just have to set the connection string up: Click Function app settings in the lower left corner. Click gfo to App Service Settings . Find and click Data Connections . Now connect your database to the function app in the same way that you did for the mobile backend. This reinforces, for me anyway, that the Function App is an App Service. It uses the same menu structure under the covers. You can also set additional app settings, link storage, and so on in the same way as on App Services. When you click on Run in your function now, you will see the log output.","title":"Database Cleanup with Azure Functions"},{"location":"chapter4/functions/#image-resize-with-azure-functions","text":"Let's create another C# Function. Before you start, link your storage account to the Function App using the Data Connections in the same way as you did your SQL database. Then click on the + New Function button and select the BlobTrigger - CSharp template. Warn Your storage account must be in the same region as your Function App. In general, resources that talk to one another should be colocated in the same region. However, this is a requirement for Azure Functions. Name your function, enter userdata within the Path field. Click on new next to the Storage account creation box. This will prompt you for a valid storage account and create a connection string for you. Click Create to create the blob. This will create the following Code: using System; public static void Run(string myBlob, TraceWriter log) { log.Info($\"C# Blob trigger function processed: {myBlob}\"); } I don't want to load each blob into a string, so this is definitely the wrong code. However, you will note that any image I have stored within the userdata area is displayed. Let's take a look at the code I use for the image resizer: #r \"System.Drawing\" #r \"Microsoft.WindowsAzure.Storage\" using System; using System.Drawing; using System.Drawing.Drawing2D; using System.Drawing.Imaging; using System.IO; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; static int requiredWidth = 800; static int requiredHeight = 600; public static async Task Run(CloudBlockBlob inputBlob, ICollector<string> outputQueueItem, TraceWriter log) { log.Info($\"processing File {inputBlob.Name}\"); var ext = Path.GetExtension(inputBlob.Name); log.Info($\"Ext = {ext}\"); if (!ext.ToLowerInvariant().Equals(\".png\")) { log.Info($\"Path {inputBlob.Name} is not a PNG file (skipping)\"); outputQueueItem.Add(inputBlob.Name); return; } var input = await inputBlob.OpenReadAsync(); // From WebJobs var image = Image.FromStream(input); if (image.Height > requiredHeight || image.Width > requiredWidth) { log.Info($\"Processing image {image.Height} x {image.Width}\"); var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight); var destImage = new Bitmap(requiredWidth, requiredHeight); destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution); using (var graphics = Graphics.FromImage(destImage)) { graphics.CompositingMode = CompositingMode.SourceCopy; graphics.CompositingQuality = CompositingQuality.Default; graphics.InterpolationMode = InterpolationMode.Bicubic; graphics.SmoothingMode = SmoothingMode.Default; graphics.PixelOffsetMode = PixelOffsetMode.Default; using (var wrapMode = new ImageAttributes()) { wrapMode.SetWrapMode(WrapMode.TileFlipXY); graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode); } } image = destImage; } log.Info(\"Writing new Image to publicdata area\"); using (var stream = new MemoryStream()) { image.Save(stream, ImageFormat.Png); stream.Position = 0; var connectionString = Environment.GetEnvironmentVariable(\"zumobook_STORAGE\"); CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer publicdata = blobClient.GetContainerReference(\"publicdata\"); CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(inputBlob.Name); blockBlob.UploadFromStream(stream); } outputQueueItem.Add(inputBlob.Name); } While this will work, we need to work on the integrations. Save the function, then click on Integrate . This is where you can set triggers (the things that cause the function to run), inputs (what is fed into the function) and outputs (what is generated by the function). The first step is to change the Blob parameter name to match what is included in the function call - in this case, my parameter is inputBlob . I could have called the trigger (which also becomes an input) myBlob and saved myself the trouble. I am against prefixing variables with my , so I like to change this. My original WebJob put the source file into a queue called delete , which I've already pre-created. I want to create an Output to do this. Click + New Output and choose Azure Storage Queue . Click on Select . You can fill in the details of the queue as follows: I'm selecting the same storage account as the rest of my application is using. The Message parameter name must match the signature of the static method in my function. Note that I'm using a ICollector<> isntead of a string. Async methods cannot use out types (or ref types, for that matter). As a result, I need a type that allows me to add things to the queue. The ICollector<> does that for me. Warn Don't forget to disable the ImageResizer WebJob in your mobile backend before testing the function. You can find selected WebJobs in the Properties\\webjobs-list.json file. You can test this function in the same way that you tested the WebJob. Use the Cloud Explorer in Visual Studio to upload a file into the userdata area of your Storage Account. If things go well, you will see the Info logs in the Logs area of the Function App. You will also see an entry in the delete queue and a file of the same name in the publicdata area when you refresh them.","title":"Image Resize with Azure Functions"},{"location":"chapter4/functions/#handling-file-deletion","text":"You can do a function to delete files using the delete queue as well. Create a new function with the QueueTrigger - CSharp template. Replace the code with the following: #r \"Microsoft.WindowsAzure.Storage\" using System; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; public static void Run(string myQueueItem, TraceWriter log) { log.Info($\"Processing File for Deletion {myQueueItem}\"); var connectionString = Environment.GetEnvironmentVariable(\"zumobook_STORAGE\"); CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer userdata = blobClient.GetContainerReference(\"userdata\"); CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(myQueueItem); blockBlob.DeleteIfExists(); } With this in place, when you upload an image to userdata, it will be processed and appear in the publicdata area. The original file will be deleted. You will be able to see the logs for all the functions in the Logs.","title":"Handling File Deletion"},{"location":"chapter4/functions/#benefits-and-drawbacks","text":"There are a few things that stand out as drawbacks: A separate resource group is required, which makes managing all resources together difficult. Tooling is limited right now and comprises of a command line (for Node.js development) and a plugin for Visual Studio 2015. That means no Intellisense, which is a primary problem. It uses a separate pricing model, so you are paying extra (at high volumes) for Functions. However, there are distinct advantages: You don't have to worry about scaling - it happens automatically. They are even simpler to debug and deploy than WebJobs (and that wasn't exactly tough). Integrated Functions monitoring is better than WebJobs. In all, I believe there is room in my toolbox for both WebJobs and Functions. The fact that you can translate between them means that you don't have to pick - you can be very situational about the choice depending on need.","title":"Benefits and Drawbacks"},{"location":"chapter4/options/","text":"Server Side Code \u00b6 At some point during your mobile client development, you will need to do something a little outside simple data access. We have already seen an example of this in custom authentication. It may be that you want to kick off a workflow, re-size an image using server-side resources, push a notification to another user, or do a complex transaction on the database. Whatever the reason, that time is when you want to use server side code. The aim is simple enough. On a trigger, execute some code and do something with the result. The trigger can be as simple as a HTTP request from your client, but could also be in response to a timer, or because something in your environment happened. The result can be sent back to the user, placed in storage, or updated in the database. There really are no rules when it comes to server side code. There are, however, options for running server side code. Client Processing with WebAPIs and Custom APIs \u00b6 The first of the set is the venerable ASP.NET WebAPI . Firstly, configure the ASP.NET application to allow attribute-based routing. This is done in your Startup.MobileApp.cs file with this line: config.MapHttpAttributeRoutes(); Any WebAPI that you provide must be preceded by a [Route] attribute. We saw an example of this in the Custom Authentication section. In custom authentication, we were setting up an endpoint that allows us to validate a login request. The attribute looked like this: namespace Backend.Controllers { [Route(\".auth/login/custom\")] public class CustomAuthController : ApiController { [HttpPost] public async Task<IHttpActionResult> Post([FromBody] UserInfo body) { ... } } } The ASP.NET WebAPI comes with a bunch of capabilities: You can use your regular ASP.NET WebAPI programming techniques. You can use Entity Framework for adjusting the database. You can use the [Authorize] attribute for controlling access. With ASP.NET WebAPI, you are responsible for absolutely everything. This option is great for providing an endpoint to your mobile client that doesn't need anything special. As flexible as the ASP.NET WebAPI is, most of the time you just want to do something and get the result returned. The Custom API feature of the Azure Mobile Apps SDK is a great option because it does a lot of the scaffolding for you. With a Custom API: Your API appears under '/api' - no exceptions. The server enforces the ZUMO-API-VERSION. The server emits a X-ZUMO-SERVER-VERSION header in the response. You can be sure, for example, that a random web crawler is not going to call your Custom API - the web crawler is not going to provide the ZUMO-API-VERSION header, so your code would never be touched. The Azure Mobile Apps Client SDK also includes a routine that assumes a Custom API. For example, let's say you create a ValuesController , then you can call this from your mobile client with the following code: var result = client.InvokeApiAsync<ResultType>('Values'); Using InvokeApiAsync is a good alternative because it provides the authentication automatically for you and uses any DelegatingHandler classes you have configured. Tip You can still use InvokeApiAsync() with an ASP.NET WebAPI. Background Processing with WebJobs & Azure Functions \u00b6 The ASP.NET WebAPI and the Custom API provide HTTP endpoints for your mobile clients to interact with. WebJobs and Azure Functions , by comparison, are primed for background tasks. Things that WebJobs and Azure Functions are good at: Image processing or other CPU-intensive work. Queue processing. RSS aggregation. File and database management. WebJobs run in the context of your site. They use the same set of virtual machines that your website uses and they share resources with your site. That means that running memory or CPU intensive jobs can affect your mobile backend. Azure Functions run in a separate project and run in \"dynamic compute\". They don't run on your virtual machines. Rather, they pick up compute power from wherever it is available. The scaling and lifecycle of the Function is handled for you by the platform. The downside is that you have to configure it as a separate project and you pay for the executions separately. Best Practices \u00b6 For each thing we have defined, we have two choices. There are reasons to use each and every one of these options. So, which do you choose. Here are my choices: Use a Custom API for basic web endpoints. Use a WebAPI for custom authentication and anything where you care about the shape of the API. Use WebJobs for clean-up tasks running on a schedule. Use Functions for triggered batch processing, like image or queue processing. Most mobile applications will be able to work with the following: A single WebJob for cleaning up the database deleted records. A Custom API for doing transaction work or triggering a batch process. An Azure Function for doing the batch processing.","title":"Options for Server Code"},{"location":"chapter4/options/#server-side-code","text":"At some point during your mobile client development, you will need to do something a little outside simple data access. We have already seen an example of this in custom authentication. It may be that you want to kick off a workflow, re-size an image using server-side resources, push a notification to another user, or do a complex transaction on the database. Whatever the reason, that time is when you want to use server side code. The aim is simple enough. On a trigger, execute some code and do something with the result. The trigger can be as simple as a HTTP request from your client, but could also be in response to a timer, or because something in your environment happened. The result can be sent back to the user, placed in storage, or updated in the database. There really are no rules when it comes to server side code. There are, however, options for running server side code.","title":"Server Side Code"},{"location":"chapter4/options/#client-processing-with-webapis-and-custom-apis","text":"The first of the set is the venerable ASP.NET WebAPI . Firstly, configure the ASP.NET application to allow attribute-based routing. This is done in your Startup.MobileApp.cs file with this line: config.MapHttpAttributeRoutes(); Any WebAPI that you provide must be preceded by a [Route] attribute. We saw an example of this in the Custom Authentication section. In custom authentication, we were setting up an endpoint that allows us to validate a login request. The attribute looked like this: namespace Backend.Controllers { [Route(\".auth/login/custom\")] public class CustomAuthController : ApiController { [HttpPost] public async Task<IHttpActionResult> Post([FromBody] UserInfo body) { ... } } } The ASP.NET WebAPI comes with a bunch of capabilities: You can use your regular ASP.NET WebAPI programming techniques. You can use Entity Framework for adjusting the database. You can use the [Authorize] attribute for controlling access. With ASP.NET WebAPI, you are responsible for absolutely everything. This option is great for providing an endpoint to your mobile client that doesn't need anything special. As flexible as the ASP.NET WebAPI is, most of the time you just want to do something and get the result returned. The Custom API feature of the Azure Mobile Apps SDK is a great option because it does a lot of the scaffolding for you. With a Custom API: Your API appears under '/api' - no exceptions. The server enforces the ZUMO-API-VERSION. The server emits a X-ZUMO-SERVER-VERSION header in the response. You can be sure, for example, that a random web crawler is not going to call your Custom API - the web crawler is not going to provide the ZUMO-API-VERSION header, so your code would never be touched. The Azure Mobile Apps Client SDK also includes a routine that assumes a Custom API. For example, let's say you create a ValuesController , then you can call this from your mobile client with the following code: var result = client.InvokeApiAsync<ResultType>('Values'); Using InvokeApiAsync is a good alternative because it provides the authentication automatically for you and uses any DelegatingHandler classes you have configured. Tip You can still use InvokeApiAsync() with an ASP.NET WebAPI.","title":"Client Processing with WebAPIs and Custom APIs"},{"location":"chapter4/options/#background-processing-with-webjobs-azure-functions","text":"The ASP.NET WebAPI and the Custom API provide HTTP endpoints for your mobile clients to interact with. WebJobs and Azure Functions , by comparison, are primed for background tasks. Things that WebJobs and Azure Functions are good at: Image processing or other CPU-intensive work. Queue processing. RSS aggregation. File and database management. WebJobs run in the context of your site. They use the same set of virtual machines that your website uses and they share resources with your site. That means that running memory or CPU intensive jobs can affect your mobile backend. Azure Functions run in a separate project and run in \"dynamic compute\". They don't run on your virtual machines. Rather, they pick up compute power from wherever it is available. The scaling and lifecycle of the Function is handled for you by the platform. The downside is that you have to configure it as a separate project and you pay for the executions separately.","title":"Background Processing with WebJobs &amp; Azure Functions"},{"location":"chapter4/options/#best-practices","text":"For each thing we have defined, we have two choices. There are reasons to use each and every one of these options. So, which do you choose. Here are my choices: Use a Custom API for basic web endpoints. Use a WebAPI for custom authentication and anything where you care about the shape of the API. Use WebJobs for clean-up tasks running on a schedule. Use Functions for triggered batch processing, like image or queue processing. Most mobile applications will be able to work with the following: A single WebJob for cleaning up the database deleted records. A Custom API for doing transaction work or triggering a batch process. An Azure Function for doing the batch processing.","title":"Best Practices"},{"location":"chapter4/recipes/","text":"Now that we have explored the syntax and methodologies of invoking server-side code, we can look at some very common use cases for custom code. Storage Related Operations \u00b6 When dealing with cloud concepts, there are multiple operating levels one can think about. At the bottom layer is Infrastructure as a Service . Most people think of this as the Virtual Machine layer, but it also incorporates basic networking and storage concepts. As you move to higher level services, you gain a lot of efficiencies by adding software components, you lose a lot of the potential management headaches, but you also lose flexibility in what you can do to the platform. At the top of the stack is Software as a Service . You may be running a helpdesk, for example, but you are completely isolated from what operating system is being run, what web services are being run, APIs that can be accessed and language that is used. Azure Mobile Apps is an opinionated combination of a client and server SDK running on top of a standard ASP.NET based web service and is normally thought of as being a Platform as a Service . You get to choose what database to use, what tables to expose, and what programming language to use. You don't get to determine when the operating system is patched or what patches are applied. It's a middle of the road between SaaS and IaaS. That isn't to say we can't dip down sometimes to deal with lower level cloud services, nor to access higher level SaaS APIs. One of those times is when dealing with files. Storage is conceptually easy - you have an amount of disk and you can store files on it. However, the management of that storage is complicated. Placing that storage at the service of a scalable web application is similarly complicated. What we intend to do is develop a set of skills that make developing storage based mobile applications easy. Blobs, Table, Queues and files \u00b6 At the top of my list of \"storage made complicated\" is the cloud storage concepts. In the old days, we stored files on a file system and we didn't really have to worry about differing types of storage, redundancy and capabilities. Cloud storage tends to come in multiple flavors: The base storage type is Blob Storage . Put simply, you have containers (roughly analogous to directories) and blobs (roughly analogous to files). It's the cheapest form of storage and is used for many things, including the underlying storage for virtual machine disks. Blob storage has many advantages. From a mobile perspective, developers will appreciate the upload/download restart capabilities within the SDK. We've already introduced Table Storage in the last chapter . It is more analogous to a NoSQL store for storing key / attribute values. It has a schemaless design, so you can store basic JSON objects. However, it has limited query capabilities, as we discussed in the last chapter. That makes it unsuited to large scale query-driven applications. You may think you want Files Storage . This provides an SMB interface to the storage layer. You would use Files Storage if you want to browse files from your PC or Mac as you can mount the file system directly from Azure Storage. Finally, Queue Storage provides cloud messaging between application components. We'll get onto Azure Functions later on, during our look at Custom API. Queue Storage will definitely be coming into play then. Think of Queue Storage as the glue that ties work flow components together. The real question is when should you use File Storage and when should you use Blob Storage. For more applications, Blob Storage is going to save you money over File Storage, so it's pretty much always the better choice. You should only be thinking of File Storage if you have other components of your system that need to access the data you upload that can only access that data via an SMB interface. If you need to explore the data that you upload or download, you can use the Azure Storage Explorer as a standalone application or you can use the Cloud Explorer in Visual Studio . Creating and Linking a Storage Account \u00b6 Before we can use storage, we need to set up a storage account and connect it to our environment. This involves: Create a Resource Group Create an Azure App Service Set up authentication on the Azure App Service Create a Storage Account Link the Storage Account to the Azure App Service. We've already covered the first three items in previous chapters. We've also created a storage account and linked it to the mobile backend during our look at the Storage Domain Manager . To create a Storage Account: Log on to the Azure portal . Click the big + NEW button in the top left corner. Click Data + Storage , then Storage account . Fill in the form: The name can only contain letters and numbers and must be unique. A GUID without the dashes is a good choice. The Deployment model should be set to Resource manager . The Account kind should be set to General purpose . The Performance should be set to Standard for this example. The Replication should be set to Locally-redundant storage (LRS) . Set the Resource group to your existing resource group. Set the Location to the same location as your App Service. Click Create . Just like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. For example, you have the option of Premium Storage - this provides all-SSD storage that has a large IOPS performance number. You can also decide how redundant you want the storage. Azure always keeps 3 copies of your data. You can choose to increase the number of copies and decide whether the additional copies will be in the same datacenter, another datacenter in the same region or another region. We have selected the slowest performance and least redundant options here to keep the cost down on your service. Warn There is no \"free\" option for Azure Storage. You pay by the kilobyte depending on the performance and redundancy selected. Once the Azure Storage account is deployed, you can link the storage account to your App Service: Open your App Service in the Azure portal . Click Data Connections under the MOBILE section in the settings menu. Click + ADD In the Add data connection blade: Set the Type to Storage . Click the Storage link. In the Storage Account selector, click the storage account you just created. Click the Connection string . In the Connection string selector, make a note of the Name field. Click OK . Click OK to close the Add data connection blade. Click on the Application Settings menu option, then scroll down to the Connection Strings section. Note that the portal has created the connection string as an App Setting for you with the right value: DefaultEndpointsProtocol=https;AccountName=thebook;AccountKey=<key1> By default, the connection string is called MS_AzureStorageAccountConnectionString and we will use that throughout our examples. The key is the access key for the storage. When a storage account is created, two keys are also created. The keys are used for secure access to the storage area. You should never distribute the storage keys nor check them into source control. If you feel they have been compromised, you should regenerate them. There are two keys for this purpose. The process of regeneration is: Regenerate KEY2 Place the regenerated KEY2 in the connection string and restart your App Service. Regenerate key1 Place the regenerated KEY1 in the connection string and restart your App Service. In this way, your App Service will always be using KEY1 except during regeneration. You can avoid the restart of your App Service by providing a management interface that sets the Account Key for the App Service. Tip For local development, there is the Azure Storage Emulator . The connection string when using the Azure Storage Emulator is UseDevelopmentStorage=true . It's normal to add the storage connection string to the Web.config file with the following: <connectionStrings> <add name=\"MS_AzureStorageAccountConnectionString\" connectionString=\"UseDevelopmentStorage=true\" /> </connectionStrings> This will be overwritten by the connection string in the App Service Application Settings. Effectively, you will be using the Azure Storage Emulator during local development and Azure Storage when you deploy to Azure App Service. The Shared Access Signature (SAS) \u00b6 The storage account key is kind of like the root or Administrator password. You should always protect it, never send it to a third party and regenerate it on a regular basis. You avoid storing the storage account key in source code by linking the storage account to the App Service. The key is stored in the connection string instead. You should never ship an account key to your mobile account. The Azure Storage SDK already has many of the features that you want in handling file upload and download. Azure Storage is optimized for streaming, for example. You can upload or download blobs in blocks, allowing you to restart the transfer and provide feedback to the user on progress, for example. You will inevitably be drawn to having your mobile client interact with Azure Storage directly rather than having an intermediary web service for this reason. If you want to interact with Azure Storage directly and you shouldn't give out the account key, how do you deal with the security of the service? The answer is with a Shared Access Signature, or SAS. The Service SAS delegates access to just a single resource in one of the storage services (Blob, Table, Queue or File service). Info There is also an Account SAS which delegates access to resources in more than one service. You generally don't want this in application development. A service SAS is a URI that is used when accessing the resource. It consists of the URI to the resource followed by a SAS token. The SAS token is an cryptographically signed opaque token that the storage service decodes. Minimally, it provides an expiry time and the permissions being granted to the SAS. Warn A SAS token ALWAYS expires. There is no way to produce a permanent SAS token. If you think you need one, think again. In mobile development, you NEVER want a non-expiring token. Accessing Azure Storage is always done with a specific version of the REST API and that follows through to the SDK. You should always request a SAS token for the appropriate API you are going to be using. We'll cover the various methods of obtaining a SAS later in the chapter. Uploading a File \u00b6 The most normal tasks for dealing with files are the upload and download of files to blob storage. There is a natural and consistent process to this which makes this recipe very repeatable. First, deal with the things you need before you start: Create an Azure Storage Account and link it to your Azure App Service. Decide how you want your files organized. Create a WebAPI to generate a SAS token for your upload or download. Blob storage is organized in a typical directory structure. Each directory is called a container, and each file is a blob. In the examples for this section, I am going to store each uploaded file in a container based on the authenticated user. My WebAPI will create the appropriate container and then return an appropriate SAS token. We can set up our custom API as follows: namespace Backend.Controllers { [Authorize] [MobileappController] public class GetStorageTokenController : ApiController { private const string connString = \"CUSTOMCONNSTR_MS_AzureStorageAccountConnectionString\"; public GetStorageTokenController() { ConnectionString = Environment.GetEnvironmentVariable(connString); StorageAccount = CloudStorageAccount.Parse(ConnectionString); BlobClient = StorageAccount.CreateCloudBlobClient(); } public string ConnectionString { get; } public CloudStorageAccount StorageAccount { get; } public CloudBlobClient BlobClient { get; } } } The ConnectionString property is the pointer to where the Azure Storage account is located and how to access it. the StorageAccount is a reference to that Azure Storage account. Finally, the BlobClient is an object used for accessing blob storage. We can access any WebAPI methods in this class by using the endpoint /api/GetStorageToken within our mobile client or using Postman. Azure Storage doesn't have a true heirarchial container system. It does have containers and directories to organize things though, so we are going to use that: private const string containerName = \"userdata\"; [HttpGet] public async Task<StorageTokenViewModel> GetAsync() { // The userId is the SID without the sid: prefix var claimsPrincipal = User as ClaimsPrincipal; var userId = claimsPrincipal .FindFirst(ClaimTypes.NameIdentifier) .Value.Substring(4); // Errors creating the storage container result in a 500 Internal Server Error var container = BlobClient.GetContainerReference(containerName); await container.CreateIfNotExistsAsync(); // Get the user directory within the container var directory = container.GetDirectoryReference(userId); var blobName = Guid.NewGuid().ToString(\"N\"); var blob = directory.GetBlockBlobReference(blobName); // Create a policy for accessing the defined blob var blobPolicy = new SharedAccessBlobPolicy { SharedAccessStartTime = DateTime.UtcNow.AddMinutes(-5), SharedAccessExpiryTime = DateTime.UtcNow.AddMinutes(60), Permissions = SharedAccessBlobPermissions.Read | SharedAccessBlobPermissions.Write | SharedAccessBlobPermissions.Create }; return new StorageTokenViewModel { Name = blobName, Uri = blob.Uri, SasToken = blob.GetSharedAccessSignature(blobPolicy) }; } The main piece of work in this API is generating the policy that is then signed and returned to the user as the SAS Token. The mobile device has permission to read, write and create the blob that we have defined for the next 60 minutes. I've provided a policy that starts in the past in case there is a little amount of clock-skew between the mobile device and the backend. Warn Container names must be a valid DNS name. The most notable requirement here is between 3 and 64 lower-case letters. Container names are case-sensitive. Check the documentation for full details on naming requirements. The StorageTokenViewModel is used for serialization purposes: public class StorageTokenViewModel { public string Name { get; set; } public Uri Uri { get; set; } public string SasToken { get; set; } } We can test this API using Postman. First, generate an authentication token. Then use Postman to do a GET of the /api/GetStorageToken endpoint: There are two pieces of information we need here. Firstly, the uri property provides the URI that we are going to use to upload the file. Secondly, the sasToken is appended to the uri when uploading to provide a link to the policy. Note that the token start and expiry time are encoded and readable in the sasToken. In real world applications, this is likely not the right method. We might want to organize the files based on information that the mobile client provides us, for example. We may also want to upload to a specific upload area and then download from another location, allowing processing of the files in between. You may also want to append the uploaded file extension to the file before uploading. There is no \"one size fits all\" token policy. You must decide on the conditions under which you will allow upload and download capabilities and then provide the appropriate logic to generate the SAS token. The Mobile Client \u00b6 Once we have the logic to generate a SAS token, we can turn our attention to the mobile clients. We need to do three things for uploading a file to the service: Get a reference to the file (as a Stream object). Generate a SAS token using the custom API. Use the Azure Storage SDK to upload directly to the Azure Storage Account. You should not upload to a custom API in your mobile backend. This needlessly ties up your mobile backend, causing your mobile backend to be less efficient at scaling. Your mobile backend will not have all the facilities that the Azure Storage endpoint has provided either. Azure Storage provides upload and download restarts and progress bar capabilities. Obtaining a reference to the file that you wish to upload is normally a per-platform API. Obtaining a reference to a photo or video involves interacting with platform-specific APIs to provide access to camera and built-in photo storage capabilities on the phone. To support such a per-platform capability, we need to add an interface for the API to the Abstractions\\IPlatform.cs file: Task<Stream> GetUploadFileAsync(); This API will interact with whatever photo sharing API is available on the device, open the requested file and return a standard Stream object. Loading a media file is made much simpler using the cross-platform Xamarin Media plugin . This plugin allows the user to take photos or video, or pick the media file from a gallery. It's available on NuGet, so add the Xam.Plugin.Media plugin to each of the platform-specific projects. Tip I still like separating out code that deals with the hardware of a mobile device into the platform-specific code. You don't need to do such separation on this project. I find that I inevitably have one thing or another that requires a platform-specific tweak, so starting with a platform-specific API is better. The Xamarin Media plugin is used like this: await CrossMedia.Current.Initialize(); var file = await CrossMedia.Current.PickPhotoAsync(); var stream = file.GetStream(); There are methods within the plugin to determine if a camera is available. Different platforms require different permissions: Android \u00b6 Android requires the WRITE_EXTERNAL_STORAGE , READ_EXTERNAL_STORAGE and CAMERA permissions. If the mobile device is running Android M or later, the plugin will automatically prompt the user for runtime permissions. You can set these permissions within Visual Studio: Double-click the Properties node within the Android project. Select Android Manifest . In the Required permissions list, check the box next to the required permissions by double-clicking the permission. Save the Properties (you may have to right-click on the TaskList.Droid tab and click on Save Selected Items ). iOS \u00b6 Apple iOS requires the NSCameraUsageDescription and NSPhotoLibraryUsageDescription keys. The string provided will be displayed to the user when they are prompted to provide permission. You can set these keys within Visual Studio: Right-click on the Info.plist file and select Open with... Choose the XML (Text) Editor then click OK . Within the <dict> node, add the following lines: <key>NSCameraUsageDescription</key> <string>This app needs access to the camera to take photos.</string> <key>NSPhotoLibraryUsageDescription</key> <string>This app needs access to photos.</string> Save and close the file. You can choose whatever string you want to display to the user. For more information on iOS 10 privacy permissions, review the Xamarin Blog . Universal Windows \u00b6 Universal Windows may require the Pictures Library capability: In the TaskList.UWP (Universal Windows) project, open Package.appxmanifest . Select the Capabilities tab. Check the box next to Pictures Library . Save the manifest. Implementing the File Reader \u00b6 The same code can be used in all three platform-specific projects, in the *Platform.cs file: /// <summary> /// Picks a photo for uploading /// </summary> /// <returns>A Stream for the photo</returns> public async Task<Stream> GetUploadFileAsync() { var mediaPlugin = CrossMedia.Current; var mainPage = Xamarin.Forms.Application.Current.MainPage; await mediaPlugin.Initialize(); if (mediaPlugin.IsPickPhotoSupported) { var mediaFile = await mediaPlugin.PickPhotoAsync(); return mediaFile.GetStream(); } else { await mainPage.DisplayAlert(\"Media Service Unavailable\", \"Cannot pick photo\", \"OK\"); return null; } } Uploading a File \u00b6 We can now put the individual pieces together to actually do an upload. In this example, we are going to use the photo picker to pick a photo and then upload it, displaying a progress bar as it happens. We start with the XAML code in Pages\\TaskList.xaml . We need a button in the toolbar to initiate the file upload: <ContentPage.ToolbarItems> <ToolbarItem Name=\"Refresh\" Command=\"{Binding RefreshCommand}\" Icon=\"refresh.png\" Order=\"Primary\" Priority=\"0\" /> <ToolbarItem Name=\"Add Task\" Command=\"{Binding AddNewItemCommand}\" Icon=\"add.png\" Order=\"Primary\" Priority=\"0\" /> <ToolbarItem Name=\"Add File\" Command=\"{Binding AddNewFileCommand}\" Icon=\"addfile.png\" Order=\"Primary\" Priority=\"0\" /> </ContentPage.ToolbarItems> Obtain a suitable \"Add File\" icon from the Internet and resize the image appropriately for the task. You will need five images total: TaskList.Droid\\Resources\\drawable\\addfile.png should be 128x128 pixels TaskList.iOS\\Resources\\addfile.png should be 25x25 pixels TaskList.iOS\\Resources\\addfile@2x.png should be 50x50 pixels TaskList.iOS\\Resources\\addfile@3x.png should be 75x75 pixels TaskList.UWP\\addfile.png should be 128x128 pixels All images should have a transparent background. The storage token is retrieved from the backend via the cloud service. Add the following to Abstractions\\ICloudService.cs : // Custom APIs Task<StorageTokenViewModel> GetSasTokenAsync(); This has a concrete implementation in Services\\AzureCloudService.cs : public async Task<StorageTokenViewModel> GetSasTokenAsync() { var parameters = new Dictionary<string, string>(); var storageToken = await Client.InvokeApiAsync<StorageTokenViewModel>(\"GetStorageToken\", HttpMethod.Get, parameters); return storageToken; } The StorageTokenViewModel is identical to the class in the GetStorageTokenController.cs controller in the Backend. I've placed the class definition in the Models namespace for the client. We could share this model between the backend and front end, but the case of sharing models is so rare I tend not to share the code. In the TaskListViewModel.cs , we can define a command that is called when the Add File button is clicked: /// <summary> /// Reference to the Platform Provider /// </summary> public IPlatform PlatformProvider => DependencyService.Get<IPlatform>(); /// <summary> /// Bindable property for the AddNewFile Command /// </summary> public ICommand AddNewFileCommand { get; } /// <summary> /// User clicked on the Add New File button /// </summary> private async Task AddNewFileAsync() { if (IsBusy) { return; } IsBusy = true; try { // Get a stream for the file var mediaStream = await PlatformProvider.GetUploadFileAsync(); if (mediaStream == null) { IsBusy = false; return; } // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(); // Use the SAS token to upload the file var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); await blobStorage.UploadFromStreamAsync(mediaStream); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Error Uploading File\", ex.Message, \"OK\"); } finally { IsBusy = false; } } Warn Azure Storage SDK support for PCL projects is only available in -preview editions. When installing the SDK, ensure you check the \"Include prerelease\" box in the NuGet package manager. The latest version with PCL (.NETPortable) support is v7.0.2-preview. You can look at the uploaded files in the Azure portal: Log in to the Azure portal . Click All resources , then your storage account. Under SERVICES , click Blobs . Click your storage container (in this example, that's called userdata ) You will see a folder for each user account. The folder is named for the SID of the account - not the username. It's a good idea to store the user SID with other data about the user in a table within your database. This allows you to associate a real user with their SID since a user will never know what their SID is. Implementing a Progress bar \u00b6 It's common to want to see the progress of the upload while it is happening. For that, we need a progress bar. First, let's add a hidden progress bar to our TaskList.xaml page: <ActivityIndicator HorizontalOptions=\"FillAndExpand\" IsRunning=\"{Binding IsBusy}\" IsVisible=\"{Binding IsBusy}\" VerticalOptions=\"Start\" /> <ProgressBar x:Name=\"fileUploadProgress\" HeightRequest=\"3\" HorizontalOptions=\"FillAndExpand\" IsVisible=\"{Binding IsUploadingFile}\" Progress=\"{Binding FileProgress}\" /> This comes with two new bindable properties. IsUploadingFile is a bool and FileProgress is a Double . FileProgress takes a value between 0 and 1 to indicate how far along the progress bar should be. This code should be in the TaskListViewModel.cs file: private bool isUploadingFile; public bool IsUploadingFile { get { return isUploadingFile; } set { SetProperty(ref isUploadingFile, value, \"IsUploadingFile\"); } } private Double fileProgress = 0.0; public Double FileProgress { get { return fileProgress; } set { SetProperty(ref fileProgress, value, \"FileProgress\"); } } Finally, we have to change the upload so that it happens a chunk at a time. In the AddNewFileAsync() method, we can replace the upload code with this: /// <summary> /// User clicked on the Add New File button /// </summary> private async Task AddNewFileAsync() { if (IsBusy) { return; } IsBusy = true; try { // Get a stream for the file var mediaStream = await PlatformProvider.GetUploadFileAsync(); if (mediaStream == null) { IsBusy = false; return; } // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(); // Use the SAS token to get a reference to the blob storage var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); // Get the length of the stream var mediaLength = mediaStream.Length; // Initialize the blocks int bytesInBlock = 1024; // The number of bytes in a single block var buffer = new byte[bytesInBlock]; // The buffer to hold the data during transfer int totalBytesRead = 0; // The number of bytes read from the stream. int bytesRead = 0; // The number of bytes read per block. int blocksWritten = 0; // The # Blocks Written IsUploadingFile = true; FileProgress = 0.00; // Loop through until we have processed the whole file do { // Read a block from the media stream bytesRead = mediaStream.Read(buffer, 0, bytesInBlock); if (bytesRead > 0) { // Move the buffer into a memory stream using (var memoryStream = new MemoryStream(buffer, 0, bytesRead)) { string blockId = GetBlockId(blocksWritten); await blobStorage.PutBlockAsync(blockId, memoryStream, null); } // Update the internal counters totalBytesRead += bytesRead; blocksWritten++; // Update the progress bar FileProgress = totalBytesRead / mediaLength; } } while (bytesRead > 0); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Error Uploading File\", ex.Message, \"OK\"); } finally { IsBusy = false; IsUploadingFile = false; FileProgress = 0.0; } } /// <summary> /// Convert the Block ID to the string we need /// </summary> /// <param name=\"block\"></param> /// <returns></returns> private string GetBlockId(int block) { char[] tempID = new char[6]; string iStr = block.ToString(); for (int j = tempID.Length - 1; j > (tempID.Length - iStr.Length - 1); j--) { tempID[j] = iStr[tempID.Length - j - 1]; } byte[] blockIDBeforeEncoding = Encoding.UTF8.GetBytes(tempID); return Convert.ToBase64String(blockIDBeforeEncoding); } The main work is done in the inner loop. We split the media stream into 1024 byte blocks. Each block is copied into a temporary buffer then transferred to cloud storage. After each block is delivered, the FileProgress counter is updated which updates the progress bar. One of the secrets for doing block-based streaming uploads is the GetBlockId() method. This properly formats the block ID (based on a rather convoluted method that ends up being Base-64 encoded). If you do not get this right, you will instead get a rather cryptic message about the query parameter for the HTTP request being wrong. Downloading a File \u00b6 You can similarly download a file from blob storage. The basics (such as the SAS token generator) are exactly the same as before. To do the basic form: // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(filename); // Use the SAS token to get a reference to the blob storage var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); // Get a stream for the blob file var mediaStream = await blobStorage.OpenReadAsync(); // Do something with the mediaStream - like move it to storage await PlatformProvider.StoreFileAsync(mediaStream); // At the end, close the stream properly mediaStream.Dispose(); Similarly, you can also produce a progress bar: // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(filename); // Use the SAS token to get a reference to the blob storage var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); var mediaStream = await blobStorage.OpenReadAsync(); var mediaLength = mediaStream.Length; byte[] buffer = new byte[1024]; var bytesRead = 0, totalBytesRead = 0; // Do what you need to for opening your output file do { bytesRead = mediaStream.ReadAsync(buffer, 0, 1024); if (bytesRead > 0) { // Do something with the buffer totalBytesRead += bytesRead; FileProgress = totalBytesRead / mediaLength; } } while (bytesRead > 0); // Potentially close your output file as well mediaStream.Dispose(); When downloading, you will need to update the GetStorageTokenController method to provide access to files. One possibility is to provide read/write access to the entire container, allowing the mobile device to get a directory listing for browsing, for example. You need to decide what permissions your mobile client needs, and provide a SAS token with those permissions. Primarily, this is done by altering the policy that is generated when you retrieve the SAS token. Table Controllers and Webhooks \u00b6 I love writing asynchronous applications. One of the four features I mentioned with Table Controllers is the Webhook. In essence, if someone inserts, updates or deletes a record, you may want to do something asynchronously. For example, you may want to do sentiment analysis on the record that was just uploaded, or perhaps execute some custom code to simulate an offline custom API. A Webhook is a callback mechanism whereby the controller will do an HTTP POST when something happens. It's an event processing system over HTTP. In this sample, we are going to generate a simple Webhook function that logs the inserted record, then adjust our table controller to call the Webhook when an insert happens. Let's first of all create a Function that handles the request. Create a Function App, and then create a new function based on the Generic Webhook - CSharp template. Call this function InsertTodoItemWebhook . You can call this whatever you want, but the URI of your Webhook is based on the name of your function. Replace the body of the function with the following: #r \"Newtonsoft.Json\" using System; using System.Net; using Newtonsoft.Json; public static async Task<object> Run(HttpRequestMessage req, TraceWriter log) { string jsonContent = await req.Content.ReadAsStringAsync(); dynamic data = JsonConvert.DeserializeObject(jsonContent); log.Info($\"Created New Todo ({data.Text}, {data.Complete})\"); return req.CreateResponse(HttpStatusCode.OK); } Note the Function Url at the top of the page. You will need to copy and paste this later on. You can test the Function in isolation by putting the following in the Request body panel: { \"Text\": \"test\", \"Complete\": true } When you click the Run button, the log should show the Info line and the Output should show a 200 OK You can now turn your attention to the mobile backend. I use a Webhook.cs helper: using System; using System.Net; using System.Net.Http; using System.Threading.Tasks; namespace Backend.Helpers { public static class Webhook { public static async Task<HttpStatusCode> SendAsync<T>(Uri uri, T data) { var httpClient = new HttpClient(); httpClient.BaseAddress = uri; var response = await httpClient.PostAsJsonAsync<T>(\"\", data); return response.StatusCode; } } } This allows me to call the Webhook in my TodoItemController.cs method like this: public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); #pragma warning disable CS4014 Webhook.SendAsync<TodoItem>(new Uri(webhookUri), current); #pragma warning restore CS4014 return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } You will see the Webhook is called when the value is inserted. We don't await the SendAsync<TodoItem>() call because we don't want the process to be held up while the webhook is running. You will notice that the response is sometimes returned to the user before the webhook is executed.","title":"Recipes"},{"location":"chapter4/recipes/#storage-related-operations","text":"When dealing with cloud concepts, there are multiple operating levels one can think about. At the bottom layer is Infrastructure as a Service . Most people think of this as the Virtual Machine layer, but it also incorporates basic networking and storage concepts. As you move to higher level services, you gain a lot of efficiencies by adding software components, you lose a lot of the potential management headaches, but you also lose flexibility in what you can do to the platform. At the top of the stack is Software as a Service . You may be running a helpdesk, for example, but you are completely isolated from what operating system is being run, what web services are being run, APIs that can be accessed and language that is used. Azure Mobile Apps is an opinionated combination of a client and server SDK running on top of a standard ASP.NET based web service and is normally thought of as being a Platform as a Service . You get to choose what database to use, what tables to expose, and what programming language to use. You don't get to determine when the operating system is patched or what patches are applied. It's a middle of the road between SaaS and IaaS. That isn't to say we can't dip down sometimes to deal with lower level cloud services, nor to access higher level SaaS APIs. One of those times is when dealing with files. Storage is conceptually easy - you have an amount of disk and you can store files on it. However, the management of that storage is complicated. Placing that storage at the service of a scalable web application is similarly complicated. What we intend to do is develop a set of skills that make developing storage based mobile applications easy.","title":"Storage Related Operations"},{"location":"chapter4/recipes/#blobs-table-queues-and-files","text":"At the top of my list of \"storage made complicated\" is the cloud storage concepts. In the old days, we stored files on a file system and we didn't really have to worry about differing types of storage, redundancy and capabilities. Cloud storage tends to come in multiple flavors: The base storage type is Blob Storage . Put simply, you have containers (roughly analogous to directories) and blobs (roughly analogous to files). It's the cheapest form of storage and is used for many things, including the underlying storage for virtual machine disks. Blob storage has many advantages. From a mobile perspective, developers will appreciate the upload/download restart capabilities within the SDK. We've already introduced Table Storage in the last chapter . It is more analogous to a NoSQL store for storing key / attribute values. It has a schemaless design, so you can store basic JSON objects. However, it has limited query capabilities, as we discussed in the last chapter. That makes it unsuited to large scale query-driven applications. You may think you want Files Storage . This provides an SMB interface to the storage layer. You would use Files Storage if you want to browse files from your PC or Mac as you can mount the file system directly from Azure Storage. Finally, Queue Storage provides cloud messaging between application components. We'll get onto Azure Functions later on, during our look at Custom API. Queue Storage will definitely be coming into play then. Think of Queue Storage as the glue that ties work flow components together. The real question is when should you use File Storage and when should you use Blob Storage. For more applications, Blob Storage is going to save you money over File Storage, so it's pretty much always the better choice. You should only be thinking of File Storage if you have other components of your system that need to access the data you upload that can only access that data via an SMB interface. If you need to explore the data that you upload or download, you can use the Azure Storage Explorer as a standalone application or you can use the Cloud Explorer in Visual Studio .","title":"Blobs, Table, Queues and files"},{"location":"chapter4/recipes/#creating-and-linking-a-storage-account","text":"Before we can use storage, we need to set up a storage account and connect it to our environment. This involves: Create a Resource Group Create an Azure App Service Set up authentication on the Azure App Service Create a Storage Account Link the Storage Account to the Azure App Service. We've already covered the first three items in previous chapters. We've also created a storage account and linked it to the mobile backend during our look at the Storage Domain Manager . To create a Storage Account: Log on to the Azure portal . Click the big + NEW button in the top left corner. Click Data + Storage , then Storage account . Fill in the form: The name can only contain letters and numbers and must be unique. A GUID without the dashes is a good choice. The Deployment model should be set to Resource manager . The Account kind should be set to General purpose . The Performance should be set to Standard for this example. The Replication should be set to Locally-redundant storage (LRS) . Set the Resource group to your existing resource group. Set the Location to the same location as your App Service. Click Create . Just like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. For example, you have the option of Premium Storage - this provides all-SSD storage that has a large IOPS performance number. You can also decide how redundant you want the storage. Azure always keeps 3 copies of your data. You can choose to increase the number of copies and decide whether the additional copies will be in the same datacenter, another datacenter in the same region or another region. We have selected the slowest performance and least redundant options here to keep the cost down on your service. Warn There is no \"free\" option for Azure Storage. You pay by the kilobyte depending on the performance and redundancy selected. Once the Azure Storage account is deployed, you can link the storage account to your App Service: Open your App Service in the Azure portal . Click Data Connections under the MOBILE section in the settings menu. Click + ADD In the Add data connection blade: Set the Type to Storage . Click the Storage link. In the Storage Account selector, click the storage account you just created. Click the Connection string . In the Connection string selector, make a note of the Name field. Click OK . Click OK to close the Add data connection blade. Click on the Application Settings menu option, then scroll down to the Connection Strings section. Note that the portal has created the connection string as an App Setting for you with the right value: DefaultEndpointsProtocol=https;AccountName=thebook;AccountKey=<key1> By default, the connection string is called MS_AzureStorageAccountConnectionString and we will use that throughout our examples. The key is the access key for the storage. When a storage account is created, two keys are also created. The keys are used for secure access to the storage area. You should never distribute the storage keys nor check them into source control. If you feel they have been compromised, you should regenerate them. There are two keys for this purpose. The process of regeneration is: Regenerate KEY2 Place the regenerated KEY2 in the connection string and restart your App Service. Regenerate key1 Place the regenerated KEY1 in the connection string and restart your App Service. In this way, your App Service will always be using KEY1 except during regeneration. You can avoid the restart of your App Service by providing a management interface that sets the Account Key for the App Service. Tip For local development, there is the Azure Storage Emulator . The connection string when using the Azure Storage Emulator is UseDevelopmentStorage=true . It's normal to add the storage connection string to the Web.config file with the following: <connectionStrings> <add name=\"MS_AzureStorageAccountConnectionString\" connectionString=\"UseDevelopmentStorage=true\" /> </connectionStrings> This will be overwritten by the connection string in the App Service Application Settings. Effectively, you will be using the Azure Storage Emulator during local development and Azure Storage when you deploy to Azure App Service.","title":"Creating and Linking a Storage Account"},{"location":"chapter4/recipes/#the-shared-access-signature-sas","text":"The storage account key is kind of like the root or Administrator password. You should always protect it, never send it to a third party and regenerate it on a regular basis. You avoid storing the storage account key in source code by linking the storage account to the App Service. The key is stored in the connection string instead. You should never ship an account key to your mobile account. The Azure Storage SDK already has many of the features that you want in handling file upload and download. Azure Storage is optimized for streaming, for example. You can upload or download blobs in blocks, allowing you to restart the transfer and provide feedback to the user on progress, for example. You will inevitably be drawn to having your mobile client interact with Azure Storage directly rather than having an intermediary web service for this reason. If you want to interact with Azure Storage directly and you shouldn't give out the account key, how do you deal with the security of the service? The answer is with a Shared Access Signature, or SAS. The Service SAS delegates access to just a single resource in one of the storage services (Blob, Table, Queue or File service). Info There is also an Account SAS which delegates access to resources in more than one service. You generally don't want this in application development. A service SAS is a URI that is used when accessing the resource. It consists of the URI to the resource followed by a SAS token. The SAS token is an cryptographically signed opaque token that the storage service decodes. Minimally, it provides an expiry time and the permissions being granted to the SAS. Warn A SAS token ALWAYS expires. There is no way to produce a permanent SAS token. If you think you need one, think again. In mobile development, you NEVER want a non-expiring token. Accessing Azure Storage is always done with a specific version of the REST API and that follows through to the SDK. You should always request a SAS token for the appropriate API you are going to be using. We'll cover the various methods of obtaining a SAS later in the chapter.","title":"The Shared Access Signature (SAS)"},{"location":"chapter4/recipes/#uploading-a-file","text":"The most normal tasks for dealing with files are the upload and download of files to blob storage. There is a natural and consistent process to this which makes this recipe very repeatable. First, deal with the things you need before you start: Create an Azure Storage Account and link it to your Azure App Service. Decide how you want your files organized. Create a WebAPI to generate a SAS token for your upload or download. Blob storage is organized in a typical directory structure. Each directory is called a container, and each file is a blob. In the examples for this section, I am going to store each uploaded file in a container based on the authenticated user. My WebAPI will create the appropriate container and then return an appropriate SAS token. We can set up our custom API as follows: namespace Backend.Controllers { [Authorize] [MobileappController] public class GetStorageTokenController : ApiController { private const string connString = \"CUSTOMCONNSTR_MS_AzureStorageAccountConnectionString\"; public GetStorageTokenController() { ConnectionString = Environment.GetEnvironmentVariable(connString); StorageAccount = CloudStorageAccount.Parse(ConnectionString); BlobClient = StorageAccount.CreateCloudBlobClient(); } public string ConnectionString { get; } public CloudStorageAccount StorageAccount { get; } public CloudBlobClient BlobClient { get; } } } The ConnectionString property is the pointer to where the Azure Storage account is located and how to access it. the StorageAccount is a reference to that Azure Storage account. Finally, the BlobClient is an object used for accessing blob storage. We can access any WebAPI methods in this class by using the endpoint /api/GetStorageToken within our mobile client or using Postman. Azure Storage doesn't have a true heirarchial container system. It does have containers and directories to organize things though, so we are going to use that: private const string containerName = \"userdata\"; [HttpGet] public async Task<StorageTokenViewModel> GetAsync() { // The userId is the SID without the sid: prefix var claimsPrincipal = User as ClaimsPrincipal; var userId = claimsPrincipal .FindFirst(ClaimTypes.NameIdentifier) .Value.Substring(4); // Errors creating the storage container result in a 500 Internal Server Error var container = BlobClient.GetContainerReference(containerName); await container.CreateIfNotExistsAsync(); // Get the user directory within the container var directory = container.GetDirectoryReference(userId); var blobName = Guid.NewGuid().ToString(\"N\"); var blob = directory.GetBlockBlobReference(blobName); // Create a policy for accessing the defined blob var blobPolicy = new SharedAccessBlobPolicy { SharedAccessStartTime = DateTime.UtcNow.AddMinutes(-5), SharedAccessExpiryTime = DateTime.UtcNow.AddMinutes(60), Permissions = SharedAccessBlobPermissions.Read | SharedAccessBlobPermissions.Write | SharedAccessBlobPermissions.Create }; return new StorageTokenViewModel { Name = blobName, Uri = blob.Uri, SasToken = blob.GetSharedAccessSignature(blobPolicy) }; } The main piece of work in this API is generating the policy that is then signed and returned to the user as the SAS Token. The mobile device has permission to read, write and create the blob that we have defined for the next 60 minutes. I've provided a policy that starts in the past in case there is a little amount of clock-skew between the mobile device and the backend. Warn Container names must be a valid DNS name. The most notable requirement here is between 3 and 64 lower-case letters. Container names are case-sensitive. Check the documentation for full details on naming requirements. The StorageTokenViewModel is used for serialization purposes: public class StorageTokenViewModel { public string Name { get; set; } public Uri Uri { get; set; } public string SasToken { get; set; } } We can test this API using Postman. First, generate an authentication token. Then use Postman to do a GET of the /api/GetStorageToken endpoint: There are two pieces of information we need here. Firstly, the uri property provides the URI that we are going to use to upload the file. Secondly, the sasToken is appended to the uri when uploading to provide a link to the policy. Note that the token start and expiry time are encoded and readable in the sasToken. In real world applications, this is likely not the right method. We might want to organize the files based on information that the mobile client provides us, for example. We may also want to upload to a specific upload area and then download from another location, allowing processing of the files in between. You may also want to append the uploaded file extension to the file before uploading. There is no \"one size fits all\" token policy. You must decide on the conditions under which you will allow upload and download capabilities and then provide the appropriate logic to generate the SAS token.","title":"Uploading a File"},{"location":"chapter4/recipes/#the-mobile-client","text":"Once we have the logic to generate a SAS token, we can turn our attention to the mobile clients. We need to do three things for uploading a file to the service: Get a reference to the file (as a Stream object). Generate a SAS token using the custom API. Use the Azure Storage SDK to upload directly to the Azure Storage Account. You should not upload to a custom API in your mobile backend. This needlessly ties up your mobile backend, causing your mobile backend to be less efficient at scaling. Your mobile backend will not have all the facilities that the Azure Storage endpoint has provided either. Azure Storage provides upload and download restarts and progress bar capabilities. Obtaining a reference to the file that you wish to upload is normally a per-platform API. Obtaining a reference to a photo or video involves interacting with platform-specific APIs to provide access to camera and built-in photo storage capabilities on the phone. To support such a per-platform capability, we need to add an interface for the API to the Abstractions\\IPlatform.cs file: Task<Stream> GetUploadFileAsync(); This API will interact with whatever photo sharing API is available on the device, open the requested file and return a standard Stream object. Loading a media file is made much simpler using the cross-platform Xamarin Media plugin . This plugin allows the user to take photos or video, or pick the media file from a gallery. It's available on NuGet, so add the Xam.Plugin.Media plugin to each of the platform-specific projects. Tip I still like separating out code that deals with the hardware of a mobile device into the platform-specific code. You don't need to do such separation on this project. I find that I inevitably have one thing or another that requires a platform-specific tweak, so starting with a platform-specific API is better. The Xamarin Media plugin is used like this: await CrossMedia.Current.Initialize(); var file = await CrossMedia.Current.PickPhotoAsync(); var stream = file.GetStream(); There are methods within the plugin to determine if a camera is available. Different platforms require different permissions:","title":"The Mobile Client"},{"location":"chapter4/recipes/#android","text":"Android requires the WRITE_EXTERNAL_STORAGE , READ_EXTERNAL_STORAGE and CAMERA permissions. If the mobile device is running Android M or later, the plugin will automatically prompt the user for runtime permissions. You can set these permissions within Visual Studio: Double-click the Properties node within the Android project. Select Android Manifest . In the Required permissions list, check the box next to the required permissions by double-clicking the permission. Save the Properties (you may have to right-click on the TaskList.Droid tab and click on Save Selected Items ).","title":"Android"},{"location":"chapter4/recipes/#ios","text":"Apple iOS requires the NSCameraUsageDescription and NSPhotoLibraryUsageDescription keys. The string provided will be displayed to the user when they are prompted to provide permission. You can set these keys within Visual Studio: Right-click on the Info.plist file and select Open with... Choose the XML (Text) Editor then click OK . Within the <dict> node, add the following lines: <key>NSCameraUsageDescription</key> <string>This app needs access to the camera to take photos.</string> <key>NSPhotoLibraryUsageDescription</key> <string>This app needs access to photos.</string> Save and close the file. You can choose whatever string you want to display to the user. For more information on iOS 10 privacy permissions, review the Xamarin Blog .","title":"iOS"},{"location":"chapter4/recipes/#universal-windows","text":"Universal Windows may require the Pictures Library capability: In the TaskList.UWP (Universal Windows) project, open Package.appxmanifest . Select the Capabilities tab. Check the box next to Pictures Library . Save the manifest.","title":"Universal Windows"},{"location":"chapter4/recipes/#implementing-the-file-reader","text":"The same code can be used in all three platform-specific projects, in the *Platform.cs file: /// <summary> /// Picks a photo for uploading /// </summary> /// <returns>A Stream for the photo</returns> public async Task<Stream> GetUploadFileAsync() { var mediaPlugin = CrossMedia.Current; var mainPage = Xamarin.Forms.Application.Current.MainPage; await mediaPlugin.Initialize(); if (mediaPlugin.IsPickPhotoSupported) { var mediaFile = await mediaPlugin.PickPhotoAsync(); return mediaFile.GetStream(); } else { await mainPage.DisplayAlert(\"Media Service Unavailable\", \"Cannot pick photo\", \"OK\"); return null; } }","title":"Implementing the File Reader"},{"location":"chapter4/recipes/#uploading-a-file_1","text":"We can now put the individual pieces together to actually do an upload. In this example, we are going to use the photo picker to pick a photo and then upload it, displaying a progress bar as it happens. We start with the XAML code in Pages\\TaskList.xaml . We need a button in the toolbar to initiate the file upload: <ContentPage.ToolbarItems> <ToolbarItem Name=\"Refresh\" Command=\"{Binding RefreshCommand}\" Icon=\"refresh.png\" Order=\"Primary\" Priority=\"0\" /> <ToolbarItem Name=\"Add Task\" Command=\"{Binding AddNewItemCommand}\" Icon=\"add.png\" Order=\"Primary\" Priority=\"0\" /> <ToolbarItem Name=\"Add File\" Command=\"{Binding AddNewFileCommand}\" Icon=\"addfile.png\" Order=\"Primary\" Priority=\"0\" /> </ContentPage.ToolbarItems> Obtain a suitable \"Add File\" icon from the Internet and resize the image appropriately for the task. You will need five images total: TaskList.Droid\\Resources\\drawable\\addfile.png should be 128x128 pixels TaskList.iOS\\Resources\\addfile.png should be 25x25 pixels TaskList.iOS\\Resources\\addfile@2x.png should be 50x50 pixels TaskList.iOS\\Resources\\addfile@3x.png should be 75x75 pixels TaskList.UWP\\addfile.png should be 128x128 pixels All images should have a transparent background. The storage token is retrieved from the backend via the cloud service. Add the following to Abstractions\\ICloudService.cs : // Custom APIs Task<StorageTokenViewModel> GetSasTokenAsync(); This has a concrete implementation in Services\\AzureCloudService.cs : public async Task<StorageTokenViewModel> GetSasTokenAsync() { var parameters = new Dictionary<string, string>(); var storageToken = await Client.InvokeApiAsync<StorageTokenViewModel>(\"GetStorageToken\", HttpMethod.Get, parameters); return storageToken; } The StorageTokenViewModel is identical to the class in the GetStorageTokenController.cs controller in the Backend. I've placed the class definition in the Models namespace for the client. We could share this model between the backend and front end, but the case of sharing models is so rare I tend not to share the code. In the TaskListViewModel.cs , we can define a command that is called when the Add File button is clicked: /// <summary> /// Reference to the Platform Provider /// </summary> public IPlatform PlatformProvider => DependencyService.Get<IPlatform>(); /// <summary> /// Bindable property for the AddNewFile Command /// </summary> public ICommand AddNewFileCommand { get; } /// <summary> /// User clicked on the Add New File button /// </summary> private async Task AddNewFileAsync() { if (IsBusy) { return; } IsBusy = true; try { // Get a stream for the file var mediaStream = await PlatformProvider.GetUploadFileAsync(); if (mediaStream == null) { IsBusy = false; return; } // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(); // Use the SAS token to upload the file var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); await blobStorage.UploadFromStreamAsync(mediaStream); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Error Uploading File\", ex.Message, \"OK\"); } finally { IsBusy = false; } } Warn Azure Storage SDK support for PCL projects is only available in -preview editions. When installing the SDK, ensure you check the \"Include prerelease\" box in the NuGet package manager. The latest version with PCL (.NETPortable) support is v7.0.2-preview. You can look at the uploaded files in the Azure portal: Log in to the Azure portal . Click All resources , then your storage account. Under SERVICES , click Blobs . Click your storage container (in this example, that's called userdata ) You will see a folder for each user account. The folder is named for the SID of the account - not the username. It's a good idea to store the user SID with other data about the user in a table within your database. This allows you to associate a real user with their SID since a user will never know what their SID is.","title":"Uploading a File"},{"location":"chapter4/recipes/#implementing-a-progress-bar","text":"It's common to want to see the progress of the upload while it is happening. For that, we need a progress bar. First, let's add a hidden progress bar to our TaskList.xaml page: <ActivityIndicator HorizontalOptions=\"FillAndExpand\" IsRunning=\"{Binding IsBusy}\" IsVisible=\"{Binding IsBusy}\" VerticalOptions=\"Start\" /> <ProgressBar x:Name=\"fileUploadProgress\" HeightRequest=\"3\" HorizontalOptions=\"FillAndExpand\" IsVisible=\"{Binding IsUploadingFile}\" Progress=\"{Binding FileProgress}\" /> This comes with two new bindable properties. IsUploadingFile is a bool and FileProgress is a Double . FileProgress takes a value between 0 and 1 to indicate how far along the progress bar should be. This code should be in the TaskListViewModel.cs file: private bool isUploadingFile; public bool IsUploadingFile { get { return isUploadingFile; } set { SetProperty(ref isUploadingFile, value, \"IsUploadingFile\"); } } private Double fileProgress = 0.0; public Double FileProgress { get { return fileProgress; } set { SetProperty(ref fileProgress, value, \"FileProgress\"); } } Finally, we have to change the upload so that it happens a chunk at a time. In the AddNewFileAsync() method, we can replace the upload code with this: /// <summary> /// User clicked on the Add New File button /// </summary> private async Task AddNewFileAsync() { if (IsBusy) { return; } IsBusy = true; try { // Get a stream for the file var mediaStream = await PlatformProvider.GetUploadFileAsync(); if (mediaStream == null) { IsBusy = false; return; } // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(); // Use the SAS token to get a reference to the blob storage var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); // Get the length of the stream var mediaLength = mediaStream.Length; // Initialize the blocks int bytesInBlock = 1024; // The number of bytes in a single block var buffer = new byte[bytesInBlock]; // The buffer to hold the data during transfer int totalBytesRead = 0; // The number of bytes read from the stream. int bytesRead = 0; // The number of bytes read per block. int blocksWritten = 0; // The # Blocks Written IsUploadingFile = true; FileProgress = 0.00; // Loop through until we have processed the whole file do { // Read a block from the media stream bytesRead = mediaStream.Read(buffer, 0, bytesInBlock); if (bytesRead > 0) { // Move the buffer into a memory stream using (var memoryStream = new MemoryStream(buffer, 0, bytesRead)) { string blockId = GetBlockId(blocksWritten); await blobStorage.PutBlockAsync(blockId, memoryStream, null); } // Update the internal counters totalBytesRead += bytesRead; blocksWritten++; // Update the progress bar FileProgress = totalBytesRead / mediaLength; } } while (bytesRead > 0); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Error Uploading File\", ex.Message, \"OK\"); } finally { IsBusy = false; IsUploadingFile = false; FileProgress = 0.0; } } /// <summary> /// Convert the Block ID to the string we need /// </summary> /// <param name=\"block\"></param> /// <returns></returns> private string GetBlockId(int block) { char[] tempID = new char[6]; string iStr = block.ToString(); for (int j = tempID.Length - 1; j > (tempID.Length - iStr.Length - 1); j--) { tempID[j] = iStr[tempID.Length - j - 1]; } byte[] blockIDBeforeEncoding = Encoding.UTF8.GetBytes(tempID); return Convert.ToBase64String(blockIDBeforeEncoding); } The main work is done in the inner loop. We split the media stream into 1024 byte blocks. Each block is copied into a temporary buffer then transferred to cloud storage. After each block is delivered, the FileProgress counter is updated which updates the progress bar. One of the secrets for doing block-based streaming uploads is the GetBlockId() method. This properly formats the block ID (based on a rather convoluted method that ends up being Base-64 encoded). If you do not get this right, you will instead get a rather cryptic message about the query parameter for the HTTP request being wrong.","title":"Implementing a Progress bar"},{"location":"chapter4/recipes/#downloading-a-file","text":"You can similarly download a file from blob storage. The basics (such as the SAS token generator) are exactly the same as before. To do the basic form: // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(filename); // Use the SAS token to get a reference to the blob storage var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); // Get a stream for the blob file var mediaStream = await blobStorage.OpenReadAsync(); // Do something with the mediaStream - like move it to storage await PlatformProvider.StoreFileAsync(mediaStream); // At the end, close the stream properly mediaStream.Dispose(); Similarly, you can also produce a progress bar: // Get the SAS token from the backend var storageToken = await CloudService.GetSasTokenAsync(filename); // Use the SAS token to get a reference to the blob storage var storageUri = new Uri($\"{storageToken.Uri}{storageToken.SasToken}\"); var blobStorage = new CloudBlockBlob(storageUri); var mediaStream = await blobStorage.OpenReadAsync(); var mediaLength = mediaStream.Length; byte[] buffer = new byte[1024]; var bytesRead = 0, totalBytesRead = 0; // Do what you need to for opening your output file do { bytesRead = mediaStream.ReadAsync(buffer, 0, 1024); if (bytesRead > 0) { // Do something with the buffer totalBytesRead += bytesRead; FileProgress = totalBytesRead / mediaLength; } } while (bytesRead > 0); // Potentially close your output file as well mediaStream.Dispose(); When downloading, you will need to update the GetStorageTokenController method to provide access to files. One possibility is to provide read/write access to the entire container, allowing the mobile device to get a directory listing for browsing, for example. You need to decide what permissions your mobile client needs, and provide a SAS token with those permissions. Primarily, this is done by altering the policy that is generated when you retrieve the SAS token.","title":"Downloading a File"},{"location":"chapter4/recipes/#table-controllers-and-webhooks","text":"I love writing asynchronous applications. One of the four features I mentioned with Table Controllers is the Webhook. In essence, if someone inserts, updates or deletes a record, you may want to do something asynchronously. For example, you may want to do sentiment analysis on the record that was just uploaded, or perhaps execute some custom code to simulate an offline custom API. A Webhook is a callback mechanism whereby the controller will do an HTTP POST when something happens. It's an event processing system over HTTP. In this sample, we are going to generate a simple Webhook function that logs the inserted record, then adjust our table controller to call the Webhook when an insert happens. Let's first of all create a Function that handles the request. Create a Function App, and then create a new function based on the Generic Webhook - CSharp template. Call this function InsertTodoItemWebhook . You can call this whatever you want, but the URI of your Webhook is based on the name of your function. Replace the body of the function with the following: #r \"Newtonsoft.Json\" using System; using System.Net; using Newtonsoft.Json; public static async Task<object> Run(HttpRequestMessage req, TraceWriter log) { string jsonContent = await req.Content.ReadAsStringAsync(); dynamic data = JsonConvert.DeserializeObject(jsonContent); log.Info($\"Created New Todo ({data.Text}, {data.Complete})\"); return req.CreateResponse(HttpStatusCode.OK); } Note the Function Url at the top of the page. You will need to copy and paste this later on. You can test the Function in isolation by putting the following in the Request body panel: { \"Text\": \"test\", \"Complete\": true } When you click the Run button, the log should show the Info line and the Output should show a 200 OK You can now turn your attention to the mobile backend. I use a Webhook.cs helper: using System; using System.Net; using System.Net.Http; using System.Threading.Tasks; namespace Backend.Helpers { public static class Webhook { public static async Task<HttpStatusCode> SendAsync<T>(Uri uri, T data) { var httpClient = new HttpClient(); httpClient.BaseAddress = uri; var response = await httpClient.PostAsJsonAsync<T>(\"\", data); return response.StatusCode; } } } This allows me to call the Webhook in my TodoItemController.cs method like this: public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); #pragma warning disable CS4014 Webhook.SendAsync<TodoItem>(new Uri(webhookUri), current); #pragma warning restore CS4014 return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } You will see the Webhook is called when the value is inserted. We don't await the SendAsync<TodoItem>() call because we don't want the process to be held up while the webhook is running. You will notice that the response is sometimes returned to the user before the webhook is executed.","title":"Table Controllers and Webhooks"},{"location":"chapter4/webjobs/","text":"I'd like to think that my mobile backend is self-managing. Just like the App Service it is running on, it cleans up after itself, always works and never needs a backup or maintenance. Alas, some things can't be encoded in a client to server transaction. For example, in chapter 3 , we discussed offline synchronization and the need for soft delete. The soft delete process just marks the records as deleted. In a busy database, those deleted records still take up space and resources during searches. One of the maintenance tasks we need to do is to clean up the database. There is almost always a need for backend processing that is not related to the client-server communication. That is where Azure Functions and WebJobs come in. They handle the custom processing that is not initiated by a mobile client. There are several examples aside from the aforementioned soft delete cleanup: You upload files and want to process them before letting other users download them. You want to do sentiment analysis or other machine learning on incoming database records. You need to do workflows like order fulfillment. You need to handle data securely and cannot transmit it to the mobile client for processing. In all these cases, the initiation may come from the mobile client or it may be scheduled. However, the defining characteristic of these requirements is that the code must be run asynchronously (the mobile client is not waiting for the result) and may take longer than the time for a normal request. You can consider Azure Functions as \"WebJobs as a Service\". WebJobs run on the same set of virtual machines that are running your mobile backend. They have access to the same resources (like CPU and memory), so they can interfere with the running of your app. Azure Functions can run this way as well, but they really take off when running in Dynamic Compute mode (which is the default). In Dynamic Compute, they run in spare compute power and potentially on a completely different set of virtual machines, so they don't interfere with your mobile backend. I recommend WebJobs for clean-up or maintenance tasks - things like cleaning up the database on a regular basis. Event-driven tasks should be handled by Azure Functions. A Database Clean Up WebJob \u00b6 As an example WebJob, let's implement the database clean-up process as a WebJob. This will run on a regular basis (say, once a day) and during processing, it will delete all records in our TodoItem table that are deleted where the updatedAt field is older than 7 days. The SQL command to run is this: DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] < DATEADD(day, -7, SYSDATETIMEOFFSET()) GO A WebJob is always a separate project from your mobile backend. To create a WebJob: Right click the solution. Choose Add -> New Item... . Search for WebJob . Select Azure WebJob with a Visual C# language. Enter a suitable name for the WebJob and click Add . Once the scaffolding has finished and the package restore is done, you will see a Program.cs file. A WebJob is just a console application running with the WebJobs SDK. By default, the WebJob will wait around for a trigger. We don't have a trigger since we are going to run this code each and every time the scheduler runs it. As a result, our code in Program.cs is relatively simple: using System; using System.Configuration; using System.Data.SqlClient; using System.Diagnostics; namespace CleanupDatabaseWebJob { class Program { static void Main() { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_TableConnectionString\"].ConnectionString; using (SqlConnection sqlConnection = new SqlConnection(connectionString)) { using (SqlCommand sqlCommand = sqlConnection.CreateCommand()) { Console.WriteLine(\"[CleanupDatabaseWebJob] Initiating SQL Connection\"); sqlConnection.Open(); Console.WriteLine(\"[CleanupDatabaseWebJob] Executing SQL Statement\"); sqlCommand.CommandText = \"DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] < DATEADD(day, -7, SYSDATETIMEOFFSET())\"; var rowsAffected = sqlCommand.ExecuteNonQuery(); Console.WriteLine($\"[CleanupDatabaseWebJob] {rowsAffected} rows deleted.\"); sqlConnection.Close(); } } } } } Debug messages that you want captured in the logs must be output with Console.WriteLine . The Debug channel is not captured in the logs. You must also define the connection string in the App.config file of the WebJob project. The scaffolding creates two connection strings, but they are not normally defined in Azure App Service. The following defines the MS_TableConnectionString that is used by the mobile backend: <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"\" /> <add name=\"AzureWebJobsDashboard\" connectionString=\"\"/> <add name=\"AzureWebJobsStorage\" connectionString=\"\"/> </connectionStrings> This is also defined (in the same way) in the Web.config file for the mobile backend. Note the two extra connection strings. These are for running WebJobs, so you always need a connected storage account to run WebJobs. WebJobs will create a number of blob containers in your storage account. These containers all start with azure-jobs or azure-webjobs . You should not mess with these in any way as they are required for WebJobs functionality. To set up the connection string, create a storage account. Once you have created the storage account: Open the Storage account blade. Click Access keys in the menu. Right click the triple dot on the same line as key1 and choose View connection string . Copy the connection string, then click OK . Close the Storage account blade. Open the App Service blade for your mobile backend. Click Application settings in the menu. Scroll down to the Connection Strings section and add a new connection string: Enter AzureWebJobsDashboard for the connection string name. Paste the connection string you copied earlier into the value. Select Custom for the type of connection string. Repeat this process to create the AzureWebJobsStorage connection string. Click Save at the top of the blade. To deploy a WebJob, we need to link it to our mobile backend: Right click the Backend project. Select Add then Existing Project as Azure WebJob . Fill in the Project name and WebJob name (if you named your project differently). In WebJob run mode , select Run on Demand . This creates two files: The webjobs-list.json in the Backend project. The webjob-publish-settings.json in the WebJob project. Visual Studio 2017 does not provide for running WebJobs on a scheduled basis. You need to edit the webjob-publish-settings.json file. For instance, to configure the WebJob as a scheduled job running once a day at 3am, use the following: { \"$schema\": \"http://schemastore.org/schemas/json/webjob-publish-settings.json\", \"webJobName\": \"CleanupDatabaseWebJob\", \"startTime\": \"2016-10-15T03:00:00-08:00\", \"endTime\": \"2016-10-16T04:00:00-08:00\", \"jobRecurrenceFrequency\": \"Day\", \"interval\": 1, \"runMode\": \"Scheduled\" } The limits for the ending time ensures that your WebJob doesn't run forever. You may have a WebJob that runs a complicated report each night to consolidate a lot of data into something that can be downloaded by your mobile client. These reports can sometimes run for hours. You should place reasonable limits on your WebJob to ensure that they don't affect the operation of your site. Right click your Backend project and select Publish... Warn WebJobs use the App_Data area. Ensure your publish profile does not delete files in the App_Data area. Since you have published before, the dialog will be on the Preview tab. Click on the Preview button to generate a view of what would happen: The WebJob has been bundled with your mobile backend for publication. If you log into the portal, there are several things you can do. Go to your mobile backend. In the menu, select WebJobs to see your configured WebJob. You can trigger a run of the WebJob independently of the schedule that you configured when you linked the WebJob to the mobile backend. You can also view the logs for the WebJob. Info You will see an additional resource group when you created a scheduler WebJob. This holds the Azure Scheduler resource that you will use. Azure Scheduler has a free tier which includes up to 3,600 job executions per month and 5 jobs. Ensure you delete the scheduler if you are not using it. An Image Resizing WebJob \u00b6 Scheduled WebJobs are great for report generation and maintenance tasks, but WebJobs can also be run continuously. In a continuous mode, they wait for some trigger and then execute the code on that. This is where the WebJobs SDK comes in. Let's take an example. In our recipes section, we have a method of uploading a photo to blob storage. However, we don't have any in-built methods of controlling what is uploaded. The Azure Storage SDK will allow any file, so the user can upload a file that is not an image, or they can upload an image that is too big. We can do something about this by running the following logic when a file is uploaded to the userdata area: If the file is an image: Resize the image to 800px x 600px. Store the new file in the publicdata area. Delete the file from the userdata area. The monitoring is done lazily and depends on lots of factors (including how busy the storage account is). It may be several minutes before the service notices that a file is available. To accomplish this, I am going to do the processing of the file in one trigger function and the deletion in another, joining the two together via an Azure Storage queue. My first step is to adjust the WebJob App.config to include the storage connection string: <connectionStrings> <add name=\"MS_AzureStorageAccountConnectionString\", connectionString=\"\" /> <add name=\"AzureWebJobsDashboard\" connectionString=\"\"/> <add name=\"AzureWebJobsStorage\" connectionString=\"\"/> </connectionStrings> My main program is in Program.cs as follows: using Microsoft.Azure.WebJobs; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; using Microsoft.WindowsAzure.Storage.Queue; using System.Configuration; namespace ImageResizeWebJob { class Program { static void Main() { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_AzureStorageAccountConnectionString\"].ConnectionString; CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudQueueClient queueClient = storageAccount.CreateCloudQueueClient(); // Ensure the userdata area exists CloudBlobContainer userdata = blobClient.GetContainerReference(\"userdata\"); userdata.CreateIfNotExists(); // Ensure the publicdata area exists CloudBlobContainer publicdata = blobClient.GetContainerReference(\"publicdata\"); publicdata.CreateIfNotExists(); // Ensure the delete queue exists CloudQueue queue = queueClient.GetQueueReference(\"delete\"); queue.CreateIfNotExists(); // Start running the WebJob var host = new JobHost(); host.RunAndBlock(); } } } We must ensure that the areas we are monitoring to trigger activity exist before the WebJob starts processing. If we don't, there is nothing to trigger. The WebJob will not find the appropriate hooks for monitoring and the WebJob will likely not run. Tip For more information on creating Azure Storage, see the recipes section of this chapter. The actual trigger methods (called \"functions\") are located in the Functions class in the default namespace of the WebJob. All trigger methods are static methods with the following basic signature: public static void ImageUploaded( [BlobTrigger(\"userdata/{name}.{ext}\")] Stream input, string name, string ext, [Queue(\"delete\")] out string path) The first parameter is always the trigger. In this case, I am triggering when a blob upload is complete. A trigger can optionally be parameterized as it is in this case. The trigger parameters come next. In this case, I am extracting the filename and extension from the path of the uploaded file. The final parameter is the output binding. For this trigger, I am going to write the path to be deleted to the delete queue so it can be picked up by another function. using Microsoft.Azure.WebJobs; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; using System.Configuration; using System.Diagnostics; using System.Drawing; using System.Drawing.Drawing2D; using System.Drawing.Imaging; using System.IO; namespace ImageResizeWebJob { public class Functions { static int requiredHeight = 600; static int requiredWidth = 800; public static void ImageUploaded( [BlobTrigger(\"userdata/{name}.{ext}\")] Stream input, string name, string ext, [Queue(\"delete\")] out string path) { if (!ext.ToLowerInvariant().Equals(\"png\")) { Debug.WriteLine($\"BlobTrigger: userdata/{name}.{ext} - not a PNG file (skipping)\"); path = $\"{name}.{ext}\"; return; } // Read the blob stream into an Image object var image = Image.FromStream(input); // Process the image object if (image.Height > requiredHeight || image.Width > requiredWidth) { var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight); var destImage = new Bitmap(requiredWidth, requiredHeight); destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution); using (var graphics = Graphics.FromImage(destImage)) { graphics.CompositingMode = CompositingMode.SourceCopy; graphics.CompositingQuality = CompositingQuality.Default; graphics.InterpolationMode = InterpolationMode.Bicubic; graphics.SmoothingMode = SmoothingMode.Default; graphics.PixelOffsetMode = PixelOffsetMode.Default; using (var wrapMode = new ImageAttributes()) { wrapMode.SetWrapMode(WrapMode.TileFlipXY); graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode); } } // Replace the original image with the bitmap we created image = destImage; } // Write the image out to the publicdata area using (var stream = new MemoryStream()) { image.Save(stream, ImageFormat.Png); stream.Position = 0; SaveFileToPublicBlob($\"{name}.{ext}\", stream); } // Write the original path to the queue for deletion path = $\"{name}.{ext}\"; } static void SaveFileToPublicBlob(string file, Stream input) { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_AzureStorageAccountConnectionString\"].ConnectionString; CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer publicdata = blobClient.GetContainerReference(\"publicdata\"); CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(file); blockBlob.UploadFromStream(input); } } } You are always going to write custom code in the middle of a function. In this case, it's my image resizing code. Once the code is run, I write out the blob and put the path on the deletion queue. Tip To learn more about image resizing in C#, this Stack Overflow question has some great details. Processing the queue is much easier than the image resizing: public static void ProcessDeleteQueue([QueueTrigger(\"delete\")] string path) { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_AzureStorageAccountConnectionString\"].ConnectionString; CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer userdata = blobClient.GetContainerReference(\"userdata\"); CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(path); blockBlob.DeleteIfExists(); } You can connect this web job to your mobile backend project in the same way. The WebJob run mode should be set to Run Continuously instead of the schedule setting we used prior. Tip Azure App Service will turn off idle sites when they are not receiving requests. If you have a WebJob that needs to run irrespective of whether the App Service is receiving client requests, ensure the Always On setting is turned on. This setting is in Application settings . To test the WebJob after publication: Connect an Azure Storage account to your mobile backend. You can find instructions in the recipes section. In the mobile backend, select WebJobs , then your WebJob and click Start . In Visual Studio, select the Cloud Explorer . If you cannot see the Cloud Explorer, you can start it by selecting the View menu. Expand the Storage Accounts node > your storage account > Blob Containers > userdata . Right click userdata and select Open Blob Container Editor . Click the Upload Blob icon (it looks like a black up arrow). Upload a suitable file for testing the process. Benefits and Drawbacks of WebJobs \u00b6 WebJobs are great for long running async batch processing and maintenance tasks. When considering if WebJobs are right for you: They are included with your App Service Plan. They scale with your App Service Plan. They are resilient to errors. They are easy to run in isolation and debug. They are managed and deployed with your mobile backend project. However, They may make your App Service use more resources than planned. They can cause an automatic scale event on your App Service.","title":"WebJobs"},{"location":"chapter4/webjobs/#a-database-clean-up-webjob","text":"As an example WebJob, let's implement the database clean-up process as a WebJob. This will run on a regular basis (say, once a day) and during processing, it will delete all records in our TodoItem table that are deleted where the updatedAt field is older than 7 days. The SQL command to run is this: DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] < DATEADD(day, -7, SYSDATETIMEOFFSET()) GO A WebJob is always a separate project from your mobile backend. To create a WebJob: Right click the solution. Choose Add -> New Item... . Search for WebJob . Select Azure WebJob with a Visual C# language. Enter a suitable name for the WebJob and click Add . Once the scaffolding has finished and the package restore is done, you will see a Program.cs file. A WebJob is just a console application running with the WebJobs SDK. By default, the WebJob will wait around for a trigger. We don't have a trigger since we are going to run this code each and every time the scheduler runs it. As a result, our code in Program.cs is relatively simple: using System; using System.Configuration; using System.Data.SqlClient; using System.Diagnostics; namespace CleanupDatabaseWebJob { class Program { static void Main() { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_TableConnectionString\"].ConnectionString; using (SqlConnection sqlConnection = new SqlConnection(connectionString)) { using (SqlCommand sqlCommand = sqlConnection.CreateCommand()) { Console.WriteLine(\"[CleanupDatabaseWebJob] Initiating SQL Connection\"); sqlConnection.Open(); Console.WriteLine(\"[CleanupDatabaseWebJob] Executing SQL Statement\"); sqlCommand.CommandText = \"DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] < DATEADD(day, -7, SYSDATETIMEOFFSET())\"; var rowsAffected = sqlCommand.ExecuteNonQuery(); Console.WriteLine($\"[CleanupDatabaseWebJob] {rowsAffected} rows deleted.\"); sqlConnection.Close(); } } } } } Debug messages that you want captured in the logs must be output with Console.WriteLine . The Debug channel is not captured in the logs. You must also define the connection string in the App.config file of the WebJob project. The scaffolding creates two connection strings, but they are not normally defined in Azure App Service. The following defines the MS_TableConnectionString that is used by the mobile backend: <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"\" /> <add name=\"AzureWebJobsDashboard\" connectionString=\"\"/> <add name=\"AzureWebJobsStorage\" connectionString=\"\"/> </connectionStrings> This is also defined (in the same way) in the Web.config file for the mobile backend. Note the two extra connection strings. These are for running WebJobs, so you always need a connected storage account to run WebJobs. WebJobs will create a number of blob containers in your storage account. These containers all start with azure-jobs or azure-webjobs . You should not mess with these in any way as they are required for WebJobs functionality. To set up the connection string, create a storage account. Once you have created the storage account: Open the Storage account blade. Click Access keys in the menu. Right click the triple dot on the same line as key1 and choose View connection string . Copy the connection string, then click OK . Close the Storage account blade. Open the App Service blade for your mobile backend. Click Application settings in the menu. Scroll down to the Connection Strings section and add a new connection string: Enter AzureWebJobsDashboard for the connection string name. Paste the connection string you copied earlier into the value. Select Custom for the type of connection string. Repeat this process to create the AzureWebJobsStorage connection string. Click Save at the top of the blade. To deploy a WebJob, we need to link it to our mobile backend: Right click the Backend project. Select Add then Existing Project as Azure WebJob . Fill in the Project name and WebJob name (if you named your project differently). In WebJob run mode , select Run on Demand . This creates two files: The webjobs-list.json in the Backend project. The webjob-publish-settings.json in the WebJob project. Visual Studio 2017 does not provide for running WebJobs on a scheduled basis. You need to edit the webjob-publish-settings.json file. For instance, to configure the WebJob as a scheduled job running once a day at 3am, use the following: { \"$schema\": \"http://schemastore.org/schemas/json/webjob-publish-settings.json\", \"webJobName\": \"CleanupDatabaseWebJob\", \"startTime\": \"2016-10-15T03:00:00-08:00\", \"endTime\": \"2016-10-16T04:00:00-08:00\", \"jobRecurrenceFrequency\": \"Day\", \"interval\": 1, \"runMode\": \"Scheduled\" } The limits for the ending time ensures that your WebJob doesn't run forever. You may have a WebJob that runs a complicated report each night to consolidate a lot of data into something that can be downloaded by your mobile client. These reports can sometimes run for hours. You should place reasonable limits on your WebJob to ensure that they don't affect the operation of your site. Right click your Backend project and select Publish... Warn WebJobs use the App_Data area. Ensure your publish profile does not delete files in the App_Data area. Since you have published before, the dialog will be on the Preview tab. Click on the Preview button to generate a view of what would happen: The WebJob has been bundled with your mobile backend for publication. If you log into the portal, there are several things you can do. Go to your mobile backend. In the menu, select WebJobs to see your configured WebJob. You can trigger a run of the WebJob independently of the schedule that you configured when you linked the WebJob to the mobile backend. You can also view the logs for the WebJob. Info You will see an additional resource group when you created a scheduler WebJob. This holds the Azure Scheduler resource that you will use. Azure Scheduler has a free tier which includes up to 3,600 job executions per month and 5 jobs. Ensure you delete the scheduler if you are not using it.","title":"A Database Clean Up WebJob"},{"location":"chapter4/webjobs/#an-image-resizing-webjob","text":"Scheduled WebJobs are great for report generation and maintenance tasks, but WebJobs can also be run continuously. In a continuous mode, they wait for some trigger and then execute the code on that. This is where the WebJobs SDK comes in. Let's take an example. In our recipes section, we have a method of uploading a photo to blob storage. However, we don't have any in-built methods of controlling what is uploaded. The Azure Storage SDK will allow any file, so the user can upload a file that is not an image, or they can upload an image that is too big. We can do something about this by running the following logic when a file is uploaded to the userdata area: If the file is an image: Resize the image to 800px x 600px. Store the new file in the publicdata area. Delete the file from the userdata area. The monitoring is done lazily and depends on lots of factors (including how busy the storage account is). It may be several minutes before the service notices that a file is available. To accomplish this, I am going to do the processing of the file in one trigger function and the deletion in another, joining the two together via an Azure Storage queue. My first step is to adjust the WebJob App.config to include the storage connection string: <connectionStrings> <add name=\"MS_AzureStorageAccountConnectionString\", connectionString=\"\" /> <add name=\"AzureWebJobsDashboard\" connectionString=\"\"/> <add name=\"AzureWebJobsStorage\" connectionString=\"\"/> </connectionStrings> My main program is in Program.cs as follows: using Microsoft.Azure.WebJobs; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; using Microsoft.WindowsAzure.Storage.Queue; using System.Configuration; namespace ImageResizeWebJob { class Program { static void Main() { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_AzureStorageAccountConnectionString\"].ConnectionString; CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudQueueClient queueClient = storageAccount.CreateCloudQueueClient(); // Ensure the userdata area exists CloudBlobContainer userdata = blobClient.GetContainerReference(\"userdata\"); userdata.CreateIfNotExists(); // Ensure the publicdata area exists CloudBlobContainer publicdata = blobClient.GetContainerReference(\"publicdata\"); publicdata.CreateIfNotExists(); // Ensure the delete queue exists CloudQueue queue = queueClient.GetQueueReference(\"delete\"); queue.CreateIfNotExists(); // Start running the WebJob var host = new JobHost(); host.RunAndBlock(); } } } We must ensure that the areas we are monitoring to trigger activity exist before the WebJob starts processing. If we don't, there is nothing to trigger. The WebJob will not find the appropriate hooks for monitoring and the WebJob will likely not run. Tip For more information on creating Azure Storage, see the recipes section of this chapter. The actual trigger methods (called \"functions\") are located in the Functions class in the default namespace of the WebJob. All trigger methods are static methods with the following basic signature: public static void ImageUploaded( [BlobTrigger(\"userdata/{name}.{ext}\")] Stream input, string name, string ext, [Queue(\"delete\")] out string path) The first parameter is always the trigger. In this case, I am triggering when a blob upload is complete. A trigger can optionally be parameterized as it is in this case. The trigger parameters come next. In this case, I am extracting the filename and extension from the path of the uploaded file. The final parameter is the output binding. For this trigger, I am going to write the path to be deleted to the delete queue so it can be picked up by another function. using Microsoft.Azure.WebJobs; using Microsoft.WindowsAzure.Storage; using Microsoft.WindowsAzure.Storage.Blob; using System.Configuration; using System.Diagnostics; using System.Drawing; using System.Drawing.Drawing2D; using System.Drawing.Imaging; using System.IO; namespace ImageResizeWebJob { public class Functions { static int requiredHeight = 600; static int requiredWidth = 800; public static void ImageUploaded( [BlobTrigger(\"userdata/{name}.{ext}\")] Stream input, string name, string ext, [Queue(\"delete\")] out string path) { if (!ext.ToLowerInvariant().Equals(\"png\")) { Debug.WriteLine($\"BlobTrigger: userdata/{name}.{ext} - not a PNG file (skipping)\"); path = $\"{name}.{ext}\"; return; } // Read the blob stream into an Image object var image = Image.FromStream(input); // Process the image object if (image.Height > requiredHeight || image.Width > requiredWidth) { var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight); var destImage = new Bitmap(requiredWidth, requiredHeight); destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution); using (var graphics = Graphics.FromImage(destImage)) { graphics.CompositingMode = CompositingMode.SourceCopy; graphics.CompositingQuality = CompositingQuality.Default; graphics.InterpolationMode = InterpolationMode.Bicubic; graphics.SmoothingMode = SmoothingMode.Default; graphics.PixelOffsetMode = PixelOffsetMode.Default; using (var wrapMode = new ImageAttributes()) { wrapMode.SetWrapMode(WrapMode.TileFlipXY); graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode); } } // Replace the original image with the bitmap we created image = destImage; } // Write the image out to the publicdata area using (var stream = new MemoryStream()) { image.Save(stream, ImageFormat.Png); stream.Position = 0; SaveFileToPublicBlob($\"{name}.{ext}\", stream); } // Write the original path to the queue for deletion path = $\"{name}.{ext}\"; } static void SaveFileToPublicBlob(string file, Stream input) { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_AzureStorageAccountConnectionString\"].ConnectionString; CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer publicdata = blobClient.GetContainerReference(\"publicdata\"); CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(file); blockBlob.UploadFromStream(input); } } } You are always going to write custom code in the middle of a function. In this case, it's my image resizing code. Once the code is run, I write out the blob and put the path on the deletion queue. Tip To learn more about image resizing in C#, this Stack Overflow question has some great details. Processing the queue is much easier than the image resizing: public static void ProcessDeleteQueue([QueueTrigger(\"delete\")] string path) { var connectionString = ConfigurationManager.ConnectionStrings[\"MS_AzureStorageAccountConnectionString\"].ConnectionString; CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString); CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient(); CloudBlobContainer userdata = blobClient.GetContainerReference(\"userdata\"); CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(path); blockBlob.DeleteIfExists(); } You can connect this web job to your mobile backend project in the same way. The WebJob run mode should be set to Run Continuously instead of the schedule setting we used prior. Tip Azure App Service will turn off idle sites when they are not receiving requests. If you have a WebJob that needs to run irrespective of whether the App Service is receiving client requests, ensure the Always On setting is turned on. This setting is in Application settings . To test the WebJob after publication: Connect an Azure Storage account to your mobile backend. You can find instructions in the recipes section. In the mobile backend, select WebJobs , then your WebJob and click Start . In Visual Studio, select the Cloud Explorer . If you cannot see the Cloud Explorer, you can start it by selecting the View menu. Expand the Storage Accounts node > your storage account > Blob Containers > userdata . Right click userdata and select Open Blob Container Editor . Click the Upload Blob icon (it looks like a black up arrow). Upload a suitable file for testing the process.","title":"An Image Resizing WebJob"},{"location":"chapter4/webjobs/#benefits-and-drawbacks-of-webjobs","text":"WebJobs are great for long running async batch processing and maintenance tasks. When considering if WebJobs are right for you: They are included with your App Service Plan. They scale with your App Service Plan. They are resilient to errors. They are easy to run in isolation and debug. They are managed and deployed with your mobile backend project. However, They may make your App Service use more resources than planned. They can cause an automatic scale event on your App Service.","title":"Benefits and Drawbacks of WebJobs"},{"location":"chapter5/android/","text":"Push notifications for Android devices are handled by Firebase Cloud Messaging - a service run by Google. This service used to be called Google Cloud Messaging and nothing has really changed since they rebranded the service. Google wanted to bundle all their mobile offerings under one roof. Firebase Cloud Messaging can still be used independently of the rest of Firebase. Preparing for development \u00b6 Warn Push notifications are one of those areas where it really pays to have a real device instead of an emulator. It's frustrating to bump into so many issues with emulation, but it's almost inevitable. If you are having problems, use a real device. Before continuing, you will need an Android Emulator with the Google Play SDKs installed, or a real Android device. You cannot use a vanilla emulator without the Google Play SDKs. To set up an emulator with the appropriate SDKs. Visual Studio 2017 Visual Studio 2017 provides four emulators and all of them include the Google APIs already, so you can skip this section if you are using Visual Studio 2017. You still need to create an appropriate emulator if you are using Visual Studio 2015 or earlier. In Visual Studio: Click Tools -> Android -> Android SDK Manager . Expand Android 6.0 (API 23) . Select Google APIs Intel x86 Atom System Image . Expand the Extras . Select Google Play Services . Click on Install . Wait for the installation to complete, then close the Android SDK Manager. Disable Hyper-V The Android Virtual Device from Google is incompatible with Hyper-V. To disable Hyper-V, open a PowerShell prompt as an Administrator, then run the command bcdedit /set hypervisorlaunchtype off and reboot. You may also have to install Intel HAXM , which is available for download through the Android SDK Manager but may need to be installed separately, depending on how you installed the Android SDK. Click Tools -> Android -> Android Emulator Manager . Click Create... Fill in the form. You must specify: Target: Android 6.0 - API Level 23 CPU/ABI: Google APIs Intel Atom (x86) Memory Options: RAM: 768 Other than these settings, there is a lot of flexibility. Consult the Android documentation . Once ready, click on OK to create the device. Click OK to confirm the creation. Test that the emulator device works: Click the device you just created. Click Start... . Click Launch... . Fixing Permission Denied You may get an error for ...\\/system.img: Permission denied . To fix this, open up a File Explorer and go to C:\\Program Files (x86)\\Android . Right-click on android-sdk and select Properties . Click the Security tab, then Edit . Highlight Users in the Group or user names box, select Allow Full control in the Permissions for Users box, then click OK . Once the Security settings have been applied, you can try to start the emulator again. If the device starts and looks like a regular Android device, then you've completed the task. You must have a working emulator or real device before continuing, so don't continue until you've got something working. Registering your app with FCM \u00b6 To start, you need a Firebase Developer Account. Go to the Firebase Developer Console and sign in with a Google account. If you have never been a Google developer before, the site will ask you to agree to their legal terms so your account can be converted to a developer account. Once done, create a Firebase application. If you previously created a Google project for authentication, you can import the Google project instead. The effect is the same - you need a Firebase project at the end. Click on Add Firebase to your Android app . You aren't actually adding Firebase - just the push capabilities. The next screen is confusing - it's talking about Android native development and we are developing in Xamarin Forms. We need to enter a namespace, so use the namespace of your application. The actual value does not matter as we are not using the majority of the Firebase SDK: Click on ADD APP , then on CONTINUE , and finally FINISH . The process will download a file: google-services.json , which is used by Android Studio in native applications. The instructions along the way are also for Android Studio. Once done, click on the cog next to your project name and select PROJECT SETTINGS . Click on the CLOUD MESSAGING tab: This gives you a server key and a sender ID. You need the \"Legacy Server Key\" if you have two keys listed. We will need these later. Linking Notification Hubs to FCM \u00b6 Now that you have the server key and sender ID, you can enter that information into Notification Hubs to enable it to push to your Android clients. Log in to the Azure portal . Find your App Service: Use All Resources , then enter the name in the Filter items... box. Use Resource Groups , find the resource group, then click on the items. Which ever way you choose, enter the App Service. Select Push from the menu (under SETTINGS ). Click Configure push notification services . Click Google (GCM) . Enter the server key in the API Key box. Click on Save . We can now turn our attention to the mobile client. Registering for Push Notifications \u00b6 Registering for push notification is always a per-platform piece, so it has to go into the platform specific code. We've seen what this means in terms of code before. First, we create a new method definition in the IPlatformProvider.cs interface and the ICloudService.cs interface, then we update the AzureCloudService.cs to call the platform-specific code. Finally, we need an platform-specific implementation. First, the IPlatformProvider.cs - I'm going to add a new method: RegisterForPushNotifications() that will do all the work for me: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; namespace TaskList.Abstractions { public interface IPlatformProvider { MobileServiceUser RetrieveTokenFromSecureStore(); void StoreTokenInSecureStore(MobileServiceUser user); void RemoveTokenFromSecureStore(); Task<MobileServiceUser> LoginAsync(MobileServiceClient client); Task RegisterForPushNotifications(MobileServiceClient client); } } The first four methods are from our authentication work. The final method is our new method. There is a similar method in the ICloudService.cs interface: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Models; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task<MobileServiceUser> LoginAsync(); Task LogoutAsync(); Task<AppServiceIdentity> GetIdentityAsync(); Task RegisterForPushNotifications(); } } We also have a concrete implementation of this in AzureCloudService.cs that simply calls the platform specific version: public async Task RegisterForPushNotifications() { var platformProvider = DependencyService.Get<IPlatformProvider>(); await platformProvider.RegisterForPushNotifications(client); } These are required irrespective of whether you are implementing iOS, Android, UWP or any combination of those platforms. Let's now work with the Android platform-specific code. Before we look at the platform specific code, we are going to need a library that implements the GCM/FCM library. Alternative Libraries We are going to use a fairly old and venerable GCM Client. There is an official Xamarin client for Google Play Services for example. Feel free to experiment with other libraries. The process for registration tends to be very similar between SDKs. Right-click on Components in the TaskList.Droid project. Select Get More Components... . Enter Google Cloud Messaging Client in the search box. Select the Google Cloud Messaging Client . Click Add to App . Now that you have the library installed, you can configure registration with FCM as follows: public void Init(Context context) { RootView = context; AccountStore = AccountStore.Create(context); try { // Check to see if this client has the right permissions GcmClient.CheckDevice(RootView); GcmClient.CheckManifest(RootView); // Register for push GcmClient.Register(RootView, GcmHandler.SenderId); Debug.WriteLine($\"GcmClient: Registered for push with FCM: {GcmClient.GetRegistrationId(RootView)}\"); } catch (Exception ex) { Debug.WriteLine($\"GcmClient: Cannot register for push: {ex.Message}\"); } } Most of this Init() method existed before. The GcmClient calls are new. The first two calls ensure that the mobile device is capable of handling registrations and that the application is properly configured. The GcmClient.Register() call registers with FCM. I'm dumping the registration ID we get back to the debug channel. We also need to define a handler for the FCM calls. FCM will send an out-of-band notification to the Android OS, which will figure out which application to wake up, and then call the defined handler. Mine is in Services\\GcmHandler.cs : using Android.App; using Android.Content; using Android.Util; using Gcm.Client; [assembly: Permission(Name = \"@PACKAGE_NAME@.permission.C2D_MESSAGE\")] [assembly: UsesPermission(Name = \"@PACKAGE_NAME@.permission.C2D_MESSAGE\")] [assembly: UsesPermission(Name = \"com.google.android.c2dm.permission.RECEIVE\")] [assembly: UsesPermission(Name = \"android.permission.INTERNET\")] [assembly: UsesPermission(Name = \"android.permission.WAKE_LOCK\")] namespace TaskList.Droid.Services { [BroadcastReceiver(Permission = Constants.PERMISSION_GCM_INTENTS)] [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_MESSAGE }, Categories = new string[] { \"@PACKAGE_NAME@\" })] [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_REGISTRATION_CALLBACK }, Categories = new string[] { \"@PACKAGE_NAME@\" })] [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_LIBRARY_RETRY }, Categories = new string[] { \"@PACKAGE_NAME@\" })] public class GcmHandler : GcmBroadcastReceiverBase<GcmService> { // Replace with your Sender ID from the Firebase Console public static string[] SenderId = new string[] { \"493509995880\" }; } [Service] public class GcmService : GcmServiceBase { public static string RegistrationID { get; private set; } public GcmService() : base(GcmHandler.SenderId) { } protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); } protected override void OnError(Context context, string errorId) { Log.Error(\"GcmService\", $\"ERROR: {errorId}\"); } protected override void OnRegistered(Context context, string registrationId) { Log.Info(\"GcmService\", $\"Registered: {registrationId}\"); GcmService.RegistrationID = registrationId; } protected override void OnUnRegistered(Context context, string registrationId) { Log.Info(\"GcmService\", $\"Unregistered device from FCM\"); GcmService.RegistrationID = null; } } } Replace the SenderId You must replace the SenderId with your sender ID that you copied from the Firebase Console. Let's start at the top. In order to use push notifications from Firebase, we need to tell our application to ask for that permission. We need the OS to wake us up (WAKE_LOCK), access the Internet (INTERNET), and handle push notifications (C2D_MESSAGE and RECEIVE). We use the GcmHandler to register a receiver. This is done via the intents that are provided by the GcmClient package. The class that will receive the events is the GcmService class. There are four methods that must be defined there - registration, un-registration, messages and errors. Right now, I'm just setting up some debug messages so I can see what is going on. We can test this right now. Place a breakpoint on each Log method, then run the app. The application could not be started If you get the error \"The application could not be started. Ensure that the application has been installed to the target device and has a launchable activity (MainLauncher=true)\". This is because the Android system can't handle package names in upper case. Right-click the TaskList.Droid project and select Properties then Android Manifest . Change the Package name field to something lower case. Once the application launches, the GcmService.OnRegistered() method is hit immediately. You will be able to see the registration ID. Click on Continue . The device is now registered with FCM. Note the registration ID as you will need it in the next step. Send an ad-hoc message as follows: Go to the Firebase Developer Console . Click your project. Click Notifications in the left hand nav. Click Send your first message . Enter something in the Message text box. Select Single device under Target. Paste the registration ID in the GCM registration token box. Click Send Message . Click Send . At this point the breakpoint in GcmService.OnMessage() will be hit. You can examine the push by looking at the intent variable. Click on Stop to stop the application. Moving onto registration with Notification Hubs, we need to pass the registration ID we received from FCM to our mobile backend. The App Service will ensure our device is registered properly. We need to do this at the appropriate time, and that depends on a lot of factors. There is no problem with registering multiple times if our requirements change. In this app, I might choose to register at the beginning of the app, once the user has authenticated and if I had a settings page, when the settings changed. This activity is done in the Services\\DroidPlatformProvider.cs file: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (GcmClient.IsRegistered(RootView)) { try { var registrationId = GcmClient.GetRegistrationId(RootView); //var push = client.GetPush(); //await push.RegisterAsync(registrationId); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"gcm\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request PushTemplate genericTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"message\"\":\"\"$(message)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var response = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); } catch (Exception ex) { Log.Error(\"DroidPlatformProvider\", $\"Could not register with NH: {ex.Message}\"); } } else { Log.Error(\"DroidPlatformProvider\", $\"Not registered with FCM\"); } } Registering without Tags You can also register without tags using the commented-out single line of code push.RegisterAsync(registrationId); You should call RegisterForPushNotifications() whenever you feel that the definition of the push endpoint should change. In my application, I added the registration after the LoginAsync() method in the ViewModels\\EntryPageViewModel.cs file: async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(); await cloudService.RegisterForPushNotifications(); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Login Failed\", ex.Message, \"OK\"); } finally { IsBusy = false; } } Run the application again, with the same breakpoint in the GcmService.OnMessage() method. You can remove the other breakpoints at this point. Log into the application this time. Let's explore some of the debugging tools for Notification Hubs. Within Visual Studio, you can use View -> Server Explorer to open the server explorer. Expand the Azure node then the Notification Hubs node: Double-click on your Notification Hub to open the developer console. This provides two functions. Firstly, we can click on the Device Registrations tab: We can see the registration of our test emulator device. Note that our request for the topic:Sports tag has also been honored. If we did not configure that tag within the Push blade in the portal, that would not have been added to our registration. We can also send to a specific device using the test send facility. Click over to the Test Send facility. Since we only have one device, we can use broadcast. Each installation will also be given a tag: $InstallationId:{guid} , where {guid} is the installation ID. Select Google (GCM) -> Default to send a message to FCM. The body will be filled in for you. Since we have already set a breakpoint at the OnMessage() method in GcmService.cs , our app is running and we have entered the app and logged in, click Send . The breakpoint should be triggered within a reasonable amount of time. I'd like to say that it will be near instantaneous, but push notifications may take some time depending on what is happening within the push notification system at the time. The push should not take more than a couple of minutes to arrive and will likely arrive much quicker. We now have the full registration lifecycle working and we can do a test send to hit the right piece of code. Processing a Push Notification \u00b6 Processing of push notifications is done within your mobile app, so you can process the push notifications however you want. For example, you may want to silently pull a specific record from the server and insert it into your SQLite offline cache when a push arrives, or you may want to pop up a message that opens the mobile app. In this example, we are going to show the message that comes in the message field of the data block. There are more examples in the recipes section . The OnMessage() method in GcmService.cs is triggered on a push. A simple notification looks like this: protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); var message = intent.Extras.GetString(\"message\"); var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager; var uiIntent = new Intent(context, typeof(MainActivity)); NotificationCompat.Builder builder = new NotificationCompat.Builder(context); var notification = builder.SetContentIntent(PendingIntent.GetActivity(context, 0, uiIntent, 0)) .SetSmallIcon(Android.Resource.Drawable.SymDefAppIcon) .SetTicker(\"TaskList\") .SetContentTitle(\"TaskList\") .SetContentText(message) .SetSound(RingtoneManager.GetDefaultUri(RingtoneType.Notification)) .SetAutoCancel(true) .Build(); notificationManager.Notify(1, notification); } The major thing to note here is how we get the contents of the message. The data block from the Notification Hub is received by the Intent into the Extras property. If you have other properties in that block, you can retrieve them the same way. The message field is standard, but you can pass other things. An example would be to pass the table name and ID of an inserted record. [5]:","title":"Android Push"},{"location":"chapter5/android/#preparing-for-development","text":"Warn Push notifications are one of those areas where it really pays to have a real device instead of an emulator. It's frustrating to bump into so many issues with emulation, but it's almost inevitable. If you are having problems, use a real device. Before continuing, you will need an Android Emulator with the Google Play SDKs installed, or a real Android device. You cannot use a vanilla emulator without the Google Play SDKs. To set up an emulator with the appropriate SDKs. Visual Studio 2017 Visual Studio 2017 provides four emulators and all of them include the Google APIs already, so you can skip this section if you are using Visual Studio 2017. You still need to create an appropriate emulator if you are using Visual Studio 2015 or earlier. In Visual Studio: Click Tools -> Android -> Android SDK Manager . Expand Android 6.0 (API 23) . Select Google APIs Intel x86 Atom System Image . Expand the Extras . Select Google Play Services . Click on Install . Wait for the installation to complete, then close the Android SDK Manager. Disable Hyper-V The Android Virtual Device from Google is incompatible with Hyper-V. To disable Hyper-V, open a PowerShell prompt as an Administrator, then run the command bcdedit /set hypervisorlaunchtype off and reboot. You may also have to install Intel HAXM , which is available for download through the Android SDK Manager but may need to be installed separately, depending on how you installed the Android SDK. Click Tools -> Android -> Android Emulator Manager . Click Create... Fill in the form. You must specify: Target: Android 6.0 - API Level 23 CPU/ABI: Google APIs Intel Atom (x86) Memory Options: RAM: 768 Other than these settings, there is a lot of flexibility. Consult the Android documentation . Once ready, click on OK to create the device. Click OK to confirm the creation. Test that the emulator device works: Click the device you just created. Click Start... . Click Launch... . Fixing Permission Denied You may get an error for ...\\/system.img: Permission denied . To fix this, open up a File Explorer and go to C:\\Program Files (x86)\\Android . Right-click on android-sdk and select Properties . Click the Security tab, then Edit . Highlight Users in the Group or user names box, select Allow Full control in the Permissions for Users box, then click OK . Once the Security settings have been applied, you can try to start the emulator again. If the device starts and looks like a regular Android device, then you've completed the task. You must have a working emulator or real device before continuing, so don't continue until you've got something working.","title":"Preparing for development"},{"location":"chapter5/android/#registering-your-app-with-fcm","text":"To start, you need a Firebase Developer Account. Go to the Firebase Developer Console and sign in with a Google account. If you have never been a Google developer before, the site will ask you to agree to their legal terms so your account can be converted to a developer account. Once done, create a Firebase application. If you previously created a Google project for authentication, you can import the Google project instead. The effect is the same - you need a Firebase project at the end. Click on Add Firebase to your Android app . You aren't actually adding Firebase - just the push capabilities. The next screen is confusing - it's talking about Android native development and we are developing in Xamarin Forms. We need to enter a namespace, so use the namespace of your application. The actual value does not matter as we are not using the majority of the Firebase SDK: Click on ADD APP , then on CONTINUE , and finally FINISH . The process will download a file: google-services.json , which is used by Android Studio in native applications. The instructions along the way are also for Android Studio. Once done, click on the cog next to your project name and select PROJECT SETTINGS . Click on the CLOUD MESSAGING tab: This gives you a server key and a sender ID. You need the \"Legacy Server Key\" if you have two keys listed. We will need these later.","title":"Registering your app with FCM"},{"location":"chapter5/android/#linking-notification-hubs-to-fcm","text":"Now that you have the server key and sender ID, you can enter that information into Notification Hubs to enable it to push to your Android clients. Log in to the Azure portal . Find your App Service: Use All Resources , then enter the name in the Filter items... box. Use Resource Groups , find the resource group, then click on the items. Which ever way you choose, enter the App Service. Select Push from the menu (under SETTINGS ). Click Configure push notification services . Click Google (GCM) . Enter the server key in the API Key box. Click on Save . We can now turn our attention to the mobile client.","title":"Linking Notification Hubs to FCM"},{"location":"chapter5/android/#registering-for-push-notifications","text":"Registering for push notification is always a per-platform piece, so it has to go into the platform specific code. We've seen what this means in terms of code before. First, we create a new method definition in the IPlatformProvider.cs interface and the ICloudService.cs interface, then we update the AzureCloudService.cs to call the platform-specific code. Finally, we need an platform-specific implementation. First, the IPlatformProvider.cs - I'm going to add a new method: RegisterForPushNotifications() that will do all the work for me: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; namespace TaskList.Abstractions { public interface IPlatformProvider { MobileServiceUser RetrieveTokenFromSecureStore(); void StoreTokenInSecureStore(MobileServiceUser user); void RemoveTokenFromSecureStore(); Task<MobileServiceUser> LoginAsync(MobileServiceClient client); Task RegisterForPushNotifications(MobileServiceClient client); } } The first four methods are from our authentication work. The final method is our new method. There is a similar method in the ICloudService.cs interface: using System.Threading.Tasks; using Microsoft.WindowsAzure.MobileServices; using TaskList.Models; namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; Task<MobileServiceUser> LoginAsync(); Task LogoutAsync(); Task<AppServiceIdentity> GetIdentityAsync(); Task RegisterForPushNotifications(); } } We also have a concrete implementation of this in AzureCloudService.cs that simply calls the platform specific version: public async Task RegisterForPushNotifications() { var platformProvider = DependencyService.Get<IPlatformProvider>(); await platformProvider.RegisterForPushNotifications(client); } These are required irrespective of whether you are implementing iOS, Android, UWP or any combination of those platforms. Let's now work with the Android platform-specific code. Before we look at the platform specific code, we are going to need a library that implements the GCM/FCM library. Alternative Libraries We are going to use a fairly old and venerable GCM Client. There is an official Xamarin client for Google Play Services for example. Feel free to experiment with other libraries. The process for registration tends to be very similar between SDKs. Right-click on Components in the TaskList.Droid project. Select Get More Components... . Enter Google Cloud Messaging Client in the search box. Select the Google Cloud Messaging Client . Click Add to App . Now that you have the library installed, you can configure registration with FCM as follows: public void Init(Context context) { RootView = context; AccountStore = AccountStore.Create(context); try { // Check to see if this client has the right permissions GcmClient.CheckDevice(RootView); GcmClient.CheckManifest(RootView); // Register for push GcmClient.Register(RootView, GcmHandler.SenderId); Debug.WriteLine($\"GcmClient: Registered for push with FCM: {GcmClient.GetRegistrationId(RootView)}\"); } catch (Exception ex) { Debug.WriteLine($\"GcmClient: Cannot register for push: {ex.Message}\"); } } Most of this Init() method existed before. The GcmClient calls are new. The first two calls ensure that the mobile device is capable of handling registrations and that the application is properly configured. The GcmClient.Register() call registers with FCM. I'm dumping the registration ID we get back to the debug channel. We also need to define a handler for the FCM calls. FCM will send an out-of-band notification to the Android OS, which will figure out which application to wake up, and then call the defined handler. Mine is in Services\\GcmHandler.cs : using Android.App; using Android.Content; using Android.Util; using Gcm.Client; [assembly: Permission(Name = \"@PACKAGE_NAME@.permission.C2D_MESSAGE\")] [assembly: UsesPermission(Name = \"@PACKAGE_NAME@.permission.C2D_MESSAGE\")] [assembly: UsesPermission(Name = \"com.google.android.c2dm.permission.RECEIVE\")] [assembly: UsesPermission(Name = \"android.permission.INTERNET\")] [assembly: UsesPermission(Name = \"android.permission.WAKE_LOCK\")] namespace TaskList.Droid.Services { [BroadcastReceiver(Permission = Constants.PERMISSION_GCM_INTENTS)] [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_MESSAGE }, Categories = new string[] { \"@PACKAGE_NAME@\" })] [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_REGISTRATION_CALLBACK }, Categories = new string[] { \"@PACKAGE_NAME@\" })] [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_LIBRARY_RETRY }, Categories = new string[] { \"@PACKAGE_NAME@\" })] public class GcmHandler : GcmBroadcastReceiverBase<GcmService> { // Replace with your Sender ID from the Firebase Console public static string[] SenderId = new string[] { \"493509995880\" }; } [Service] public class GcmService : GcmServiceBase { public static string RegistrationID { get; private set; } public GcmService() : base(GcmHandler.SenderId) { } protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); } protected override void OnError(Context context, string errorId) { Log.Error(\"GcmService\", $\"ERROR: {errorId}\"); } protected override void OnRegistered(Context context, string registrationId) { Log.Info(\"GcmService\", $\"Registered: {registrationId}\"); GcmService.RegistrationID = registrationId; } protected override void OnUnRegistered(Context context, string registrationId) { Log.Info(\"GcmService\", $\"Unregistered device from FCM\"); GcmService.RegistrationID = null; } } } Replace the SenderId You must replace the SenderId with your sender ID that you copied from the Firebase Console. Let's start at the top. In order to use push notifications from Firebase, we need to tell our application to ask for that permission. We need the OS to wake us up (WAKE_LOCK), access the Internet (INTERNET), and handle push notifications (C2D_MESSAGE and RECEIVE). We use the GcmHandler to register a receiver. This is done via the intents that are provided by the GcmClient package. The class that will receive the events is the GcmService class. There are four methods that must be defined there - registration, un-registration, messages and errors. Right now, I'm just setting up some debug messages so I can see what is going on. We can test this right now. Place a breakpoint on each Log method, then run the app. The application could not be started If you get the error \"The application could not be started. Ensure that the application has been installed to the target device and has a launchable activity (MainLauncher=true)\". This is because the Android system can't handle package names in upper case. Right-click the TaskList.Droid project and select Properties then Android Manifest . Change the Package name field to something lower case. Once the application launches, the GcmService.OnRegistered() method is hit immediately. You will be able to see the registration ID. Click on Continue . The device is now registered with FCM. Note the registration ID as you will need it in the next step. Send an ad-hoc message as follows: Go to the Firebase Developer Console . Click your project. Click Notifications in the left hand nav. Click Send your first message . Enter something in the Message text box. Select Single device under Target. Paste the registration ID in the GCM registration token box. Click Send Message . Click Send . At this point the breakpoint in GcmService.OnMessage() will be hit. You can examine the push by looking at the intent variable. Click on Stop to stop the application. Moving onto registration with Notification Hubs, we need to pass the registration ID we received from FCM to our mobile backend. The App Service will ensure our device is registered properly. We need to do this at the appropriate time, and that depends on a lot of factors. There is no problem with registering multiple times if our requirements change. In this app, I might choose to register at the beginning of the app, once the user has authenticated and if I had a settings page, when the settings changed. This activity is done in the Services\\DroidPlatformProvider.cs file: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (GcmClient.IsRegistered(RootView)) { try { var registrationId = GcmClient.GetRegistrationId(RootView); //var push = client.GetPush(); //await push.RegisterAsync(registrationId); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"gcm\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request PushTemplate genericTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"message\"\":\"\"$(message)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var response = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); } catch (Exception ex) { Log.Error(\"DroidPlatformProvider\", $\"Could not register with NH: {ex.Message}\"); } } else { Log.Error(\"DroidPlatformProvider\", $\"Not registered with FCM\"); } } Registering without Tags You can also register without tags using the commented-out single line of code push.RegisterAsync(registrationId); You should call RegisterForPushNotifications() whenever you feel that the definition of the push endpoint should change. In my application, I added the registration after the LoginAsync() method in the ViewModels\\EntryPageViewModel.cs file: async Task ExecuteLoginCommand() { if (IsBusy) return; IsBusy = true; try { var cloudService = ServiceLocator.Instance.Resolve<ICloudService>(); await cloudService.LoginAsync(); await cloudService.RegisterForPushNotifications(); Application.Current.MainPage = new NavigationPage(new Pages.TaskList()); } catch (Exception ex) { await Application.Current.MainPage.DisplayAlert(\"Login Failed\", ex.Message, \"OK\"); } finally { IsBusy = false; } } Run the application again, with the same breakpoint in the GcmService.OnMessage() method. You can remove the other breakpoints at this point. Log into the application this time. Let's explore some of the debugging tools for Notification Hubs. Within Visual Studio, you can use View -> Server Explorer to open the server explorer. Expand the Azure node then the Notification Hubs node: Double-click on your Notification Hub to open the developer console. This provides two functions. Firstly, we can click on the Device Registrations tab: We can see the registration of our test emulator device. Note that our request for the topic:Sports tag has also been honored. If we did not configure that tag within the Push blade in the portal, that would not have been added to our registration. We can also send to a specific device using the test send facility. Click over to the Test Send facility. Since we only have one device, we can use broadcast. Each installation will also be given a tag: $InstallationId:{guid} , where {guid} is the installation ID. Select Google (GCM) -> Default to send a message to FCM. The body will be filled in for you. Since we have already set a breakpoint at the OnMessage() method in GcmService.cs , our app is running and we have entered the app and logged in, click Send . The breakpoint should be triggered within a reasonable amount of time. I'd like to say that it will be near instantaneous, but push notifications may take some time depending on what is happening within the push notification system at the time. The push should not take more than a couple of minutes to arrive and will likely arrive much quicker. We now have the full registration lifecycle working and we can do a test send to hit the right piece of code.","title":"Registering for Push Notifications"},{"location":"chapter5/android/#processing-a-push-notification","text":"Processing of push notifications is done within your mobile app, so you can process the push notifications however you want. For example, you may want to silently pull a specific record from the server and insert it into your SQLite offline cache when a push arrives, or you may want to pop up a message that opens the mobile app. In this example, we are going to show the message that comes in the message field of the data block. There are more examples in the recipes section . The OnMessage() method in GcmService.cs is triggered on a push. A simple notification looks like this: protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); var message = intent.Extras.GetString(\"message\"); var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager; var uiIntent = new Intent(context, typeof(MainActivity)); NotificationCompat.Builder builder = new NotificationCompat.Builder(context); var notification = builder.SetContentIntent(PendingIntent.GetActivity(context, 0, uiIntent, 0)) .SetSmallIcon(Android.Resource.Drawable.SymDefAppIcon) .SetTicker(\"TaskList\") .SetContentTitle(\"TaskList\") .SetContentText(message) .SetSound(RingtoneManager.GetDefaultUri(RingtoneType.Notification)) .SetAutoCancel(true) .Build(); notificationManager.Notify(1, notification); } The major thing to note here is how we get the contents of the message. The data block from the Notification Hub is received by the Intent into the Extras property. If you have other properties in that block, you can retrieve them the same way. The message field is standard, but you can pass other things. An example would be to pass the table name and ID of an inserted record. [5]:","title":"Processing a Push Notification"},{"location":"chapter5/concepts/","text":"Thus far, we've looked at options for communicating with the backend while the client is running. When the user changes apps, the client is suspended or placed on a low priority background thread. No user interaction is possible during this time. Most developers have a need to communicate interesting things to the user and this can only happen if the app is running. Fortunately, the major mobile platform providers have implemented some form of push notifications which are delivered to the app when it isn't running. Push notifications are messages that are sent to your mobile client whether the app is running or not. The mobile device will wake up your app to deliver the message. You have probably seen many examples of push notifications in your daily mobile phone usage. There are several uses, but they fall into two broad areas. Marketing messages are sent to inform the user of the app of something. Perhaps it's a new version, or a specific promotion for your favorite store. Silent notifications are sent to inform the app that something important has happened. For example, you may want to send a message when a data element has been updated in a table. Silent notifications are generally not to be seen by the user. Delivery of these messages comes with some significant penalties. You cannot guarantee the delivery of a message. The user of the device decides whether to accept messages or not. You cannot guarantee a delivery time, even though most messages are delivered within a couple of minutes. Finally, there is no built in acknowledgement of the message. You have to do something extra in code to ensure the delivery happens. How Push Notifications Works \u00b6 Each platform provider provides their own push notification service. For example, iOS uses Apple Push Notification Service (APNS) . Google uses Firebase Communications Manager (FCM) . This used to be called Google Communications Manager or GCM. It's the same service; just rebranded. Newer versions of Windows (including Universal Windows) use Windows Notification Service (WNS) whereas older versions of Windows Phone used Microsoft Platform Notification Service (MPNS) . There are other push notification services, for example, for FireOS (run by Amazon) and China (run by Baidu). In all cases, the process is the same: The mobile device initiates the process, registering with the Platform Notification Service (PNS). It will receive a Registration ID in return. The registration ID is specific to an app running on a specific device. Once you have the registration ID, you will pass that registration ID to your backend. The backend will use the registration ID when communicating with the PNS to send your app messages. This is where complexity rears its ugly head. Without an intervening service, the backend will need to do the following: Store the registration ID and PNS in a database for later reference. Lookup the list of registration IDs on a per-PNS basis and send a provided message in batches. Handle retry, incremental back-off, throttling and tracking for each message. Deal with registration failure and maintenance of the database. This is just the start of the functionality. In general, marketeers will want tracking of the messages (such as how many were opened or acted on, what demographics were the opened messages, etc.) and they will want to push to only a subset of users, targetted by opt-in lists or other demographic information. Introducing Notification Hubs \u00b6 Developing a system for pushing notifications to devices is a significant undertaking. I would not recommend anyone undertake such a service for their app. Fortunately, there are a number of services that can do this for you. Azure's entry into this space is Azure Notification Hubs . Notification Hubs (or NH as we will call it) handles all the registration and bulk sending logic to allow you to send a single message to multiple recipients without having to worry about what platform they are on. In addition, NH has support for tagging individual device registrations with information about the user, groups, or opt-in lists. Azure Mobile Apps has direct support for Notification Hubs within the client SDK and Azure App Service has a registration service built right in for NH, allowing you to easily integrate your mobile app with the facilities that NH provides. Tip You do not have to run Azure Mobile Apps or Azure App Service to use Notification Hubs. You do need to have a registration service somewhere. However, Notification Hubs is a standalone service. Do not use Notification Hubs as the registration service - you will need to distribute the key with your service, and that opens up security concerns. Notification Hubs has two features that are important in mobile push scenarios - tags and templates. while you will see these two features a lot in this chapter, we will also use other features of Notification Hubs, such as Scheduled Push, Geofenced Push and Analytics. Tags \u00b6 When a device registers itself with Notification Hubs (via the registration endpoint), you can specify a number of tags that are associated with the device. The tag allows you to segment the devices and push a message to only a portion of the devices. Technically, a tag is a string. The string can be up to 120 characters long, but has a restricted character set (alphanumeric plus a few special characters). You can use tags to allow the user to register interest in a topic, or register on their behalf based on just about anything you want. If you want to push to a department or users in a specific location, you can automatically register for those tags within the registration service. You can also use tags to do \"user tagging\" - allowing you to push to a user ID or email address instead of a device ID. When sending a push notification, you can broadcast a message to everyone, but it's generally better to send to a tag. You can also combine tags with boolean operations. For example, you might want to push a Marketing message to all sales people in Washington with (state:Washington && dept:Sales) . Tag expressions like this are limited in the number of tags allowed. Templates \u00b6 There are multiple plaform notification systems and each one wants a message sent to them in a specific format. APNS and FCM require a JSON payload (each of which is different), while WNS requires an XML payload. Effectively, this makes the backend of your app responsible for a part of the presentation layer, which is something that has been avoided thus far. In addition, you might want to localize the message for your audience and potentially use string replacement to customize the message for the recipient. Templates provide a way to send cross-platform notifications and customize the message for each recipient. You can use locale files to insert locale-specific messages into the template. Configuring Notification Hubs \u00b6 Our first step is to configure our backend. Thus far, we have implemented an Azure App Service with a SQL Azure database, and that is our starting point again. To those resources, we will add the Notification Hub, which starts just like the addition of any other resource: Log into the Azure portal . Click on the + NEW button in the top right corner (or the + ADD button at the top of your resource group). Select or search for Notification Hub . Click on Create . Enter the information required. You will need to create both a notification hub and a namespace. I generally add the -ns designation to a namespace. Click on Create . Notification Hubs has three tiers which give you increasing numbers of pushes, plus additional features. The Free tier provides just push services. The Basic tier gives you the more pushes plus the opportunity to buy additional pushes. Telemetry, scheduled push and multi-tenancy are only provided in the Standard tier, which should be your choice for production workloads. When considering architecture, you should use one Notification Hub per mobile backend. Notification Hub namespaces are used for deployment grouping. For example, you might want to give a different namespace to each tenant in a multi-tenant environment. App Service Push is Global to the App Service If you run your App Service with multiple slots (see Chapter 9 for details on slots), then note that the configuration of App Service Push is global to all slots within your App Service. Once your notification hub has been created, you will see both the hub and the namespace listed in your resource group. Note that we have not actually linked the notification hub to any platform notification services yet. We will do that later. Configuring Push Registration \u00b6 Push registration is generally handled by the mobile backend, and there is a feature of the App Service for this purpose. Although it is possible, resist trying to get mobile clients to register with the notification hub directly. Log into the Azure portal . Select your mobile backend. Click on Push (under the SETTINGS menu). Click on Connect . Select the notification hub you created earlier. Wait for the notification hub to be connected (it takes approximately 10 seconds). You can now decide which tags are valid for this application. The mobile client will request a list of tags. The push registration service will use this information to register the appropriate tags with the notification hub. Info The process is called push registration, but the entity that is created in notification hubs is called an Installation . There are two types of tags. Client requested tags may be requested by the mobile client during the registration process. Automatically added tags are added by the mobile backend. Let's take an example. My mobile backend is connected to Azure Active Directory. I've configured my backend as follows: Here, I have three client requested tags and an automatically generated tag that is only added when authenticated. Let's suppose that the mobile client requested [ topic:World topic:Sports ] and the mobile client was authenticated as username user@foo.com . With this configuration, the user would be registered for the following tags: topic:Sports auto:user@foo.com The topic:Politics and topic:News tags would not be added because the user did not request them. The topic:World tag would not be added because it is not in the whitelist of allowed client requested tags. Info There is no ability to request any tag (a wild-card) because it is a large security hole. With a wild-card tag, you could request push notifications for a user or group to which you were not allowed. The $(provider.claim) format is used in automatically added tags to add claims from the authenticated userinto the notification hub installation. You can use any claim that is returned by the /.auth/me endpoint. The standard identifiers are: $( provider .emailaddress) $( provider .identityprovider) $( provider .name) Replace provider with the provider name (facebook, google, microsoftaccount, twitter or aad). The list of claims that are available is different for each provider and additional claims may be available. It is possible to configure Azure AD to return groups, for example. Check the output of the /.auth/me endpoint to determine which claims are available. Info The automatic claim tags are only included when authenticated with a supported authentication provider. If you are using Custom Authentication, you must also use a custom WebAPI controller to do push registration if you wants claims based on your authentication. A claim name can resolve to multiple claim types. For example, $(aad.name) resolves to the following claims: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name http://schemas.microsoft.com/ws/2008/06/identity/claims/name name In this case, three tags may be created - one for each unique name. Warn App Service Push provides a default tag called _UserId. This is currently badly formed based on the MD5 of the SID provided in the authentication token. My recommendation is to not rely on or use the default tags that are provided. Set up your own tags. Once you have configure the tags, click on Save to save your work. Registering Your Mobile Client \u00b6 After you have gained the requisite platform-specific registration ID, you need to pass this to the mobile backend. The registration endpoint listens on /push/installations/{guid} where the GUID is a unique ID for the app on a specific mobile device. The Azure Mobile Apps client generates this for you. You must HTTP PUT a JSON Installation object to this URL. A simple (empty) Installation object looks like this: { \"installationId\": \"{guid}\", \"platform\": \"gcm\", \"pushChannel\": \"{registrationid}\" } The minimal settings are the installationId, platform, and pushChannel. The pushChannel field needs to be set to the registration ID of the platform. The platform is set to the appropriate value for the platform you are using - gcm, apns or wns. You can also add tags and template into this installation object. Check out the recipes section for details on requesting tags and templates. There is a simple class for implementing an installation, which I place in Abstractions\\DeviceInstallation.cs within the shared project: using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using Newtonsoft.Json; namespace TaskList.Abstractions { public class DeviceInstallation { public DeviceInstallation() { Tags = new List<string>(); Templates = new Dictionary<string, PushTemplate>(); } [JsonProperty(PropertyName = \"installationId\")] public string InstallationId { get; set; } [JsonProperty(PropertyName = \"platform\")] public string Platform { get; set; } [JsonProperty(PropertyName = \"pushChannel\")] public string PushChannel { get; set; } [JsonProperty(PropertyName = \"tags\")] public List<string> Tags { get; set; } [JsonProperty(PropertyName = \"templates\")] public Dictionary<string, PushTemplate> Templates { get; set; } } public class PushTemplate { public PushTemplate() { Tags = new List<string>(); } [JsonProperty(PropertyName = \"body\")] public string Body { get; set; } [JsonProperty(PropertyName = \"tags\")] public List<string> Tags { get; set; } } } The Installation object is also available in the Notification Hubs SDK, but I find bringing in the entire Notification Hubs SDK just for this is a little overkill. You can create a suitable installation and register with the InvokeApiAsync<T,U>() method: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (GcmClient.IsRegistered(RootView)) { try { var registrationId = GcmClient.GetRegistrationId(RootView); //var push = client.GetPush(); //await push.RegisterAsync(registrationId); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"gcm\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request PushTemplate genericTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"message\"\":\"\"$(messageParam)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var response = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); } catch (Exception ex) { Log.Error(\"DroidPlatformProvider\", $\"Could not register with NH: {ex.Message}\"); } } else { Log.Error(\"DroidPlatformProvider\", $\"Not registered with GCM\"); } } Where is that endpoint? The /push/installations endpoint is part of App Service Push - a feature of the Azure App Service resource. This exists on your .azurewebsites.net domain. It is not part of Notification Hubs. This is normally placed within the platform-specific provider since the details on how to get the registration ID and platform are different. This version is for the Android edition as an example. Using InvokeApiAsync() instead of relying on the in-built Azure Mobile Apps push registration methods gives you more control over the process of registration. Next Steps \u00b6 Each push notification system is different and requires different configuration in both the mobile client and backend registration system. You can jump directly to a specific platform: Android . iOS . Windows .","title":"Concepts"},{"location":"chapter5/concepts/#how-push-notifications-works","text":"Each platform provider provides their own push notification service. For example, iOS uses Apple Push Notification Service (APNS) . Google uses Firebase Communications Manager (FCM) . This used to be called Google Communications Manager or GCM. It's the same service; just rebranded. Newer versions of Windows (including Universal Windows) use Windows Notification Service (WNS) whereas older versions of Windows Phone used Microsoft Platform Notification Service (MPNS) . There are other push notification services, for example, for FireOS (run by Amazon) and China (run by Baidu). In all cases, the process is the same: The mobile device initiates the process, registering with the Platform Notification Service (PNS). It will receive a Registration ID in return. The registration ID is specific to an app running on a specific device. Once you have the registration ID, you will pass that registration ID to your backend. The backend will use the registration ID when communicating with the PNS to send your app messages. This is where complexity rears its ugly head. Without an intervening service, the backend will need to do the following: Store the registration ID and PNS in a database for later reference. Lookup the list of registration IDs on a per-PNS basis and send a provided message in batches. Handle retry, incremental back-off, throttling and tracking for each message. Deal with registration failure and maintenance of the database. This is just the start of the functionality. In general, marketeers will want tracking of the messages (such as how many were opened or acted on, what demographics were the opened messages, etc.) and they will want to push to only a subset of users, targetted by opt-in lists or other demographic information.","title":"How Push Notifications Works"},{"location":"chapter5/concepts/#introducing-notification-hubs","text":"Developing a system for pushing notifications to devices is a significant undertaking. I would not recommend anyone undertake such a service for their app. Fortunately, there are a number of services that can do this for you. Azure's entry into this space is Azure Notification Hubs . Notification Hubs (or NH as we will call it) handles all the registration and bulk sending logic to allow you to send a single message to multiple recipients without having to worry about what platform they are on. In addition, NH has support for tagging individual device registrations with information about the user, groups, or opt-in lists. Azure Mobile Apps has direct support for Notification Hubs within the client SDK and Azure App Service has a registration service built right in for NH, allowing you to easily integrate your mobile app with the facilities that NH provides. Tip You do not have to run Azure Mobile Apps or Azure App Service to use Notification Hubs. You do need to have a registration service somewhere. However, Notification Hubs is a standalone service. Do not use Notification Hubs as the registration service - you will need to distribute the key with your service, and that opens up security concerns. Notification Hubs has two features that are important in mobile push scenarios - tags and templates. while you will see these two features a lot in this chapter, we will also use other features of Notification Hubs, such as Scheduled Push, Geofenced Push and Analytics.","title":"Introducing Notification Hubs"},{"location":"chapter5/concepts/#tags","text":"When a device registers itself with Notification Hubs (via the registration endpoint), you can specify a number of tags that are associated with the device. The tag allows you to segment the devices and push a message to only a portion of the devices. Technically, a tag is a string. The string can be up to 120 characters long, but has a restricted character set (alphanumeric plus a few special characters). You can use tags to allow the user to register interest in a topic, or register on their behalf based on just about anything you want. If you want to push to a department or users in a specific location, you can automatically register for those tags within the registration service. You can also use tags to do \"user tagging\" - allowing you to push to a user ID or email address instead of a device ID. When sending a push notification, you can broadcast a message to everyone, but it's generally better to send to a tag. You can also combine tags with boolean operations. For example, you might want to push a Marketing message to all sales people in Washington with (state:Washington && dept:Sales) . Tag expressions like this are limited in the number of tags allowed.","title":"Tags"},{"location":"chapter5/concepts/#templates","text":"There are multiple plaform notification systems and each one wants a message sent to them in a specific format. APNS and FCM require a JSON payload (each of which is different), while WNS requires an XML payload. Effectively, this makes the backend of your app responsible for a part of the presentation layer, which is something that has been avoided thus far. In addition, you might want to localize the message for your audience and potentially use string replacement to customize the message for the recipient. Templates provide a way to send cross-platform notifications and customize the message for each recipient. You can use locale files to insert locale-specific messages into the template.","title":"Templates"},{"location":"chapter5/concepts/#configuring-notification-hubs","text":"Our first step is to configure our backend. Thus far, we have implemented an Azure App Service with a SQL Azure database, and that is our starting point again. To those resources, we will add the Notification Hub, which starts just like the addition of any other resource: Log into the Azure portal . Click on the + NEW button in the top right corner (or the + ADD button at the top of your resource group). Select or search for Notification Hub . Click on Create . Enter the information required. You will need to create both a notification hub and a namespace. I generally add the -ns designation to a namespace. Click on Create . Notification Hubs has three tiers which give you increasing numbers of pushes, plus additional features. The Free tier provides just push services. The Basic tier gives you the more pushes plus the opportunity to buy additional pushes. Telemetry, scheduled push and multi-tenancy are only provided in the Standard tier, which should be your choice for production workloads. When considering architecture, you should use one Notification Hub per mobile backend. Notification Hub namespaces are used for deployment grouping. For example, you might want to give a different namespace to each tenant in a multi-tenant environment. App Service Push is Global to the App Service If you run your App Service with multiple slots (see Chapter 9 for details on slots), then note that the configuration of App Service Push is global to all slots within your App Service. Once your notification hub has been created, you will see both the hub and the namespace listed in your resource group. Note that we have not actually linked the notification hub to any platform notification services yet. We will do that later.","title":"Configuring Notification Hubs"},{"location":"chapter5/concepts/#configuring-push-registration","text":"Push registration is generally handled by the mobile backend, and there is a feature of the App Service for this purpose. Although it is possible, resist trying to get mobile clients to register with the notification hub directly. Log into the Azure portal . Select your mobile backend. Click on Push (under the SETTINGS menu). Click on Connect . Select the notification hub you created earlier. Wait for the notification hub to be connected (it takes approximately 10 seconds). You can now decide which tags are valid for this application. The mobile client will request a list of tags. The push registration service will use this information to register the appropriate tags with the notification hub. Info The process is called push registration, but the entity that is created in notification hubs is called an Installation . There are two types of tags. Client requested tags may be requested by the mobile client during the registration process. Automatically added tags are added by the mobile backend. Let's take an example. My mobile backend is connected to Azure Active Directory. I've configured my backend as follows: Here, I have three client requested tags and an automatically generated tag that is only added when authenticated. Let's suppose that the mobile client requested [ topic:World topic:Sports ] and the mobile client was authenticated as username user@foo.com . With this configuration, the user would be registered for the following tags: topic:Sports auto:user@foo.com The topic:Politics and topic:News tags would not be added because the user did not request them. The topic:World tag would not be added because it is not in the whitelist of allowed client requested tags. Info There is no ability to request any tag (a wild-card) because it is a large security hole. With a wild-card tag, you could request push notifications for a user or group to which you were not allowed. The $(provider.claim) format is used in automatically added tags to add claims from the authenticated userinto the notification hub installation. You can use any claim that is returned by the /.auth/me endpoint. The standard identifiers are: $( provider .emailaddress) $( provider .identityprovider) $( provider .name) Replace provider with the provider name (facebook, google, microsoftaccount, twitter or aad). The list of claims that are available is different for each provider and additional claims may be available. It is possible to configure Azure AD to return groups, for example. Check the output of the /.auth/me endpoint to determine which claims are available. Info The automatic claim tags are only included when authenticated with a supported authentication provider. If you are using Custom Authentication, you must also use a custom WebAPI controller to do push registration if you wants claims based on your authentication. A claim name can resolve to multiple claim types. For example, $(aad.name) resolves to the following claims: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name http://schemas.microsoft.com/ws/2008/06/identity/claims/name name In this case, three tags may be created - one for each unique name. Warn App Service Push provides a default tag called _UserId. This is currently badly formed based on the MD5 of the SID provided in the authentication token. My recommendation is to not rely on or use the default tags that are provided. Set up your own tags. Once you have configure the tags, click on Save to save your work.","title":"Configuring Push Registration"},{"location":"chapter5/concepts/#registering-your-mobile-client","text":"After you have gained the requisite platform-specific registration ID, you need to pass this to the mobile backend. The registration endpoint listens on /push/installations/{guid} where the GUID is a unique ID for the app on a specific mobile device. The Azure Mobile Apps client generates this for you. You must HTTP PUT a JSON Installation object to this URL. A simple (empty) Installation object looks like this: { \"installationId\": \"{guid}\", \"platform\": \"gcm\", \"pushChannel\": \"{registrationid}\" } The minimal settings are the installationId, platform, and pushChannel. The pushChannel field needs to be set to the registration ID of the platform. The platform is set to the appropriate value for the platform you are using - gcm, apns or wns. You can also add tags and template into this installation object. Check out the recipes section for details on requesting tags and templates. There is a simple class for implementing an installation, which I place in Abstractions\\DeviceInstallation.cs within the shared project: using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using Newtonsoft.Json; namespace TaskList.Abstractions { public class DeviceInstallation { public DeviceInstallation() { Tags = new List<string>(); Templates = new Dictionary<string, PushTemplate>(); } [JsonProperty(PropertyName = \"installationId\")] public string InstallationId { get; set; } [JsonProperty(PropertyName = \"platform\")] public string Platform { get; set; } [JsonProperty(PropertyName = \"pushChannel\")] public string PushChannel { get; set; } [JsonProperty(PropertyName = \"tags\")] public List<string> Tags { get; set; } [JsonProperty(PropertyName = \"templates\")] public Dictionary<string, PushTemplate> Templates { get; set; } } public class PushTemplate { public PushTemplate() { Tags = new List<string>(); } [JsonProperty(PropertyName = \"body\")] public string Body { get; set; } [JsonProperty(PropertyName = \"tags\")] public List<string> Tags { get; set; } } } The Installation object is also available in the Notification Hubs SDK, but I find bringing in the entire Notification Hubs SDK just for this is a little overkill. You can create a suitable installation and register with the InvokeApiAsync<T,U>() method: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (GcmClient.IsRegistered(RootView)) { try { var registrationId = GcmClient.GetRegistrationId(RootView); //var push = client.GetPush(); //await push.RegisterAsync(registrationId); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"gcm\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request PushTemplate genericTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"message\"\":\"\"$(messageParam)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var response = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); } catch (Exception ex) { Log.Error(\"DroidPlatformProvider\", $\"Could not register with NH: {ex.Message}\"); } } else { Log.Error(\"DroidPlatformProvider\", $\"Not registered with GCM\"); } } Where is that endpoint? The /push/installations endpoint is part of App Service Push - a feature of the Azure App Service resource. This exists on your .azurewebsites.net domain. It is not part of Notification Hubs. This is normally placed within the platform-specific provider since the details on how to get the registration ID and platform are different. This version is for the Android edition as an example. Using InvokeApiAsync() instead of relying on the in-built Azure Mobile Apps push registration methods gives you more control over the process of registration.","title":"Registering Your Mobile Client"},{"location":"chapter5/concepts/#next-steps","text":"Each push notification system is different and requires different configuration in both the mobile client and backend registration system. You can jump directly to a specific platform: Android . iOS . Windows .","title":"Next Steps"},{"location":"chapter5/ios/","text":"Push notifications for Apple devices is handled by Apple Push Notification Service or APNS. APNS is certificate based, rather than secret based as is the case with FCM. You will find that there are two certificates - a test certificate that is used for test devices, and a production certificate that is used for production devices. You can use a common certificate for both (a so-called Universal Certificate). However, you must ensure that you use the appropriate endpoints - test or production. It is imperitive that you use a Mac for this configuration. You will be using Apple native tools to generate certificates and the process of configuring the APNS gateway is made easier by using XCode tools. You can do certain things on a PC (like editing the plist files appropriately), but you will end up spending a significant amount of time on the Mac. As a result, I'm going to do this entire section on a Mac. If you have not done so already, read through the Android Push section to get all the code for the shared project - it won't be repeated in this section. Registering with APNS \u00b6 Registering with APNS is a multi-step process: Register an App ID for your app, and select Push Notifications as a capability. Create an appropriate certificate for the push channel (either a Development or Distribution certificate). Configure Notification Hubs to use APNS. Configure your application to support Push Notifications. Add code for handling push notifications to your app. Let's cover each one in turn: Register an App ID for your app \u00b6 Once you get to adding push notifications to your application, you are going to need that full developers license from Apple. You need to work with real devices and that means you need code signing certificates on your mac. If you have not spent the cash for the Apple Developers program, then you will probably find you need to at this point. Registering an App ID is handled on the Apple Developer Portal . Apple does a good job of documenting the process , so these instructions are duplicative of the instructions that Apple provides. Go to the Apple Developer Portal and log in with your Apple developer ID. In the left-hand menu, click Certificates, IDs & Profiles . In the left-hand menu, Identifiers , click App IDs . Click the + button in the top right corner. Fill in the form: The App ID Description is not used and can be set to anything (subject to validation rules) Choose an Explicit App ID for this app. Enter the App ID suffix according to the rules. I used com.shellmonger.tasklist . Select Push Notifications in the App Services section. Click Continue when the form is complete. Make a note of the Identifier in the next screen, then click Register . Click Done . Note that the Push Notifications capability will be listed as Configurable until you create a certificate that is used for push notifications. Once that happens, the capability will be listed as Enabled . Create a certificate for the push channel \u00b6 I mentioned earlier that APNS is certificate based. That means that you need to generate an SSL certificate to fully configure push notifications: Staying in Certificates, Identifiers & Profiles , click All under the Certificates heading in the left hand menu. Click on the + button in the top right corner. Select the Apple Push Notification service SSL (Sandbox & Production) Click Continue . Select the App ID you just created from the list, then click Continue . Follow the on-screen instructions for creating a Certificate Signing Request (CSR). Once you have generated the CSR, click Continue in the browser. Select the CSR you just generated using the Choose File button, then click Continue . Click Download to download the resulting certificate. Click Done when the download is complete. Find your downloaded certificate and double-click on it to import it into Keychain Access Your certificate will also appear in the Certificates > All list within the Apple Developer console. Configure Notification Hubs \u00b6 Notification Hubs requires you to upload the certificate as a .p12 (PKCS#12) file. To generate this file: Open Keychain Access. Select My Certificates from the left hand menu Look for the certificate you just generated, and expand it to show the private key. Right-click the private key and select Export... . Select Personal Information Exchange (.p12) as the type and give it a name and location. Click Save . Enter a password (twice) to protect the certificate. Click OK . Upload the certificate to Azure: Open and log into the Azure portal . Select Notification Hubs , then the notification hub that is connected to your mobile backend. Click Push Notification Services , then select Apple (APNS) . Click + Upload Certificate . Fill in the form: Select the .p12 file you just created. Enter the password that you entered to secure the .p12 file. Select Sandbox (probably) or Production as appropriate. Click OK . It's important to figure out whether you are operating in the Sandbox (Development) or Production mode. During development, it's likely that your device will be registered on the Apple Developer console and you will be operating in the sandbox. Any device not listed with the developer console is considered \"production\". You must update the certificate to a production certificate and specify the production mode when you release your app. Apple APNS provides two endpoints for pushing notifications. If you use the wrong one, then APNS will return an error code. This will cause Notification Hubs to delete the registration and your push will fail. Configure your application \u00b6 Before we start with code, you will want a Provisioning Profile . This small file is key to being able to use push notifications on your device. You MUST have a real device at this point. The easiest way for this to happen is to plug the iPhone or iPad that you want to use into your development system. Once your device is recognized by iTunes, close iTunes down and start XCode. First, locate the Device ID for your iDevice. This can be found by opening Window -> Devices . Click on your iDevice in the left hand bar and copy the Identifier field. There are several other ways of finding the device ID. Refer to the Apple documentation for the other ways. Once you have the Device ID, you can register the device as a development device. Sign into the Apple Developer Portal , then: Under Devices , click All . Click the + button in the upper-right corner. Select Register Device . Enter a device name and the device ID you found earlier. Click Continue . Click Register . Click Done . Now, create a Provisioning Profile: Under Provisioning Profiles , click All . Click the + button in the upper-right corner. Select iOS App Development , then click Continue . Select the App ID you created earlier from the dropdown, then click Continue . Select the certificates you want to include, then click Continue . If you are unsure, include them all. Select the device(s) you want to use, then click Continue . Enter a Profile name, then click Continue . You can (and should) download your provisioning profile to your local machine. Click Done . For more information on creating a Provisioning Profile, see the Apple documentation . Download your Provisioning Profile to XCode You can also download your provisioning profile within XCode for later use. Visual Studio for Mac will be able to more easily detect it. Open XCode, then open XCode > Preferences . Click Accounts , then your account. Click your Agent entry in the right hand panel, then click View Details . Finally, click Download All Profiles . Once the download is complete, you can close the windows and return to Visual Studio. Next, configure the iOS project for push notifications. Start by loading your project in Visual Studio for Mac. Expand the TaskList.iOS project and open the Info.plist file. In the Identity section, fill in the Bundle Identifier . It must match the App ID Suffix that you set earlier. Scroll down until you see Background Modes . Check the Enable Background Modes checkbox. Adding an Account Visual Studio for Mac uses fastlane for account authentication. You will be walked through the process of adding an account the first time, and prompted to select an account thereafter. Note that fastlane does not work when your Apple ID has 2-factor authentication enabled. Turn 2FA off before you try to add an account. Check the Remote notifications checkbox. Save and close the Info.plist file. Right-click on the TaskList.iOS project, then select Options . Click iOS Bundle Signing in the left hand menu. Ensure the Platform is set to iPhone and not iPhoneSimulator . Select your Signing Identity and Provisioning Profile. Click OK . Provisioning Profiles are frustrating If you find yourself going round and round in circles on getting the signing certificate and provisioning profile right, you are not alone. This is possibly one of the most frustrating pieces of iOS development. See this Xamarin Forums post for a good list of details. Code the push handler \u00b6 The push handler is coded in the AppDelegate.cs file. Unlike other platforms (like Android), you don't have to write code to define the push handler. It's always in the same place. Add the following code to the AppDelegate.cs file: public static NSData PushDeviceToken { get; private set; } = null; public override bool FinishedLaunching(UIApplication app, NSDictionary options) { Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(); LoadApplication(new App()); if (UIDevice.CurrentDevice.CheckSystemVersion(8, 0)) { var pushSettings = UIUserNotificationSettings.GetSettingsForTypes( UIUserNotificationType.Alert | UIUserNotificationType.Badge | UIUserNotificationType.Sound, new NSSet()); UIApplication.SharedApplication.RegisterUserNotificationSettings(pushSettings); UIApplication.SharedApplication.RegisterForRemoteNotifications(); } return base.FinishedLaunching(app, options); } /// <summary> /// Called when the push notification system is registered /// </summary> /// <param name=\"application\">Application.</param> /// <param name=\"deviceToken\">Device token.</param> public override void RegisteredForRemoteNotifications(UIApplication application, NSData deviceToken) { AppDelegate.PushDeviceToken = deviceToken; } public override void DidReceiveRemoteNotification(UIApplication application, NSDictionary userInfo, Action<UIBackgroundFetchResult> completionHandler) { NSDictionary aps = userInfo.ObjectForKey(new NSString(\"aps\")) as NSDictionary; // The aps is a dictionary with the template values in it // You can adjust this section to do whatever you need to with the push notification string alert = string.Empty; if (aps.ContainsKey(new NSString(\"alert\"))) alert = (aps[new NSString(\"alert\")] as NSString).ToString(); //show alert if (!string.IsNullOrEmpty(alert)) { UIAlertView avAlert = new UIAlertView(\"Notification\", alert, null, \"OK\", null); avAlert.Show(); } } The NSDictionary , NSData , and NSString classes are part of the iOS programming model and do exactly what you would expect them to do. The UIAlertView class provides a standard alert. We need to add a little bit of code to the FinishedLaunching() method to send the registration request to APNS. When the response is received, the RegisteredForRemoteNotifications() method is called. Finally, the DidReceiveRemoteNotification() method is called whenever a remote push notification is received. Call common code for push notifications One of the great things about Xamarin Forms is that it is cross-platform. However, that all breaks down when you move to push notifications. One of the things you can do is to use the push handler to generate a model and then pass that model to a method in your PCL project. This allows you to express the differences clearly and yet still do the majority of the logic in a cross-platform manner. Registering with Azure Mobile Apps \u00b6 As with Android, I recommend using a HttpClient for registering with Notification Hubs via the Azure Mobile Apps Push handler. Here is the code that does basically the same thing as the Android version from the Services\\iOSPlatformProvider.cs file: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (AppDelegate.PushDeviceToken != null) { try { var registrationId = AppDelegate.PushDeviceToken.Description .Trim('<', '>').Replace(\" \", string.Empty).ToUpperInvariant(); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"apns\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request PushTemplate genericTemplate = new PushTemplate { Body = @\"{\"\"aps\"\":{\"\"alert\"\":\"\"$(messageParam)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var response = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); } catch (Exception ex) { System.Diagnostics.Debug.Fail($\"[iOSPlatformProvider]: Could not register with NH: {ex.Message}\"); } } } In this case, we don't have a service class to deal with - the iOS AppDelegate does all the work for us. The registration Id is stored in the AppDelegate once registered, but needs to be decoded (which is relatively simple). Similar to the Android version, we make the template we are using match what we are expecting within our push handler. Receiving Notifications in the background If you want your app to be notified when a notification is received when your app is in the background, you need to set the Background Fetch capability and your payload should include the key content-available with a value of 1 (true). You can add this to the Body of the template in the above sample. iOS will wake up the app and you will have 30 seconds to fetch any information you might need to update. Check the documentation for more details. Testing Notifications \u00b6 Our final step is to test the whole process. As with Android, there are two tests we need to perform. The first is to ensure that a registration happens when we expect it to. In the case of our app, that happens immediately after the authentication. There is no Notifications Hub registration monitor in Visual Studio for Mac, so we have to get that information an alternate way, by querying the hub registration endpoint. I've written [a script] for this purpose. To install: Install NodeJS . Go to the tools directory on the books GitHub repository. Run npm install . To use, you will need the endpoint for your notification hub namespace. Log onto the Azure portal . Open your Notification Hub namespace. Click Access Policies . Copy the connection string of the RootManagedSharedAccessKey (which is probably the only policy you have). You can now use the program using: node get_nh_registrations.js -c '<your connection string>' -h <your hub name> You will need to put the connection string in quotes generally. For example: node .\\get_nh_registrations.js -c 'Endpoint=sb://zumobook-ns.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey= ****c9VoZHtxSGliSIhH5EEuar1B/jsrgTQTHOTA=' -h zumobook-hub The output will look something like the following: Type: APNS (Template) Id: 219869525209729025-4738338868778066550-1 Device Token: 681F6BB012C62A61AA2185A676B23907A5FEFE9268283DD226B08B5F0336A552 Tag: topic:Sports Tag: _UserId:a9650e1c4d3268ec912f4d9ca6d1d933 Tag: photoadrian@outlook.com Tag: $InstallationId:{d7323fe4-64bb-4d99-a6cc-e7690032350f} Expires: 9999-12-31T23:59:59.9999999Z Note that this script does not deal with \"continuation tokens\", so it can only return the first page of information. This is generally suitable for testing purposes. We can also send a test message for push notifications. This can be done via the Azure Portal. Log onto the Azure portal . Find the Hub resource for your connected Notification Hub and open it. Click Test Send . Select Apple as the Platform, then click on Send . Your device should also receive the push notification and display an alert. You can also do a test send to an explicit tag. This can narrow the test send to just one device if necessary. To send to a specific device, you need to know the installation ID of the registration. Common Problems \u00b6 As you might expect, there is plenty to go wrong here. The majority of the issues come down to the fact that there are two endpoints on APNS - a Sandbox (or Developer) endpoint and a Production endpoint. If you are using the wrong endpoint, the notification hub will receive an error. If the notification hub receives an error from the APNS endpoint, it will remove the registration causing the error. This manifests itself in two ways. Firstly, your device will not receive the push notification. Secondly, the registration will be removed from the list of valid registrations, causing you to think that the device has not been registered. This has not been made easy by the fact that Apple has combined the certificates needed to push into a single certificate for both Sandbox and Production use cases. To correct this issue, ensure the notifiction hub is set up with the appropriate endpoint - Sandbox or Production. Next you can move onto Windows Push or skip to the Recipes Section .","title":"iOS Push"},{"location":"chapter5/ios/#registering-with-apns","text":"Registering with APNS is a multi-step process: Register an App ID for your app, and select Push Notifications as a capability. Create an appropriate certificate for the push channel (either a Development or Distribution certificate). Configure Notification Hubs to use APNS. Configure your application to support Push Notifications. Add code for handling push notifications to your app. Let's cover each one in turn:","title":"Registering with APNS"},{"location":"chapter5/ios/#register-an-app-id-for-your-app","text":"Once you get to adding push notifications to your application, you are going to need that full developers license from Apple. You need to work with real devices and that means you need code signing certificates on your mac. If you have not spent the cash for the Apple Developers program, then you will probably find you need to at this point. Registering an App ID is handled on the Apple Developer Portal . Apple does a good job of documenting the process , so these instructions are duplicative of the instructions that Apple provides. Go to the Apple Developer Portal and log in with your Apple developer ID. In the left-hand menu, click Certificates, IDs & Profiles . In the left-hand menu, Identifiers , click App IDs . Click the + button in the top right corner. Fill in the form: The App ID Description is not used and can be set to anything (subject to validation rules) Choose an Explicit App ID for this app. Enter the App ID suffix according to the rules. I used com.shellmonger.tasklist . Select Push Notifications in the App Services section. Click Continue when the form is complete. Make a note of the Identifier in the next screen, then click Register . Click Done . Note that the Push Notifications capability will be listed as Configurable until you create a certificate that is used for push notifications. Once that happens, the capability will be listed as Enabled .","title":"Register an App ID for your app"},{"location":"chapter5/ios/#create-a-certificate-for-the-push-channel","text":"I mentioned earlier that APNS is certificate based. That means that you need to generate an SSL certificate to fully configure push notifications: Staying in Certificates, Identifiers & Profiles , click All under the Certificates heading in the left hand menu. Click on the + button in the top right corner. Select the Apple Push Notification service SSL (Sandbox & Production) Click Continue . Select the App ID you just created from the list, then click Continue . Follow the on-screen instructions for creating a Certificate Signing Request (CSR). Once you have generated the CSR, click Continue in the browser. Select the CSR you just generated using the Choose File button, then click Continue . Click Download to download the resulting certificate. Click Done when the download is complete. Find your downloaded certificate and double-click on it to import it into Keychain Access Your certificate will also appear in the Certificates > All list within the Apple Developer console.","title":"Create a certificate for the push channel"},{"location":"chapter5/ios/#configure-notification-hubs","text":"Notification Hubs requires you to upload the certificate as a .p12 (PKCS#12) file. To generate this file: Open Keychain Access. Select My Certificates from the left hand menu Look for the certificate you just generated, and expand it to show the private key. Right-click the private key and select Export... . Select Personal Information Exchange (.p12) as the type and give it a name and location. Click Save . Enter a password (twice) to protect the certificate. Click OK . Upload the certificate to Azure: Open and log into the Azure portal . Select Notification Hubs , then the notification hub that is connected to your mobile backend. Click Push Notification Services , then select Apple (APNS) . Click + Upload Certificate . Fill in the form: Select the .p12 file you just created. Enter the password that you entered to secure the .p12 file. Select Sandbox (probably) or Production as appropriate. Click OK . It's important to figure out whether you are operating in the Sandbox (Development) or Production mode. During development, it's likely that your device will be registered on the Apple Developer console and you will be operating in the sandbox. Any device not listed with the developer console is considered \"production\". You must update the certificate to a production certificate and specify the production mode when you release your app. Apple APNS provides two endpoints for pushing notifications. If you use the wrong one, then APNS will return an error code. This will cause Notification Hubs to delete the registration and your push will fail.","title":"Configure Notification Hubs"},{"location":"chapter5/ios/#configure-your-application","text":"Before we start with code, you will want a Provisioning Profile . This small file is key to being able to use push notifications on your device. You MUST have a real device at this point. The easiest way for this to happen is to plug the iPhone or iPad that you want to use into your development system. Once your device is recognized by iTunes, close iTunes down and start XCode. First, locate the Device ID for your iDevice. This can be found by opening Window -> Devices . Click on your iDevice in the left hand bar and copy the Identifier field. There are several other ways of finding the device ID. Refer to the Apple documentation for the other ways. Once you have the Device ID, you can register the device as a development device. Sign into the Apple Developer Portal , then: Under Devices , click All . Click the + button in the upper-right corner. Select Register Device . Enter a device name and the device ID you found earlier. Click Continue . Click Register . Click Done . Now, create a Provisioning Profile: Under Provisioning Profiles , click All . Click the + button in the upper-right corner. Select iOS App Development , then click Continue . Select the App ID you created earlier from the dropdown, then click Continue . Select the certificates you want to include, then click Continue . If you are unsure, include them all. Select the device(s) you want to use, then click Continue . Enter a Profile name, then click Continue . You can (and should) download your provisioning profile to your local machine. Click Done . For more information on creating a Provisioning Profile, see the Apple documentation . Download your Provisioning Profile to XCode You can also download your provisioning profile within XCode for later use. Visual Studio for Mac will be able to more easily detect it. Open XCode, then open XCode > Preferences . Click Accounts , then your account. Click your Agent entry in the right hand panel, then click View Details . Finally, click Download All Profiles . Once the download is complete, you can close the windows and return to Visual Studio. Next, configure the iOS project for push notifications. Start by loading your project in Visual Studio for Mac. Expand the TaskList.iOS project and open the Info.plist file. In the Identity section, fill in the Bundle Identifier . It must match the App ID Suffix that you set earlier. Scroll down until you see Background Modes . Check the Enable Background Modes checkbox. Adding an Account Visual Studio for Mac uses fastlane for account authentication. You will be walked through the process of adding an account the first time, and prompted to select an account thereafter. Note that fastlane does not work when your Apple ID has 2-factor authentication enabled. Turn 2FA off before you try to add an account. Check the Remote notifications checkbox. Save and close the Info.plist file. Right-click on the TaskList.iOS project, then select Options . Click iOS Bundle Signing in the left hand menu. Ensure the Platform is set to iPhone and not iPhoneSimulator . Select your Signing Identity and Provisioning Profile. Click OK . Provisioning Profiles are frustrating If you find yourself going round and round in circles on getting the signing certificate and provisioning profile right, you are not alone. This is possibly one of the most frustrating pieces of iOS development. See this Xamarin Forums post for a good list of details.","title":"Configure your application"},{"location":"chapter5/ios/#code-the-push-handler","text":"The push handler is coded in the AppDelegate.cs file. Unlike other platforms (like Android), you don't have to write code to define the push handler. It's always in the same place. Add the following code to the AppDelegate.cs file: public static NSData PushDeviceToken { get; private set; } = null; public override bool FinishedLaunching(UIApplication app, NSDictionary options) { Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(); LoadApplication(new App()); if (UIDevice.CurrentDevice.CheckSystemVersion(8, 0)) { var pushSettings = UIUserNotificationSettings.GetSettingsForTypes( UIUserNotificationType.Alert | UIUserNotificationType.Badge | UIUserNotificationType.Sound, new NSSet()); UIApplication.SharedApplication.RegisterUserNotificationSettings(pushSettings); UIApplication.SharedApplication.RegisterForRemoteNotifications(); } return base.FinishedLaunching(app, options); } /// <summary> /// Called when the push notification system is registered /// </summary> /// <param name=\"application\">Application.</param> /// <param name=\"deviceToken\">Device token.</param> public override void RegisteredForRemoteNotifications(UIApplication application, NSData deviceToken) { AppDelegate.PushDeviceToken = deviceToken; } public override void DidReceiveRemoteNotification(UIApplication application, NSDictionary userInfo, Action<UIBackgroundFetchResult> completionHandler) { NSDictionary aps = userInfo.ObjectForKey(new NSString(\"aps\")) as NSDictionary; // The aps is a dictionary with the template values in it // You can adjust this section to do whatever you need to with the push notification string alert = string.Empty; if (aps.ContainsKey(new NSString(\"alert\"))) alert = (aps[new NSString(\"alert\")] as NSString).ToString(); //show alert if (!string.IsNullOrEmpty(alert)) { UIAlertView avAlert = new UIAlertView(\"Notification\", alert, null, \"OK\", null); avAlert.Show(); } } The NSDictionary , NSData , and NSString classes are part of the iOS programming model and do exactly what you would expect them to do. The UIAlertView class provides a standard alert. We need to add a little bit of code to the FinishedLaunching() method to send the registration request to APNS. When the response is received, the RegisteredForRemoteNotifications() method is called. Finally, the DidReceiveRemoteNotification() method is called whenever a remote push notification is received. Call common code for push notifications One of the great things about Xamarin Forms is that it is cross-platform. However, that all breaks down when you move to push notifications. One of the things you can do is to use the push handler to generate a model and then pass that model to a method in your PCL project. This allows you to express the differences clearly and yet still do the majority of the logic in a cross-platform manner.","title":"Code the push handler"},{"location":"chapter5/ios/#registering-with-azure-mobile-apps","text":"As with Android, I recommend using a HttpClient for registering with Notification Hubs via the Azure Mobile Apps Push handler. Here is the code that does basically the same thing as the Android version from the Services\\iOSPlatformProvider.cs file: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (AppDelegate.PushDeviceToken != null) { try { var registrationId = AppDelegate.PushDeviceToken.Description .Trim('<', '>').Replace(\" \", string.Empty).ToUpperInvariant(); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"apns\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request PushTemplate genericTemplate = new PushTemplate { Body = @\"{\"\"aps\"\":{\"\"alert\"\":\"\"$(messageParam)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var response = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); } catch (Exception ex) { System.Diagnostics.Debug.Fail($\"[iOSPlatformProvider]: Could not register with NH: {ex.Message}\"); } } } In this case, we don't have a service class to deal with - the iOS AppDelegate does all the work for us. The registration Id is stored in the AppDelegate once registered, but needs to be decoded (which is relatively simple). Similar to the Android version, we make the template we are using match what we are expecting within our push handler. Receiving Notifications in the background If you want your app to be notified when a notification is received when your app is in the background, you need to set the Background Fetch capability and your payload should include the key content-available with a value of 1 (true). You can add this to the Body of the template in the above sample. iOS will wake up the app and you will have 30 seconds to fetch any information you might need to update. Check the documentation for more details.","title":"Registering with Azure Mobile Apps"},{"location":"chapter5/ios/#testing-notifications","text":"Our final step is to test the whole process. As with Android, there are two tests we need to perform. The first is to ensure that a registration happens when we expect it to. In the case of our app, that happens immediately after the authentication. There is no Notifications Hub registration monitor in Visual Studio for Mac, so we have to get that information an alternate way, by querying the hub registration endpoint. I've written [a script] for this purpose. To install: Install NodeJS . Go to the tools directory on the books GitHub repository. Run npm install . To use, you will need the endpoint for your notification hub namespace. Log onto the Azure portal . Open your Notification Hub namespace. Click Access Policies . Copy the connection string of the RootManagedSharedAccessKey (which is probably the only policy you have). You can now use the program using: node get_nh_registrations.js -c '<your connection string>' -h <your hub name> You will need to put the connection string in quotes generally. For example: node .\\get_nh_registrations.js -c 'Endpoint=sb://zumobook-ns.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey= ****c9VoZHtxSGliSIhH5EEuar1B/jsrgTQTHOTA=' -h zumobook-hub The output will look something like the following: Type: APNS (Template) Id: 219869525209729025-4738338868778066550-1 Device Token: 681F6BB012C62A61AA2185A676B23907A5FEFE9268283DD226B08B5F0336A552 Tag: topic:Sports Tag: _UserId:a9650e1c4d3268ec912f4d9ca6d1d933 Tag: photoadrian@outlook.com Tag: $InstallationId:{d7323fe4-64bb-4d99-a6cc-e7690032350f} Expires: 9999-12-31T23:59:59.9999999Z Note that this script does not deal with \"continuation tokens\", so it can only return the first page of information. This is generally suitable for testing purposes. We can also send a test message for push notifications. This can be done via the Azure Portal. Log onto the Azure portal . Find the Hub resource for your connected Notification Hub and open it. Click Test Send . Select Apple as the Platform, then click on Send . Your device should also receive the push notification and display an alert. You can also do a test send to an explicit tag. This can narrow the test send to just one device if necessary. To send to a specific device, you need to know the installation ID of the registration.","title":"Testing Notifications"},{"location":"chapter5/ios/#common-problems","text":"As you might expect, there is plenty to go wrong here. The majority of the issues come down to the fact that there are two endpoints on APNS - a Sandbox (or Developer) endpoint and a Production endpoint. If you are using the wrong endpoint, the notification hub will receive an error. If the notification hub receives an error from the APNS endpoint, it will remove the registration causing the error. This manifests itself in two ways. Firstly, your device will not receive the push notification. Secondly, the registration will be removed from the list of valid registrations, causing you to think that the device has not been registered. This has not been made easy by the fact that Apple has combined the certificates needed to push into a single certificate for both Sandbox and Production use cases. To correct this issue, ensure the notifiction hub is set up with the appropriate endpoint - Sandbox or Production. Next you can move onto Windows Push or skip to the Recipes Section .","title":"Common Problems"},{"location":"chapter5/recipes/","text":"This section is dedicated to exploring various common recipes for push notifications and how we can achieve those recipes through cross-platform code. Marketing Push \u00b6 The most common requirement for push notifications is to alert users of a special offer or other marketing information. The general idea is that the marketing person will create a \"campaign\" that includes a push notification. When the user receives the push notification, they will accept it. If a user accepts the push notification, the mobile app will deep-link into a specific page and store the fact that the user viewed the page within the database. To implement this sort of functionality within a cross-platform application, we need to implement Templates . We gave demonstrations of the implementation of the templates while we were discussing the various platform implementations. However, we didn't actually use them. A template is provides by the mobile client when registering. Let's take a look at a typical template as implemented by each platform: Android : { \"data\": { \"message\": \"$(message)\", \"picture\": \"$(picture)\" } } iOS : { \"aps\": { \"alert\": \"$(message)\", \"picture\": \"$(picture)\" } } Windows : <?xml version=\"1.0\" encoding=\"utf-8\"?> <toast launch=\"zumobook\"> <visual> <binding template=\"ToastGeneric\"> <text>$(message)</text> </binding> </visual> <actions> <action content=\"Open\" arguments=\"$(picture)\" /> <action content=\"Cancel\" arguments=\"cancel\" /> </actions> </toast> Toast, Tile and Badge Schemas If you want to understand the format of the XML that we are using in the Windows section, it's laid out in the MSDN documentation . Each of these formats can be specified in the appropriate registration call: // Android Version var genericTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"message\"\":\"\"$(message)\"\",\"\"picture\"\":\"\"$(picture)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // iOS Version var genericTemplate = new PushTemplate { Body = @\"{\"\"aps\"\":{\"\"alert\"\":\"\"$(message)\"\",\"\"picture\"\":\"\"$(picture)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Windows Version var genericTemplate = new WindowsPushTemplate { Body = @\"<?xml version=\"\"1.0\"\" encoding=\"\"utf-8\"\"?> <toast launch=\"\"zumobook\"\"> <visual> <binding template=\"\"ToastGeneric\"\"> <text>$(message)</text> </binding> </visual> <actions> <action content=\"\"Open\"\" arguments=\"\"$(picture)\"\" /> <action content=\"\"Cancel\"\" arguments=\"\"cancel\"\" /> </actions> </toast>\" }; genericTemplate.Headers.Add(\"X-WNS-Type\", \"wns/toast\"); installation.Templates.Add(\"genericTemplate\", genericTemplate); To push, we can use the same Test Send facility in the Azure Portal. In the Test Send screen, set the Platforms field to be Custom Template , and the payload to be a JSON document with the two fields: { \"message\": \"Test Message\", \"picture\": \"http://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg\" } If you have done all the changes thus far, you will receive the same notification as before. The difference is that you are pushing a message once and receiving that same message across all the Android, iOS and Windows systems at the same time. You no longer have to know what sort of device your users are holding - the message will get to them. We can take this a step further, however, by deep-linking. Deep-linking is a technique often used in push notification systems whereby we present the user a dialog that asks them to open the notification. If the notification is opened, they are taken directly to a new view with the appropriate content provided. Deep Linking with Android \u00b6 Let's start our investigation with the Android code-base. Our push notification is received by the OnMessage() method within the GcmService class in the GcmHandler.cs file. We can easily extract the two fields we need to execute our deep-link: protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); var message = intent.Extras.GetString(\"message\") ?? \"Unknown Message\"; var picture = intent.Extras.GetString(\"picture\"); CreateNotification(\"TaskList\", message, picture); } We can continue by implementing a special format of the notification message we used earlier to send a notification: private void CreateNotification(string title, string msg, string parameter = null) { var startupIntent = new Intent(this, typeof(MainActivity)); startupIntent.PutExtra(\"param\", parameter); var stackBuilder = TaskStackBuilder.Create(this); stackBuilder.AddParentStack(Java.Lang.Class.FromType(typeof(MainActivity))); stackBuilder.AddNextIntent(startupIntent); var pendingIntent = stackBuilder.GetPendingIntent(0, PendingIntentFlags.OneShot); var notification = new Notification.Builder(this) .SetContentIntent(pendingIntent) .SetContentTitle(title) .SetContentText(msg) .SetSmallIcon(Resource.Drawable.icon) .SetAutoCancel(true) .Build(); var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager; notificationManager.Notify(0, notification); } The additional piece is the startupIntent . When the user clicks on open, the mobile app is called with the startupIntent included in the context. We update the OnCreate() method with MainActivity.cs to read this intent: [Activity(Label = \"TaskList.Droid\", Icon = \"@drawable/icon\", MainLauncher = true, ConfigurationChanges = ConfigChanges.ScreenSize | ConfigChanges.Orientation)] public class MainActivity : global::Xamarin.Forms.Platform.Android.FormsApplicationActivity { protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); ((DroidPlatformProvider)DependencyService.Get<IPlatformProvider>()).Init(this); string param = this.Intent.GetStringExtra(\"param\"); LoadApplication(new App(loadParameter: param)); } } The param string is null on the first start (or when the intent is not present). This get's passed to our App() constructor (in the shared project): public App(string loadParameter = null) { ServiceLocator.Instance.Add<ICloudService, AzureCloudService>(); if (loadParameter == null) { MainPage = new NavigationPage(new Pages.EntryPage()); } else { MainPage = new NavigationPage(new Pages.PictureView(loadParameter)); } } If the App() constructor is passed a non-null parameter, then we deep-link to a new page instead of going to the entry page. Now all we need to do is create a XAML page as follows that loads a picture. The Pages.PictureView.xaml is small enough since its only function is to display a picture: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.PictureView\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\"> <Image x:Name=\"background\" Source=\"{Binding PictureSource, Mode=OneWay}\" /> </ContentPage> The code behind file looks similar to the TaskDetail page: using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TaskList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class PictureView : ContentPage { public PictureView(string picture) { InitializeComponent(); BindingContext = new ViewModels.PictureViewModel(picture); } } } Finally, the view model should be familiar at this point: using TaskList.Abstractions; using Xamarin.Forms; namespace TaskList.ViewModels { public class PictureViewModel : BaseViewModel { public PictureViewModel(string picture = null) { if (picture != null) { PictureSource = picture; } Title = \"A Picture for you\"; } public string PictureSource { get; } } } If I were to continue, I would add some controls that allow me to go back to the task list (if I am logged in) or the entry page (if I am not logged in). Keep the Push Small You should keep the push payload as small as possible. There are limits and they vary by platform (but are in the range of 4-5Kb). Note that I don't include the full URL of the picture, for example, nor do I include the picture as binary data. This allows me to adjust to an appropriate image URLwithin the client. This keeps the number of bytes in the push small, but also allows me to adjust the image for the platform, if necessary. Deep Linking with iOS \u00b6 Deep linking with iOS follows a similar pattern to Android. The notification is received by DidReceiveRemoteNotification() method in the AppDelegate.cs , which we can then process to load the appropriate page from the background. First, update the DidReceiveRemoteNotification() method to call a new method we will define in a moment. This allows us to call the notification processor from multiple places: /// <summary> /// Handler for Push Notifications /// </summary> public override void DidReceiveRemoteNotification(UIApplication application, NSDictionary userInfo, Action<UIBackgroundFetchResult> completionHandler) { ProcessNotification(userInfo, false); } This method is also defined in the AppDelegate.cs class: private void ProcessNotification(NSDictionary options, bool fromFinishedLoading) { if (!(options != null && options.ContainsKey(new NSString(\"aps\")))) { // Short circuit - nothing to do return; } NSDictionary aps = options.ObjectForKey(new NSString(\"aps\")) as NSDictionary; // Obtain the alert and picture elements if they are there var alertString = GetStringFromOptions(aps, \"alert\"); var pictureString = GetStringFromOptions(aps, \"picture\"); if (!fromFinishedLoading) { // Manually show an alert if (!string.IsNullOrEmpty(alertString)) { UIAlertView alertView = new UIAlertView( \"TaskList\", alertString, null, NSBundle.MainBundle.LocalizedString(\"Cancel\", \"Cancel\"), NSBundle.MainBundle.LocalizedString(\"OK\", \"OK\") ); alertView.Clicked += (sender, args) => { if (args.ButtonIndex != alertView.CancelButtonIndex) { if (!string.IsNullOrEmpty(pictureString)) { App.Current.MainPage = new NavigationPage(new Pages.PictureView(pictureString)); } } }; alertView.Show(); } } } private string GetStringFromOptions(NSDictionary options, string key) { string v = string.Empty; if (options.ContainsKey(new NSString(key))) { v = (options[new NSString(key)] as NSString).ToString(); } return v; } This method checks to see if there is something to do. If there is, it generates the alert as before. This time, however, if the user clicks on OK, then it sets the current page to the same PictureView view that was used by the Android application. The GetStringFromOptions() method is a convenience method for extracting strings from the push notification payload. Send the following push notification to receive the picture: { \"aps\":{ \"alert\":\"Notification Hub test notification\", \"picture\":\"http://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg\" } } You should test this in the following cases: The app is running and in the foreground. The app is running, but in the background. The app is not running at all. Deep Linking with UWP \u00b6 Universal Windows is perhaps the most complete story for notifications out there. Firstly, let's construct our notification. On the Test Send blade within your notification hub in the Azure portal, choose Windows as the platform and cut and paste the following into the Payload: <?xml version=\"1.0\" encoding=\"utf-8\"?> <toast launch=\"zumobook\"> <visual> <binding template=\"ToastGeneric\"> <text>This is a simple toast notification example</text> </binding> </visual> <actions> <action content=\"Open\" arguments=\"http://static.boredpanda.com/blog/wp-content/uploads/2016/08/cute-kittens-7-57b30aa10707a__605.jpg\" /> <action content=\"Cancel\" arguments=\"cancel\" /> </actions> </toast> This payload provides a textual response with two buttons - an open button and a cancel button. The most important part of this, however, is the launch=\"zumobook\" . If the user clicks on Open, the application it is associated with is launched via the OnActivated() method, and the toast information is passed into that method. This method is located in the App.xaml.cs file of the TaskList.UWP project: protected override void OnActivated(IActivatedEventArgs args) { if (args.Kind == ActivationKind.ToastNotification) { var toastArgs = args as ToastNotificationActivatedEventArgs; Xamarin.Forms.Application.Current.MainPage = new Xamarin.Forms.NavigationPage( new Pages.PictureView(toastArgs.Argument)); } } The only real problem here is that there is a conflict within this file between the standard Frame object and the Xamarin Forms version of the Frame object. If you use using Xamarin.Forms; in this file, you have to fully qualify conflicting classes. It's just as easy to fully-qualify the specific Xamarin Forms classes when they are needed, as I did above. Push to Sync \u00b6 Sometimes, you want to alert the user that there is something new for that user. When the user is alerted, acceptance of the push notification indicates that the user wants to go to the app and synchronize the database before viewing the data. Push to Sync is very similar to the Marketing Push, but there are some caveats. In general, the synchronization process should happen within 30 seconds. That's not very long in the mobile world. So, what do you do? Firstly, let's look at the code for the server-side. We need to generate an asynchronous push whenever a record is updated. We will pass the ID of the updated record with the push. Here is an example table controller: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Microsoft.Azure.Mobile.Server; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server.Config; using Microsoft.Azure.NotificationHubs; using System.Collections.Generic; using System; namespace Backend.Controllers { [Authorize] public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request, enableSoftDelete: true); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query(); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) { return Lookup(id); } // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) { var item = await UpdateAsync(id, patch); await PushToSyncAsync(\"todoitem\", item.Id); return item; } // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); await PushToSyncAsync(\"todoitem\", item.Id); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task DeleteTodoItem(string id) { await PushToSyncAsync(\"todoitem\", id); await DeleteAsync(id); } private async Task PushToSyncAsync(string table, string id) { var appSettings = this.Configuration.GetMobileAppSettingsProvider().GetMobileAppSettings(); var nhName = appSettings.NotificationHubName; var nhConnection = appSettings.Connections[MobileAppSettingsKeys.NotificationHubConnectionString].ConnectionString; // Create a new Notification Hub client var hub = NotificationHubClient.CreateClientFromConnectionString(nhConnection, nhName); // Create a template message var templateParams = new Dictionary<string, string>(); templateParams[\"op\"] = \"sync\"; templateParams[\"table\"] = table; templateParams[\"id\"] = id; // Send the template message try { var result = await hub.SendTemplateNotificationAsync(templateParams); Configuration.Services.GetTraceWriter().Info(result.State.ToString()); } catch (Exception ex) { Configuration.Services.GetTraceWriter().Error(ex.Message, null, \"PushToSync Error\"); } } } } The important code here is the PushToSyncAsync() method. This does the actual push to your clients. In this version, any client that has registered a template with the $(op) , $(table) and $(id) variables will get the push notification. The Notifications Hub SDK SendTemplateNotificationAsync() method can also send to a list of devices and a tag expression via various overloads. We have to also register a new template. Here are the two versions: // Android version var pushToSyncTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"op\"\":\"\"$(op)\"\",\"\"table\"\":\"\"$(table)\"\",\"\"id\"\":\"\"$(id)\"\"}}\" }; installation.Templates.Add(\"pushToSync\", pushToSyncTemplate); // iOS version PushTemplate pushToSyncTemplate = new PushTemplate { Body = @\"{\"\"aps\"\":{\"\"op\"\":\"\"$(op)\"\",\"\"table\"\":\"\"$(table)\"\",\"\"id\"\":\"\"$(id)\"\"},\"\"content-available\"\":1}\" } installation.Templates.Add(\"pushToSync\", pushToSyncTemplate); What about Universal Windows You can do some remarkable things with Universal Windows, but you have to resort to raw pushes. At that point, you can decide what to put in the payload. When running, these are handled the same way as the marketing push. For more information, see the WNS documentation . The Push to Sync message needs to be handled by the TaskList view. The easiest mechanism of communicating with the view is to use the MessagingCenter . The TaskList view already has the appropriate code to refresh the list when it receives a message: // Execute the refresh command RefreshCommand.Execute(null); MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); We can add an appropriate push-to-sync version like this: MessageCenter.Subscribe<PushToSync>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); This is the same code, but listens for a notification from a different class. That class is defined in the shared project Models folder: namespace TaskList.Models { public class PushToSync { public string Table { get; set; } public string Id { get; set; } } } When the MessagingCenter is sent a message for push-to-sync, it will execute the refresh command, thus refreshing the data. All that remains is to actually send that message in response to the notification. For Android, this is done in the Services\\GcmHandler.cs file in the OnMessage() method: protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); var op = intent.Extras.GetString(\"op\"); if (op != null) { var syncMessage = new PushToSync() { Table = intent.Extras.GetString(\"table\"), Id = intent.Extras.GetString(\"id\") }; MessagingCenter.Send<PushToSync>(syncMessage, \"ItemsChanged\"); } else { var message = intent.Extras.GetString(\"message\") ?? \"Unknown Message\"; var picture = intent.Extras.GetString(\"picture\"); CreateNotification(\"TaskList\", message, picture); } } For iOS, the send happens in the ProcessNotification() method of the AppDelegate.cs class: private void ProcessNotification(NSDictionary options, bool fromFinishedLoading) { if (!(options != null && options.ContainsKey(new NSString(\"aps\")))) return; NSDictionary aps = options.ObjectForKey(new NSString(\"aps\")) as NSDictionary; if (!fromFinishedLoading) { var alertString = GetStringFromOptions(aps, \"alert\"); if (!string.IsNullOrEmpty(alertString)) { // Create the alert (removed for brevity) } var opString = GetStringFromOptions(aps, \"op\"); if (!string.IsNullOrEmpty(opString) && opString.Equals(\"sync\")) { var syncMessage = new PushToSync() { Table = GetStringFromOptions(aps, \"table\"), Id = GetStringFromOptions(aps, \"id\") }; MessagingCenter.Send<PushToSync>(syncMessage, \"ItemsChanged\"); } } } When a client inserts or updates a record into the database on the server, PushToSync() is called. That emits a push notification in the proper form defined within the mobile app. When the mobile app receives that push notification, it sends an \"ItemsChanged\" event to the messaging center. The TaskList view subscribes to those events and performs a sync in response to that event. There are several things we could do to this code, including: Push to the UserId that owns the record only - this will reduce the number of pushes that happen. Only pull the specific record on the specific table that is needed. This is available in the sender object.","title":"Push Recipes"},{"location":"chapter5/recipes/#marketing-push","text":"The most common requirement for push notifications is to alert users of a special offer or other marketing information. The general idea is that the marketing person will create a \"campaign\" that includes a push notification. When the user receives the push notification, they will accept it. If a user accepts the push notification, the mobile app will deep-link into a specific page and store the fact that the user viewed the page within the database. To implement this sort of functionality within a cross-platform application, we need to implement Templates . We gave demonstrations of the implementation of the templates while we were discussing the various platform implementations. However, we didn't actually use them. A template is provides by the mobile client when registering. Let's take a look at a typical template as implemented by each platform: Android : { \"data\": { \"message\": \"$(message)\", \"picture\": \"$(picture)\" } } iOS : { \"aps\": { \"alert\": \"$(message)\", \"picture\": \"$(picture)\" } } Windows : <?xml version=\"1.0\" encoding=\"utf-8\"?> <toast launch=\"zumobook\"> <visual> <binding template=\"ToastGeneric\"> <text>$(message)</text> </binding> </visual> <actions> <action content=\"Open\" arguments=\"$(picture)\" /> <action content=\"Cancel\" arguments=\"cancel\" /> </actions> </toast> Toast, Tile and Badge Schemas If you want to understand the format of the XML that we are using in the Windows section, it's laid out in the MSDN documentation . Each of these formats can be specified in the appropriate registration call: // Android Version var genericTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"message\"\":\"\"$(message)\"\",\"\"picture\"\":\"\"$(picture)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // iOS Version var genericTemplate = new PushTemplate { Body = @\"{\"\"aps\"\":{\"\"alert\"\":\"\"$(message)\"\",\"\"picture\"\":\"\"$(picture)\"\"}}\" }; installation.Templates.Add(\"genericTemplate\", genericTemplate); // Windows Version var genericTemplate = new WindowsPushTemplate { Body = @\"<?xml version=\"\"1.0\"\" encoding=\"\"utf-8\"\"?> <toast launch=\"\"zumobook\"\"> <visual> <binding template=\"\"ToastGeneric\"\"> <text>$(message)</text> </binding> </visual> <actions> <action content=\"\"Open\"\" arguments=\"\"$(picture)\"\" /> <action content=\"\"Cancel\"\" arguments=\"\"cancel\"\" /> </actions> </toast>\" }; genericTemplate.Headers.Add(\"X-WNS-Type\", \"wns/toast\"); installation.Templates.Add(\"genericTemplate\", genericTemplate); To push, we can use the same Test Send facility in the Azure Portal. In the Test Send screen, set the Platforms field to be Custom Template , and the payload to be a JSON document with the two fields: { \"message\": \"Test Message\", \"picture\": \"http://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg\" } If you have done all the changes thus far, you will receive the same notification as before. The difference is that you are pushing a message once and receiving that same message across all the Android, iOS and Windows systems at the same time. You no longer have to know what sort of device your users are holding - the message will get to them. We can take this a step further, however, by deep-linking. Deep-linking is a technique often used in push notification systems whereby we present the user a dialog that asks them to open the notification. If the notification is opened, they are taken directly to a new view with the appropriate content provided.","title":"Marketing Push"},{"location":"chapter5/recipes/#deep-linking-with-android","text":"Let's start our investigation with the Android code-base. Our push notification is received by the OnMessage() method within the GcmService class in the GcmHandler.cs file. We can easily extract the two fields we need to execute our deep-link: protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); var message = intent.Extras.GetString(\"message\") ?? \"Unknown Message\"; var picture = intent.Extras.GetString(\"picture\"); CreateNotification(\"TaskList\", message, picture); } We can continue by implementing a special format of the notification message we used earlier to send a notification: private void CreateNotification(string title, string msg, string parameter = null) { var startupIntent = new Intent(this, typeof(MainActivity)); startupIntent.PutExtra(\"param\", parameter); var stackBuilder = TaskStackBuilder.Create(this); stackBuilder.AddParentStack(Java.Lang.Class.FromType(typeof(MainActivity))); stackBuilder.AddNextIntent(startupIntent); var pendingIntent = stackBuilder.GetPendingIntent(0, PendingIntentFlags.OneShot); var notification = new Notification.Builder(this) .SetContentIntent(pendingIntent) .SetContentTitle(title) .SetContentText(msg) .SetSmallIcon(Resource.Drawable.icon) .SetAutoCancel(true) .Build(); var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager; notificationManager.Notify(0, notification); } The additional piece is the startupIntent . When the user clicks on open, the mobile app is called with the startupIntent included in the context. We update the OnCreate() method with MainActivity.cs to read this intent: [Activity(Label = \"TaskList.Droid\", Icon = \"@drawable/icon\", MainLauncher = true, ConfigurationChanges = ConfigChanges.ScreenSize | ConfigChanges.Orientation)] public class MainActivity : global::Xamarin.Forms.Platform.Android.FormsApplicationActivity { protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init(); global::Xamarin.Forms.Forms.Init(this, bundle); ((DroidPlatformProvider)DependencyService.Get<IPlatformProvider>()).Init(this); string param = this.Intent.GetStringExtra(\"param\"); LoadApplication(new App(loadParameter: param)); } } The param string is null on the first start (or when the intent is not present). This get's passed to our App() constructor (in the shared project): public App(string loadParameter = null) { ServiceLocator.Instance.Add<ICloudService, AzureCloudService>(); if (loadParameter == null) { MainPage = new NavigationPage(new Pages.EntryPage()); } else { MainPage = new NavigationPage(new Pages.PictureView(loadParameter)); } } If the App() constructor is passed a non-null parameter, then we deep-link to a new page instead of going to the entry page. Now all we need to do is create a XAML page as follows that loads a picture. The Pages.PictureView.xaml is small enough since its only function is to display a picture: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.PictureView\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\"> <Image x:Name=\"background\" Source=\"{Binding PictureSource, Mode=OneWay}\" /> </ContentPage> The code behind file looks similar to the TaskDetail page: using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace TaskList.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class PictureView : ContentPage { public PictureView(string picture) { InitializeComponent(); BindingContext = new ViewModels.PictureViewModel(picture); } } } Finally, the view model should be familiar at this point: using TaskList.Abstractions; using Xamarin.Forms; namespace TaskList.ViewModels { public class PictureViewModel : BaseViewModel { public PictureViewModel(string picture = null) { if (picture != null) { PictureSource = picture; } Title = \"A Picture for you\"; } public string PictureSource { get; } } } If I were to continue, I would add some controls that allow me to go back to the task list (if I am logged in) or the entry page (if I am not logged in). Keep the Push Small You should keep the push payload as small as possible. There are limits and they vary by platform (but are in the range of 4-5Kb). Note that I don't include the full URL of the picture, for example, nor do I include the picture as binary data. This allows me to adjust to an appropriate image URLwithin the client. This keeps the number of bytes in the push small, but also allows me to adjust the image for the platform, if necessary.","title":"Deep Linking with Android"},{"location":"chapter5/recipes/#deep-linking-with-ios","text":"Deep linking with iOS follows a similar pattern to Android. The notification is received by DidReceiveRemoteNotification() method in the AppDelegate.cs , which we can then process to load the appropriate page from the background. First, update the DidReceiveRemoteNotification() method to call a new method we will define in a moment. This allows us to call the notification processor from multiple places: /// <summary> /// Handler for Push Notifications /// </summary> public override void DidReceiveRemoteNotification(UIApplication application, NSDictionary userInfo, Action<UIBackgroundFetchResult> completionHandler) { ProcessNotification(userInfo, false); } This method is also defined in the AppDelegate.cs class: private void ProcessNotification(NSDictionary options, bool fromFinishedLoading) { if (!(options != null && options.ContainsKey(new NSString(\"aps\")))) { // Short circuit - nothing to do return; } NSDictionary aps = options.ObjectForKey(new NSString(\"aps\")) as NSDictionary; // Obtain the alert and picture elements if they are there var alertString = GetStringFromOptions(aps, \"alert\"); var pictureString = GetStringFromOptions(aps, \"picture\"); if (!fromFinishedLoading) { // Manually show an alert if (!string.IsNullOrEmpty(alertString)) { UIAlertView alertView = new UIAlertView( \"TaskList\", alertString, null, NSBundle.MainBundle.LocalizedString(\"Cancel\", \"Cancel\"), NSBundle.MainBundle.LocalizedString(\"OK\", \"OK\") ); alertView.Clicked += (sender, args) => { if (args.ButtonIndex != alertView.CancelButtonIndex) { if (!string.IsNullOrEmpty(pictureString)) { App.Current.MainPage = new NavigationPage(new Pages.PictureView(pictureString)); } } }; alertView.Show(); } } } private string GetStringFromOptions(NSDictionary options, string key) { string v = string.Empty; if (options.ContainsKey(new NSString(key))) { v = (options[new NSString(key)] as NSString).ToString(); } return v; } This method checks to see if there is something to do. If there is, it generates the alert as before. This time, however, if the user clicks on OK, then it sets the current page to the same PictureView view that was used by the Android application. The GetStringFromOptions() method is a convenience method for extracting strings from the push notification payload. Send the following push notification to receive the picture: { \"aps\":{ \"alert\":\"Notification Hub test notification\", \"picture\":\"http://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg\" } } You should test this in the following cases: The app is running and in the foreground. The app is running, but in the background. The app is not running at all.","title":"Deep Linking with iOS"},{"location":"chapter5/recipes/#deep-linking-with-uwp","text":"Universal Windows is perhaps the most complete story for notifications out there. Firstly, let's construct our notification. On the Test Send blade within your notification hub in the Azure portal, choose Windows as the platform and cut and paste the following into the Payload: <?xml version=\"1.0\" encoding=\"utf-8\"?> <toast launch=\"zumobook\"> <visual> <binding template=\"ToastGeneric\"> <text>This is a simple toast notification example</text> </binding> </visual> <actions> <action content=\"Open\" arguments=\"http://static.boredpanda.com/blog/wp-content/uploads/2016/08/cute-kittens-7-57b30aa10707a__605.jpg\" /> <action content=\"Cancel\" arguments=\"cancel\" /> </actions> </toast> This payload provides a textual response with two buttons - an open button and a cancel button. The most important part of this, however, is the launch=\"zumobook\" . If the user clicks on Open, the application it is associated with is launched via the OnActivated() method, and the toast information is passed into that method. This method is located in the App.xaml.cs file of the TaskList.UWP project: protected override void OnActivated(IActivatedEventArgs args) { if (args.Kind == ActivationKind.ToastNotification) { var toastArgs = args as ToastNotificationActivatedEventArgs; Xamarin.Forms.Application.Current.MainPage = new Xamarin.Forms.NavigationPage( new Pages.PictureView(toastArgs.Argument)); } } The only real problem here is that there is a conflict within this file between the standard Frame object and the Xamarin Forms version of the Frame object. If you use using Xamarin.Forms; in this file, you have to fully qualify conflicting classes. It's just as easy to fully-qualify the specific Xamarin Forms classes when they are needed, as I did above.","title":"Deep Linking with UWP"},{"location":"chapter5/recipes/#push-to-sync","text":"Sometimes, you want to alert the user that there is something new for that user. When the user is alerted, acceptance of the push notification indicates that the user wants to go to the app and synchronize the database before viewing the data. Push to Sync is very similar to the Marketing Push, but there are some caveats. In general, the synchronization process should happen within 30 seconds. That's not very long in the mobile world. So, what do you do? Firstly, let's look at the code for the server-side. We need to generate an asynchronous push whenever a record is updated. We will pass the ID of the updated record with the push. Here is an example table controller: using System.Linq; using System.Threading.Tasks; using System.Web.Http; using System.Web.Http.Controllers; using System.Web.Http.OData; using Microsoft.Azure.Mobile.Server; using Backend.DataObjects; using Backend.Models; using Microsoft.Azure.Mobile.Server.Config; using Microsoft.Azure.NotificationHubs; using System.Collections.Generic; using System; namespace Backend.Controllers { [Authorize] public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request, enableSoftDelete: true); } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query(); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) { return Lookup(id); } // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) { var item = await UpdateAsync(id, patch); await PushToSyncAsync(\"todoitem\", item.Id); return item; } // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { TodoItem current = await InsertAsync(item); await PushToSyncAsync(\"todoitem\", item.Id); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public async Task DeleteTodoItem(string id) { await PushToSyncAsync(\"todoitem\", id); await DeleteAsync(id); } private async Task PushToSyncAsync(string table, string id) { var appSettings = this.Configuration.GetMobileAppSettingsProvider().GetMobileAppSettings(); var nhName = appSettings.NotificationHubName; var nhConnection = appSettings.Connections[MobileAppSettingsKeys.NotificationHubConnectionString].ConnectionString; // Create a new Notification Hub client var hub = NotificationHubClient.CreateClientFromConnectionString(nhConnection, nhName); // Create a template message var templateParams = new Dictionary<string, string>(); templateParams[\"op\"] = \"sync\"; templateParams[\"table\"] = table; templateParams[\"id\"] = id; // Send the template message try { var result = await hub.SendTemplateNotificationAsync(templateParams); Configuration.Services.GetTraceWriter().Info(result.State.ToString()); } catch (Exception ex) { Configuration.Services.GetTraceWriter().Error(ex.Message, null, \"PushToSync Error\"); } } } } The important code here is the PushToSyncAsync() method. This does the actual push to your clients. In this version, any client that has registered a template with the $(op) , $(table) and $(id) variables will get the push notification. The Notifications Hub SDK SendTemplateNotificationAsync() method can also send to a list of devices and a tag expression via various overloads. We have to also register a new template. Here are the two versions: // Android version var pushToSyncTemplate = new PushTemplate { Body = @\"{\"\"data\"\":{\"\"op\"\":\"\"$(op)\"\",\"\"table\"\":\"\"$(table)\"\",\"\"id\"\":\"\"$(id)\"\"}}\" }; installation.Templates.Add(\"pushToSync\", pushToSyncTemplate); // iOS version PushTemplate pushToSyncTemplate = new PushTemplate { Body = @\"{\"\"aps\"\":{\"\"op\"\":\"\"$(op)\"\",\"\"table\"\":\"\"$(table)\"\",\"\"id\"\":\"\"$(id)\"\"},\"\"content-available\"\":1}\" } installation.Templates.Add(\"pushToSync\", pushToSyncTemplate); What about Universal Windows You can do some remarkable things with Universal Windows, but you have to resort to raw pushes. At that point, you can decide what to put in the payload. When running, these are handled the same way as the marketing push. For more information, see the WNS documentation . The Push to Sync message needs to be handled by the TaskList view. The easiest mechanism of communicating with the view is to use the MessagingCenter . The TaskList view already has the appropriate code to refresh the list when it receives a message: // Execute the refresh command RefreshCommand.Execute(null); MessagingCenter.Subscribe<TaskDetailViewModel>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); We can add an appropriate push-to-sync version like this: MessageCenter.Subscribe<PushToSync>(this, \"ItemsChanged\", async (sender) => { await ExecuteRefreshCommand(); }); This is the same code, but listens for a notification from a different class. That class is defined in the shared project Models folder: namespace TaskList.Models { public class PushToSync { public string Table { get; set; } public string Id { get; set; } } } When the MessagingCenter is sent a message for push-to-sync, it will execute the refresh command, thus refreshing the data. All that remains is to actually send that message in response to the notification. For Android, this is done in the Services\\GcmHandler.cs file in the OnMessage() method: protected override void OnMessage(Context context, Intent intent) { Log.Info(\"GcmService\", $\"Message {intent.ToString()}\"); var op = intent.Extras.GetString(\"op\"); if (op != null) { var syncMessage = new PushToSync() { Table = intent.Extras.GetString(\"table\"), Id = intent.Extras.GetString(\"id\") }; MessagingCenter.Send<PushToSync>(syncMessage, \"ItemsChanged\"); } else { var message = intent.Extras.GetString(\"message\") ?? \"Unknown Message\"; var picture = intent.Extras.GetString(\"picture\"); CreateNotification(\"TaskList\", message, picture); } } For iOS, the send happens in the ProcessNotification() method of the AppDelegate.cs class: private void ProcessNotification(NSDictionary options, bool fromFinishedLoading) { if (!(options != null && options.ContainsKey(new NSString(\"aps\")))) return; NSDictionary aps = options.ObjectForKey(new NSString(\"aps\")) as NSDictionary; if (!fromFinishedLoading) { var alertString = GetStringFromOptions(aps, \"alert\"); if (!string.IsNullOrEmpty(alertString)) { // Create the alert (removed for brevity) } var opString = GetStringFromOptions(aps, \"op\"); if (!string.IsNullOrEmpty(opString) && opString.Equals(\"sync\")) { var syncMessage = new PushToSync() { Table = GetStringFromOptions(aps, \"table\"), Id = GetStringFromOptions(aps, \"id\") }; MessagingCenter.Send<PushToSync>(syncMessage, \"ItemsChanged\"); } } } When a client inserts or updates a record into the database on the server, PushToSync() is called. That emits a push notification in the proper form defined within the mobile app. When the mobile app receives that push notification, it sends an \"ItemsChanged\" event to the messaging center. The TaskList view subscribes to those events and performs a sync in response to that event. There are several things we could do to this code, including: Push to the UserId that owns the record only - this will reduce the number of pushes that happen. Only pull the specific record on the specific table that is needed. This is available in the sender object.","title":"Push to Sync"},{"location":"chapter5/windows/","text":"Push Notifications for Windows are handled by one of two services. Use the Microsoft Push Notification Service (or MPNS) for Windows Phone devices (not including Windows Phone 10). Use Windows Notification Service (or WNS) for Windows 10, including Phone, Universal Windows applications and Windows store applications. Inevitably, this will mean you are more interested in WNS than MPNS. Registering with WNS \u00b6 There are a few steps for registering with WNS: Apply for a Windows Developer Account . Register your application with the Windows Store. Configure Notification Hubs. Finally, add code to your project to register and handle push notifications. If you are pushing to WNS, the application doesn't have to be a mobile app - it can be a Universal Windows app running on your laptop or desktop development computer. This makes dealing with push notifications much easier than you would expect. Apply for a Windows Developer Account \u00b6 The first thing you need is a Windows Developer Account . This is a paid account, but the account is a one-time cost. If you have an MSDN subscription, the account is free. Just go to the MSDN Website , log in and check your MSDN Subscription. Under My Account there is a section entitled Windows and Windows Phone developer accounts . Click on the Get Code link to get your registration code. When you are prompted for payment, there is a place to put the code for a free registration. Warn Use the same account as the account you use to sign in to Visual Studio. Register your application with the Windows Store \u00b6 To register your application with the Windows Store: Open your solution in Visual Studio Right-click the TaskList.UWP project, select Store > Associate App with the Store . Click Next . Enter a unique name for your app, then click Reserve . Once the app list is updated, highlight the new app name, then click Next . Click Associate . This will create a Package.StoreAssociation.xml file. This file can easily be recreated by re-associating the app with the store. You should not check this file into source code control. If you use the .gitignore generator , then it is automatically added to your .gitignore file. Add Push Notifications to your App registration \u00b6 Using a web browser, navigate to the App Overview page on the Windows Developer Center. You will need to log in with the same account used for your Windows Developer Account. Click the application you just associated. Click the Get started link under Push Notifications . Click the Live Services site in the middle of the WNS section. Leave this page open, but make a note of the two values I have highlighted. You will need these values when you register your WNS connection with the notification hub. You should never reveal these values to anyone as they will enable anyone to impersonate your applications to the WNS push notification system, allowing them to push to your users. Configure Notification Hubs \u00b6 The Notification Hubs configuration follows a similar pattern to the other push notification systems: Log on to the Azure portal . Find your notification hub, then select Push notification services . Enter the Package SID from the Live Services site for your app in the appropriate box. Enter the application secret from the Live Services site in the Security Key box. Click Save . The notification hub will be updated with your security credentials. Tip If you think your application secret has been compromised (or you publish it in a book), go back to the Live Services site and click Generate new password . Then click Activate next to your new application secret. Finally, click Save at the bottom of the page. Copy the new secret into the Security Key field of your notification hub configuration for WNS and click Save there to store the new security key. Register for Push Notifications in your App \u00b6 Edit the App.xaml.cs file in your TaskList.UWP project. Add the following code to the OnLaunched() method: protected async override void OnLaunched(LaunchActivatedEventArgs e) { UWPPlatformProvider.Channel = await PushNotificationChannelManager .CreatePushNotificationChannelForApplicationAsync(); // Rest of the OnLaunched method goes here } We have also made the OnLaunched() method asynchronous to accomodate the push notification channel manager. The PushNotificationChannelManager class is in Windows.Networking.PushNotifications . We are going to store the channel in the UWPPlatformProvider : public static PushNotificationChannel Channel { get; set; } = null; This will make the channel globally available to our push registration code. Registering with Azure Mobile Apps \u00b6 The registration code is also in the UWPPlatformProvider since it is part of the IPlatformProvider interface we have been using: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (UWPPlatformProvider.Channel != null) { try { var registrationId = UWPPlatformProvider.Channel.Uri.ToString(); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"wns\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request var genericTemplate = new WindowsPushTemplate { Body = @\"<toast><visual><binding template=\"\"genericTemplate\"\"><text id=\"\"1\"\">$(message)</text></binding></visual></toast>\" }; genericTemplate.Headers.Add(\"X-WNS-Type\", \"wns/toast\"); installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var recordedInstallation = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); System.Diagnostics.Debug.WriteLine(\"Completed NH Push Installation\"); } catch (Exception ex) { System.Diagnostics.Debug.Fail($\"[UWPPlatformProvider]: Could not register with NH: {ex.Message}\"); } } } The template used by WNS is slightly different. It is based on the regular PushTemplate used by iOS and Android, but it has an extra \"Headers\" field. You must specify the X-WNS-Type , which can be one of the following: wns/toast wns/tile wns/badge wns/raw Each of these has their own properties and require a specific set of properties to be specified in the XML body. As a result of this configuration, you don't need to code anything to receive push messages - UWP already knows how to decode and display them. The WindowsPushTemplate is added to the Abstractions\\DeviceInstallation.cs file: public class WindowsPushTemplate : PushTemplate { public WindowsPushTemplate() : base() { Headers = new Dictionary<string, string>(); } [JsonProperty(PropertyName = \"headers\")] public Dictionary<string, string> Headers { get; set; } } Testing Notifications \u00b6 You can send an appropriately formed test message to WNS within Visual Studio: Open the Server Explorer . Expand Azure > Notification Hubs . Double-click the notification hub. Select Windows (WNS) > Toast as the type. Change the body if required. Click Send . The message will appear in the notification area as a \"New Notification\".","title":"Windows Push"},{"location":"chapter5/windows/#registering-with-wns","text":"There are a few steps for registering with WNS: Apply for a Windows Developer Account . Register your application with the Windows Store. Configure Notification Hubs. Finally, add code to your project to register and handle push notifications. If you are pushing to WNS, the application doesn't have to be a mobile app - it can be a Universal Windows app running on your laptop or desktop development computer. This makes dealing with push notifications much easier than you would expect.","title":"Registering with WNS"},{"location":"chapter5/windows/#apply-for-a-windows-developer-account","text":"The first thing you need is a Windows Developer Account . This is a paid account, but the account is a one-time cost. If you have an MSDN subscription, the account is free. Just go to the MSDN Website , log in and check your MSDN Subscription. Under My Account there is a section entitled Windows and Windows Phone developer accounts . Click on the Get Code link to get your registration code. When you are prompted for payment, there is a place to put the code for a free registration. Warn Use the same account as the account you use to sign in to Visual Studio.","title":"Apply for a Windows Developer Account"},{"location":"chapter5/windows/#register-your-application-with-the-windows-store","text":"To register your application with the Windows Store: Open your solution in Visual Studio Right-click the TaskList.UWP project, select Store > Associate App with the Store . Click Next . Enter a unique name for your app, then click Reserve . Once the app list is updated, highlight the new app name, then click Next . Click Associate . This will create a Package.StoreAssociation.xml file. This file can easily be recreated by re-associating the app with the store. You should not check this file into source code control. If you use the .gitignore generator , then it is automatically added to your .gitignore file.","title":"Register your application with the Windows Store"},{"location":"chapter5/windows/#add-push-notifications-to-your-app-registration","text":"Using a web browser, navigate to the App Overview page on the Windows Developer Center. You will need to log in with the same account used for your Windows Developer Account. Click the application you just associated. Click the Get started link under Push Notifications . Click the Live Services site in the middle of the WNS section. Leave this page open, but make a note of the two values I have highlighted. You will need these values when you register your WNS connection with the notification hub. You should never reveal these values to anyone as they will enable anyone to impersonate your applications to the WNS push notification system, allowing them to push to your users.","title":"Add Push Notifications to your App registration"},{"location":"chapter5/windows/#configure-notification-hubs","text":"The Notification Hubs configuration follows a similar pattern to the other push notification systems: Log on to the Azure portal . Find your notification hub, then select Push notification services . Enter the Package SID from the Live Services site for your app in the appropriate box. Enter the application secret from the Live Services site in the Security Key box. Click Save . The notification hub will be updated with your security credentials. Tip If you think your application secret has been compromised (or you publish it in a book), go back to the Live Services site and click Generate new password . Then click Activate next to your new application secret. Finally, click Save at the bottom of the page. Copy the new secret into the Security Key field of your notification hub configuration for WNS and click Save there to store the new security key.","title":"Configure Notification Hubs"},{"location":"chapter5/windows/#register-for-push-notifications-in-your-app","text":"Edit the App.xaml.cs file in your TaskList.UWP project. Add the following code to the OnLaunched() method: protected async override void OnLaunched(LaunchActivatedEventArgs e) { UWPPlatformProvider.Channel = await PushNotificationChannelManager .CreatePushNotificationChannelForApplicationAsync(); // Rest of the OnLaunched method goes here } We have also made the OnLaunched() method asynchronous to accomodate the push notification channel manager. The PushNotificationChannelManager class is in Windows.Networking.PushNotifications . We are going to store the channel in the UWPPlatformProvider : public static PushNotificationChannel Channel { get; set; } = null; This will make the channel globally available to our push registration code.","title":"Register for Push Notifications in your App"},{"location":"chapter5/windows/#registering-with-azure-mobile-apps","text":"The registration code is also in the UWPPlatformProvider since it is part of the IPlatformProvider interface we have been using: public async Task RegisterForPushNotifications(MobileServiceClient client) { if (UWPPlatformProvider.Channel != null) { try { var registrationId = UWPPlatformProvider.Channel.Uri.ToString(); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"wns\", PushChannel = registrationId }; // Set up tags to request installation.Tags.Add(\"topic:Sports\"); // Set up templates to request var genericTemplate = new WindowsPushTemplate { Body = @\"<toast><visual><binding template=\"\"genericTemplate\"\"><text id=\"\"1\"\">$(message)</text></binding></visual></toast>\" }; genericTemplate.Headers.Add(\"X-WNS-Type\", \"wns/toast\"); installation.Templates.Add(\"genericTemplate\", genericTemplate); // Register with NH var recordedInstallation = await client.InvokeApiAsync<DeviceInstallation, DeviceInstallation>( $\"/push/installations/{client.InstallationId}\", installation, HttpMethod.Put, new Dictionary<string, string>()); System.Diagnostics.Debug.WriteLine(\"Completed NH Push Installation\"); } catch (Exception ex) { System.Diagnostics.Debug.Fail($\"[UWPPlatformProvider]: Could not register with NH: {ex.Message}\"); } } } The template used by WNS is slightly different. It is based on the regular PushTemplate used by iOS and Android, but it has an extra \"Headers\" field. You must specify the X-WNS-Type , which can be one of the following: wns/toast wns/tile wns/badge wns/raw Each of these has their own properties and require a specific set of properties to be specified in the XML body. As a result of this configuration, you don't need to code anything to receive push messages - UWP already knows how to decode and display them. The WindowsPushTemplate is added to the Abstractions\\DeviceInstallation.cs file: public class WindowsPushTemplate : PushTemplate { public WindowsPushTemplate() : base() { Headers = new Dictionary<string, string>(); } [JsonProperty(PropertyName = \"headers\")] public Dictionary<string, string> Headers { get; set; } }","title":"Registering with Azure Mobile Apps"},{"location":"chapter5/windows/#testing-notifications","text":"You can send an appropriately formed test message to WNS within Visual Studio: Open the Server Explorer . Expand Azure > Notification Hubs . Double-click the notification hub. Select Windows (WNS) > Toast as the type. Change the body if required. Click Send . The message will appear in the notification area as a \"New Notification\".","title":"Testing Notifications"},{"location":"chapter6/angular/","text":"Angular Applications \u00b6 For much of the last couple of years, Angular has been the JavaScript framework of choice For front-end developers. Developed by Google, it is fully-featured, albeit complex, but with a solid community of developers willing to help, and the support for learning through tutorials , videos , and blogs . The most recent iteration, Angular 2, is taking off as a great framework as well. Learning a new framework is time consuming, but the outcomes can be remarkable. In this section, I'm going to adjust a single class in the Angular version of ToDoMVC so that it works with the Azure Mobile Apps JavaScript SDK. You can find the full source code in the Chapter6 project on the books GitHub page. Angular in ASP.NET MVC \u00b6 Before we get started, let's get the default ToDoMVC application running in our ASP.NET MVC framework. Add a Controller Method and a View \u00b6 Edit the Controllers\\SPAController.cs and add the following method: public ActionResult Angular() { return View(); } Also, add the following in Views\\SPA\\Angular.cshtml : @{ Layout = null; } <!doctype html> <html lang=\"en\" data-framework=\"angularjs\"> <head> <meta charset=\"utf-8\"> <title>AngularJS \u2022 TodoMVC</title> <link rel=\"stylesheet\" href=\"~/Content/spa/todomvc/base.css\"> <link rel=\"stylesheet\" href=\"~/Content/spa/todomvc/index.css\"> <style> [ng-cloak] { display: none; } </style> </head> <body ng-app=\"todomvc\"> <ng-view /> <script type=\"text/ng-template\" id=\"todomvc-index.html\"> <section id=\"todoapp\"> <header id=\"header\"> <h1>todos</h1> <form id=\"todo-form\" ng-submit=\"addTodo()\"> <input id=\"new-todo\" placeholder=\"What needs to be done?\" ng-model=\"newTodo\" ng-disabled=\"saving\" autofocus> </form> </header> <section id=\"main\" ng-show=\"todos.length\" ng-cloak> <input id=\"toggle-all\" type=\"checkbox\" ng-model=\"allChecked\" ng-click=\"markAll(allChecked)\"> <label for=\"toggle-all\">Mark all as complete</label> <ul id=\"todo-list\"> <li ng-repeat=\"todo in todos | filter:statusFilter track by $index\" ng-class=\"{completed: todo.completed, editing: todo == editedTodo}\"> <div class=\"view\"> <input class=\"toggle\" type=\"checkbox\" ng-model=\"todo.completed\" ng-change=\"toggleCompleted(todo)\"> <label ng-dblclick=\"editTodo(todo)\">{{todo.title}}</label> <button class=\"destroy\" ng-click=\"removeTodo(todo)\"></button> </div> <form ng-submit=\"saveEdits(todo, 'submit')\"> <input class=\"edit\" ng-trim=\"false\" ng-model=\"todo.title\" todo-escape=\"revertEdits(todo)\" ng-blur=\"saveEdits(todo, 'blur')\" todo-focus=\"todo == editedTodo\"> </form> </li> </ul> </section> <footer id=\"footer\" ng-show=\"todos.length\" ng-cloak> <span id=\"todo-count\"> <strong>{{remainingCount}}</strong> <ng-pluralize count=\"remainingCount\" when=\"{ one: 'item left', other: 'items left' }\"></ng-pluralize> </span> <ul id=\"filters\"> <li> <a ng-class=\"{selected: status == ''} \" href=\"#/\">All</a> </li> <li> <a ng-class=\"{selected: status == 'active'}\" href=\"#/active\">Active</a> </li> <li> <a ng-class=\"{selected: status == 'completed'}\" href=\"#/completed\">Completed</a> </li> </ul> <button id=\"clear-completed\" ng-click=\"clearCompletedTodos()\" ng-show=\"completedCount\">Clear completed</button> </footer> </section> <footer id=\"info\"> <p>Double-click to edit a todo</p> <p> Credits: <a href=\"http://twitter.com/cburgdorf\">Christoph Burgdorf</a>, <a href=\"http://ericbidelman.com\">Eric Bidelman</a>, <a href=\"http://jacobmumm.com\">Jacob Mumm</a> and <a href=\"http://blog.igorminar.com\">Igor Minar</a> </p> <p>Adjusted for Azure Mobile Apps by <a href=\"https://github.com/adrianhall\">Adrian Hall</a>.</p> <p>Part of <a href=\"http://todomvc.com\">TodoMVC</a></p> </footer> </script> <script src=\"~/Content/spa/todomvc/base.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-route.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-resource.min.js\"></script> <script src=\"~/Content/spa/angular/app.js\"></script> <script src=\"~/Content/spa/angular/controllers/todoCtrl.js\"></script> <script src=\"~/Content/spa/angular/services/todoStorage.js\"></script> <script src=\"~/Content/spa/angular/directives/todoFocus.js\"></script> <script src=\"~/Content/spa/angular/directives/todoEscape.js\"></script> </body> </html> I haven't done much to these except adjust the script links to resolve to the JavaScript CDN. In addition, there are some basic CSS/JS libraries that all the ToDoMVC applications use. I've copied those from the ToDoMVC site into my project in ~/Content/spa/todomvc . Copy the ToDoMVC application into Content \u00b6 I've created a directory ~/Content/spa/angular with a direct copy of the AngularJS application. Everything under the js directory has been copied, preserving the directory structure. At this point, you can publish your application and you will see the original ToDoMVC application prior to making it work with Azure Mobile Apps. Cloud Connectivity \u00b6 The logic for the storage of the data behind the task list all happens in services\\todoStorage.js , and our changes are all limited to that file. In this case, we will have a local cache of the data. This local cache is read at the beginning of the application. When the user wants to make a change to the data, we modify the data locally and remotely at the same time. We have a small complexity - the model used by ToDoMVC does not match the model on the backend. As a result, we need to do conversions between the two models when we perform backend operations. This is surprisingly common, especially when using backend databases that you do not control. Let's start with the basics. Here is the recipe for the promise-based Angular factory, with our Azure Mobile Apps initializer embedded: /*global angular */ /** * Services that persists and retrieves todos from localStorage or a backend API * if available. * * They both follow the same API, returning promises for all changes to the * model. */ angular.module('todomvc') .factory('todoStorage', function ($q) { var store = { todos: [], client: null, table: null, // Additional methods Here get: function() { }, delete: function(todo) { }, insert: function(todo) { }, put: function(todo, index) { }, clearCompleted: function() { } }; var deferred = $q.defer(); store.client = new WindowsAzure.MobileServiceClient(location.origin); store.table = store.client.getTable('todoitem'); deferred.resolve(store); return deferred.promise; }); Since our application is doing all the filtering client-side, we are going to cache the table data in the store.todos variable. I've created the API that the ToDoMVC application expects, but with empty contents. Each method is expected to return a promise that resolves to the new list of todo items. Getting the data is easier than the jQuery version as we don't have to deal with filtering: get: function () { var deferred = $q.defer(); store.table.read().then(function (items) { // Convert the items into todos for this application var todoList = items.map(function (item) { return { id: item.id, completed: item.complete, title: item.text }; }); angular.copy(todoList, store.todos); deferred.resolve(store.todos); }); return deferred.promise; }, Angular comes with a A+/Promise library that is referenced in a similar way to either the regular Promise API or like the jQuery deferred API. Here, I am creating a promise, then doing the work, resolving the promise when the work is complete. The Array.map() method in JavaScript is great for doing the work of converting one model to a new shape. Deleting, Updating and Inserting are all very similar to the jQuery version. Since we are maintaing a cache, we don't resolve the promise we need until the server comes back with the new data: delete: function (todo) { var deferred = $q.defer(); store.table.del({ id: todo.id }).done(function () { store.todos.splice(store.todos.indexOf(todo), 1); deferred.resolve(store.todos); }); return deferred.promise; }, insert: function (todo) { var deferred = $q.defer(); store.table.insert({ text: todo.title }).then(function (newItem) { todo.id = newItem.id; todo.title = newItem.text; todo.completed = newItem.complete; store.todos.push(todo); deferred.resolve(store.todos); }); return deferred.promise; }, put: function (todo, index) { var deferred = $q.defer(); store.table.update({ id: todo.id, text: todo.title, complete: todo.completed }) .then(function (item) { todo.title = item.text; todo.completed = item.complete; store.todos[index] = todo; deferred.resolve(store.todos); }); return deferred.promise; } The major reason for not updating the cache directly is that we need the ID of the new record. That ID is created on the server for us. We could, as an improvement, include the uuid package and generate a GUID on the client, storing that instead. Finally, there is a method for clearing (aka deleting) the completed records. This is difficult primarily because the server only handles one record at a time. The Angular promise library has an API for that called .all() . This method is given an array of promises and waits for all of them to be resolved. We can use this as follows: clearCompleted: function () { var deferred = $q.defer(); var promises = []; var completeTodos = store.todos.filter(function (todo) { return todo.completed; }); completeTodo.forEach(function (todo) { promises.push(store.table.del({ id: todo.id })); }); $q.all(promises).then(function () { var incompleteTodos = store.todos.filter(function (todo) { return !todo.completed; }); angular.copy(incompleteTodos, store.todos); deferred.resolve(store.todos); }); return deferred.promise; }, We spend our initial time creating a promise for each record to be deleted. That promise resolves when the record is deleted. Once all the records have been deleted, we filter the cache similarly. Angular Gotchas \u00b6 The main problem I see over and over is that the WindowsAzure.MobileServiceClient class is not available when it is used. By default, Angular waits for the DOMContentLoaded event, which signals that all the scripts have been loaded. Inevitably, when I look at the failing code, the call to initialize the MobileServiceClient is called outside of a factory. If you place the new WindowsAzure.MobileServiceClient() call inside of a service or factory, then you fix the two major problems. Firstly, the MobileServiceClient class will be available when called. Secondly, you instantiate a singleton copy of the MobileServiceClient, which is exactly what is required by the SDK. Authentication \u00b6 The Azure Mobile Apps JavaScript SDK includes a call client.login('provider') for server-flow authentication and a similar functionality for client-flow authentication. If you have configured your authentication service in the Azure Portal properly, then calling the .login() method will pop up a small window to complete the normal authentication flow. The token is then stored inside the MobileServiceClient object. When using authentication this way, it is vital that you have a singleton model for your MobileServiceClient. In this case, I would break down the backend connectivity into three or more distinct services - one for the client connection, one for authenticating users, and one for each table controller you wish to expose.","title":"Angular Applications"},{"location":"chapter6/angular/#angular-applications","text":"For much of the last couple of years, Angular has been the JavaScript framework of choice For front-end developers. Developed by Google, it is fully-featured, albeit complex, but with a solid community of developers willing to help, and the support for learning through tutorials , videos , and blogs . The most recent iteration, Angular 2, is taking off as a great framework as well. Learning a new framework is time consuming, but the outcomes can be remarkable. In this section, I'm going to adjust a single class in the Angular version of ToDoMVC so that it works with the Azure Mobile Apps JavaScript SDK. You can find the full source code in the Chapter6 project on the books GitHub page.","title":"Angular Applications"},{"location":"chapter6/angular/#angular-in-aspnet-mvc","text":"Before we get started, let's get the default ToDoMVC application running in our ASP.NET MVC framework.","title":"Angular in ASP.NET MVC"},{"location":"chapter6/angular/#add-a-controller-method-and-a-view","text":"Edit the Controllers\\SPAController.cs and add the following method: public ActionResult Angular() { return View(); } Also, add the following in Views\\SPA\\Angular.cshtml : @{ Layout = null; } <!doctype html> <html lang=\"en\" data-framework=\"angularjs\"> <head> <meta charset=\"utf-8\"> <title>AngularJS \u2022 TodoMVC</title> <link rel=\"stylesheet\" href=\"~/Content/spa/todomvc/base.css\"> <link rel=\"stylesheet\" href=\"~/Content/spa/todomvc/index.css\"> <style> [ng-cloak] { display: none; } </style> </head> <body ng-app=\"todomvc\"> <ng-view /> <script type=\"text/ng-template\" id=\"todomvc-index.html\"> <section id=\"todoapp\"> <header id=\"header\"> <h1>todos</h1> <form id=\"todo-form\" ng-submit=\"addTodo()\"> <input id=\"new-todo\" placeholder=\"What needs to be done?\" ng-model=\"newTodo\" ng-disabled=\"saving\" autofocus> </form> </header> <section id=\"main\" ng-show=\"todos.length\" ng-cloak> <input id=\"toggle-all\" type=\"checkbox\" ng-model=\"allChecked\" ng-click=\"markAll(allChecked)\"> <label for=\"toggle-all\">Mark all as complete</label> <ul id=\"todo-list\"> <li ng-repeat=\"todo in todos | filter:statusFilter track by $index\" ng-class=\"{completed: todo.completed, editing: todo == editedTodo}\"> <div class=\"view\"> <input class=\"toggle\" type=\"checkbox\" ng-model=\"todo.completed\" ng-change=\"toggleCompleted(todo)\"> <label ng-dblclick=\"editTodo(todo)\">{{todo.title}}</label> <button class=\"destroy\" ng-click=\"removeTodo(todo)\"></button> </div> <form ng-submit=\"saveEdits(todo, 'submit')\"> <input class=\"edit\" ng-trim=\"false\" ng-model=\"todo.title\" todo-escape=\"revertEdits(todo)\" ng-blur=\"saveEdits(todo, 'blur')\" todo-focus=\"todo == editedTodo\"> </form> </li> </ul> </section> <footer id=\"footer\" ng-show=\"todos.length\" ng-cloak> <span id=\"todo-count\"> <strong>{{remainingCount}}</strong> <ng-pluralize count=\"remainingCount\" when=\"{ one: 'item left', other: 'items left' }\"></ng-pluralize> </span> <ul id=\"filters\"> <li> <a ng-class=\"{selected: status == ''} \" href=\"#/\">All</a> </li> <li> <a ng-class=\"{selected: status == 'active'}\" href=\"#/active\">Active</a> </li> <li> <a ng-class=\"{selected: status == 'completed'}\" href=\"#/completed\">Completed</a> </li> </ul> <button id=\"clear-completed\" ng-click=\"clearCompletedTodos()\" ng-show=\"completedCount\">Clear completed</button> </footer> </section> <footer id=\"info\"> <p>Double-click to edit a todo</p> <p> Credits: <a href=\"http://twitter.com/cburgdorf\">Christoph Burgdorf</a>, <a href=\"http://ericbidelman.com\">Eric Bidelman</a>, <a href=\"http://jacobmumm.com\">Jacob Mumm</a> and <a href=\"http://blog.igorminar.com\">Igor Minar</a> </p> <p>Adjusted for Azure Mobile Apps by <a href=\"https://github.com/adrianhall\">Adrian Hall</a>.</p> <p>Part of <a href=\"http://todomvc.com\">TodoMVC</a></p> </footer> </script> <script src=\"~/Content/spa/todomvc/base.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-route.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-resource.min.js\"></script> <script src=\"~/Content/spa/angular/app.js\"></script> <script src=\"~/Content/spa/angular/controllers/todoCtrl.js\"></script> <script src=\"~/Content/spa/angular/services/todoStorage.js\"></script> <script src=\"~/Content/spa/angular/directives/todoFocus.js\"></script> <script src=\"~/Content/spa/angular/directives/todoEscape.js\"></script> </body> </html> I haven't done much to these except adjust the script links to resolve to the JavaScript CDN. In addition, there are some basic CSS/JS libraries that all the ToDoMVC applications use. I've copied those from the ToDoMVC site into my project in ~/Content/spa/todomvc .","title":"Add a Controller Method and a View"},{"location":"chapter6/angular/#copy-the-todomvc-application-into-content","text":"I've created a directory ~/Content/spa/angular with a direct copy of the AngularJS application. Everything under the js directory has been copied, preserving the directory structure. At this point, you can publish your application and you will see the original ToDoMVC application prior to making it work with Azure Mobile Apps.","title":"Copy the ToDoMVC application into Content"},{"location":"chapter6/angular/#cloud-connectivity","text":"The logic for the storage of the data behind the task list all happens in services\\todoStorage.js , and our changes are all limited to that file. In this case, we will have a local cache of the data. This local cache is read at the beginning of the application. When the user wants to make a change to the data, we modify the data locally and remotely at the same time. We have a small complexity - the model used by ToDoMVC does not match the model on the backend. As a result, we need to do conversions between the two models when we perform backend operations. This is surprisingly common, especially when using backend databases that you do not control. Let's start with the basics. Here is the recipe for the promise-based Angular factory, with our Azure Mobile Apps initializer embedded: /*global angular */ /** * Services that persists and retrieves todos from localStorage or a backend API * if available. * * They both follow the same API, returning promises for all changes to the * model. */ angular.module('todomvc') .factory('todoStorage', function ($q) { var store = { todos: [], client: null, table: null, // Additional methods Here get: function() { }, delete: function(todo) { }, insert: function(todo) { }, put: function(todo, index) { }, clearCompleted: function() { } }; var deferred = $q.defer(); store.client = new WindowsAzure.MobileServiceClient(location.origin); store.table = store.client.getTable('todoitem'); deferred.resolve(store); return deferred.promise; }); Since our application is doing all the filtering client-side, we are going to cache the table data in the store.todos variable. I've created the API that the ToDoMVC application expects, but with empty contents. Each method is expected to return a promise that resolves to the new list of todo items. Getting the data is easier than the jQuery version as we don't have to deal with filtering: get: function () { var deferred = $q.defer(); store.table.read().then(function (items) { // Convert the items into todos for this application var todoList = items.map(function (item) { return { id: item.id, completed: item.complete, title: item.text }; }); angular.copy(todoList, store.todos); deferred.resolve(store.todos); }); return deferred.promise; }, Angular comes with a A+/Promise library that is referenced in a similar way to either the regular Promise API or like the jQuery deferred API. Here, I am creating a promise, then doing the work, resolving the promise when the work is complete. The Array.map() method in JavaScript is great for doing the work of converting one model to a new shape. Deleting, Updating and Inserting are all very similar to the jQuery version. Since we are maintaing a cache, we don't resolve the promise we need until the server comes back with the new data: delete: function (todo) { var deferred = $q.defer(); store.table.del({ id: todo.id }).done(function () { store.todos.splice(store.todos.indexOf(todo), 1); deferred.resolve(store.todos); }); return deferred.promise; }, insert: function (todo) { var deferred = $q.defer(); store.table.insert({ text: todo.title }).then(function (newItem) { todo.id = newItem.id; todo.title = newItem.text; todo.completed = newItem.complete; store.todos.push(todo); deferred.resolve(store.todos); }); return deferred.promise; }, put: function (todo, index) { var deferred = $q.defer(); store.table.update({ id: todo.id, text: todo.title, complete: todo.completed }) .then(function (item) { todo.title = item.text; todo.completed = item.complete; store.todos[index] = todo; deferred.resolve(store.todos); }); return deferred.promise; } The major reason for not updating the cache directly is that we need the ID of the new record. That ID is created on the server for us. We could, as an improvement, include the uuid package and generate a GUID on the client, storing that instead. Finally, there is a method for clearing (aka deleting) the completed records. This is difficult primarily because the server only handles one record at a time. The Angular promise library has an API for that called .all() . This method is given an array of promises and waits for all of them to be resolved. We can use this as follows: clearCompleted: function () { var deferred = $q.defer(); var promises = []; var completeTodos = store.todos.filter(function (todo) { return todo.completed; }); completeTodo.forEach(function (todo) { promises.push(store.table.del({ id: todo.id })); }); $q.all(promises).then(function () { var incompleteTodos = store.todos.filter(function (todo) { return !todo.completed; }); angular.copy(incompleteTodos, store.todos); deferred.resolve(store.todos); }); return deferred.promise; }, We spend our initial time creating a promise for each record to be deleted. That promise resolves when the record is deleted. Once all the records have been deleted, we filter the cache similarly.","title":"Cloud Connectivity"},{"location":"chapter6/angular/#angular-gotchas","text":"The main problem I see over and over is that the WindowsAzure.MobileServiceClient class is not available when it is used. By default, Angular waits for the DOMContentLoaded event, which signals that all the scripts have been loaded. Inevitably, when I look at the failing code, the call to initialize the MobileServiceClient is called outside of a factory. If you place the new WindowsAzure.MobileServiceClient() call inside of a service or factory, then you fix the two major problems. Firstly, the MobileServiceClient class will be available when called. Secondly, you instantiate a singleton copy of the MobileServiceClient, which is exactly what is required by the SDK.","title":"Angular Gotchas"},{"location":"chapter6/angular/#authentication","text":"The Azure Mobile Apps JavaScript SDK includes a call client.login('provider') for server-flow authentication and a similar functionality for client-flow authentication. If you have configured your authentication service in the Azure Portal properly, then calling the .login() method will pop up a small window to complete the normal authentication flow. The token is then stored inside the MobileServiceClient object. When using authentication this way, it is vital that you have a singleton model for your MobileServiceClient. In this case, I would break down the backend connectivity into three or more distinct services - one for the client connection, one for authenticating users, and one for each table controller you wish to expose.","title":"Authentication"},{"location":"chapter6/jquery/","text":"One of the major changes that has happened within web applications in the past few years is the single page application, or SPA, coupled with the rise of JavaScript frameworks. No-one can directly support all the JavaScript frameworks out there, but this section will cover a couple of the main ones - jQuery , React and Angular . Azure Mobile Apps has a JavaScript SDK that can be used for accessing table controllers and identity services within your mobile backend. The JavaScript SDK and jQuery \u00b6 jQuery has a long history in web development at this point. It is easy to pick up and functionally great for small applications. The important thing to note here is that you cannot instantiate the Azure Mobile Apps client SDK until all the scripts are loaded. Fortunately, JavaScript (and jQuery) provide events when this happens. I tend to add SPA applications to ASP.NET MVC apps by making the main HTML page a View. First, add a controller called \"Controllers\\SPAController\" with the following contents: using System.Web.Mvc; namespace Backend.Controllers { public class SPAController : Controller { public ActionResult JQuery() { return View(); } } } Also, create a directory Views\\SPA and add a JQuery.cshtml file: @{ Layout = null; } <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width\"> <title>JQuery</title> <link rel=\"stylesheet\" href=\"~/Content/spa/jquery/application.css\"/> </head> <body> <div id=\"wrapper\"> <article> <header> <h2>Azure</h2> <h1>Mobile Apps</h1> <form id=\"add-item\"> <button type=\"submit\" id=\"refresh\">Refresh</button> <button type=\"submit\">Add</button> <div> <input type=\"text\" id=\"new-item-text\" placeholder=\"Enter new task\"/> </div> </form> </header> <ul id=\"todo-items\"></ul> <p id=\"summary\">Initializing...</p> </article> <footer> <ul id=\"errorlog\"></ul> </footer> </div> <script src=\"https://code.jquery.com/jquery-2.2.1.min.js\"></script> <script src=\"https://zumo.blob.core.windows.net/sdk/azure-mobile-apps-client.min.js\"></script> <script src=\"~/Content/spa/jquery/application.js\"></script> </body> </html> This will be accessed via the /SPA/JQuery path. Unlike other (mostly component based) applications, jQuery uses mark-up extensively, so we are really setting up the basics of the application. You can find the CSS on the GitHub repository as its contents are not germane to the discussion here. The main item to note in the scripts section is the path of the Azure Mobile Apps client. This is loaded from the ZUMO CDN. You can also download the JavaScript file (both minified and non-minified versions exist), and include it in your Scripts directory if you prefer a local copy. The ~/Content/spa/jquery/application.js file contains our JavaScript code. It starts with a standard IIFE (immediately invoked function expression) - a common method of wrapping code such that variables don't leak into the global namespace: (function () { \"use strict\"; $(onBrowserReady); /* Rest of the application */ })(); We cannot create a client definition until the \"DOMContentLoaded\" event has fired, which ensures that all libraries (including those slower libraries loaded from a CDN) have been loaded and executed. This, in turn, ensures that the WindowsAzure.MobileServiceClient object is available. The onBrowserReady() function will be called when this happens: var client, table; /** * Event handler, called when the browser has loaded all scripts * @event */ function onBrowserReady() { // Create a connection reference to our Azure Mobile Apps backend client = new WindowsAzure.MobileServiceClient(location.origin); // Create a table reference table = client.getTable('todoitem'); // Refresh the todoItems refreshDisplay(); // Wire up the UI event handler for the add item $('#add-item').submit(addItemHandler); $('#refresh').on('click', refreshDisplay); } Once the browser is ready for action, we create a MobileServiceClient , get a reference to the table, then wire up the rest of the UI that we need to handle. You may notice that the API that the JavaScript SDK for Azure Mobile Apps exposes is very similar to the .NET SDK. This is intentional across all the Azure Mobile Apps SDK. If you know the API surface of one version, it's likely you can take that knowledge to the other SDKs. You only have to learn the new programming language. Reading Table Data \u00b6 We can see this more clearly when working with the refreshDisplay() function: /** * Refresh the items within a page */ function refreshDisplay() { updateSummaryMessage('Loading data from Azure'); // Execute a query for uncompleted items and process table .where({ complete: false }) // Set up the query .read() // Send query and read results .then(createTodoItemList, handleError); } /** * Create the DOM for a single todo item * @param {Object} item the Todo item * @param {string} item.id the ID of the item * @param {bool} item.complete true if the item is complete * @param {string} item.text the text value * @returns {jQuery} jQuery DOM object */ function createTodoItem(item) { return $('<li>') .attr('data-todoitem-id', item.id) .append($('<button class=\"item-delete\">Delete</button>')) .append($('<input type=\"checkbox\" class=\"item-complete\">') .prop('checked', item.complete)) .append($('<div>') .append($('<input class=\"item-text\">').val(item.text))); } /** * Create a list of Todo Items * @param {TodoItem[]} items an array of items * @returns {void} */ function createTodoItemList(items) { // Cycle through each item received from Azure and add items to the item list var listItems = $.map(items, createTodoItem); $('#todo-items').empty().append(listItems).toggle(listItems.length > 0); updateSummaryMessage('<strong>' + items.length + '</strong> item(s)'); // Wire up event handlers $('.item-delete').on('click', deleteItemHandler); $('.item-text').on('change', updateItemTextHandler); $('.item-complete').on('change', updateItemCompleteHandler); } In the refreshDisplay() function, we can see the parallel with the LINQ language within the .NET world. JavaScript does not have a LINQ library, so Azure Mobile Apps provides a simplified version of LINQ (called QueryJS ) for use with Azure Mobile Apps. Once you have set up the appropriate query, calling .read() will actually execute the HTTP GET to obtain the results. The other two functions fill in the list of tasks with the appropriate HTML trimmings. There is no concept of \"offline-sync\" in the Azure Mobile Apps JavaScript SDK as there is not an equivalent of the SQLite database available. You can perform paging with .skip(n) and .take(n) , include the total number of records that would be returned without paging with .includeTotalCount() , order the returned results and filter by function instead of a specific value. For example, let's say you had an orders table and you wanted to produce a paged list of results where the state was OPEN (a constant) that belonged to a specific user, ordered by their completion date?: function filter (userId, state) { return this.owner === userId && this.state == state; } function processResults (results) { totalCount = results.totalCount; // Handle results here } table .where(filter, filteredUser, constants.OPEN) .orderBy('completionDate') .skip(startRecord) .take(pageSize) .includeTotalCount() .read() .then(processResults, handleError); You can get just the total count using .take(0).includeTotalCount() . QueryJS is extremely effective at (and optimized for) selecting the exact data you need to do live displays without extra data being transferred. Modifying Data \u00b6 In a similar way to the .InsertAsync() , .UpdateAsync() and .DeleteAsync() methods in the .NET SDK, there are methods for inserting, modifying and deleting data: /** * Given a sub-element of an LI, find the TodoItem ID associated with the list member * * @param {DOMElement} el the form element * @returns {string} the ID of the TodoItem */ function getTodoItemId(el) { return $(el).closest('li').attr('data-todoitem-id'); } /** * Event handler for when the user enters some text and clicks on Add * @param {Event} event the event that caused the request * @returns {void} */ function addItemHandler(event) { var textbox = $('#new-item-text'), itemText = textbox.val(); updateSummaryMessage('Adding New Item'); if (itemText !== '') { table.insert({ text: itemText, complete: false }).then(refreshDisplay, handleError); } textbox.val('').focus(); event.preventDefault(); } /** * Event handler for when the user clicks on Delete next to a todo item * @param {Event} event the event that caused the request * @returns {void} */ function deleteItemHandler(event) { var itemId = getTodoItemId(event.currentTarget); updateSummaryMessage('Deleting Item in Azure'); table .del({ id: itemId }) // Async send the deletion to backend .then(refreshDisplay, handleError); // Update the UI event.preventDefault(); } /** * Event handler for when the user updates the text of a todo item * @param {Event} event the event that caused the request * @returns {void} */ function updateItemTextHandler(event) { var itemId = getTodoItemId(event.currentTarget), newText = $(event.currentTarget).val(); updateSummaryMessage('Updating Item in Azure'); table .update({ id: itemId, text: newText }) // Async send the update to backend .then(refreshDisplay, handleError); // Update the UI event.preventDefault(); } /** * Event handler for when the user updates the completed checkbox of a todo item * @param {Event} event the event that caused the request * @returns {void} */ function updateItemCompleteHandler(event) { var itemId = getTodoItemId(event.currentTarget), isComplete = $(event.currentTarget).prop('checked'); updateSummaryMessage('Updating Item in Azure'); table .update({ id: itemId, complete: isComplete }) // Async send the update to backend .then(refreshDisplay, handleError); // Update the UI } We insert data by passing the object to insert to table.insert() , modify with table.update() and delete with table.del() . These functions all operate using promises. Once the promise returns, we call the refreshDisplay() method to refresh the data. In the case of the .insert() and .update() functions, the promise is called with the updated item (containing the extra fields that the server adds), allowing the code to update a cache if necessary and avoiding a round-trip for the search.","title":"jQuery Applications"},{"location":"chapter6/jquery/#the-javascript-sdk-and-jquery","text":"jQuery has a long history in web development at this point. It is easy to pick up and functionally great for small applications. The important thing to note here is that you cannot instantiate the Azure Mobile Apps client SDK until all the scripts are loaded. Fortunately, JavaScript (and jQuery) provide events when this happens. I tend to add SPA applications to ASP.NET MVC apps by making the main HTML page a View. First, add a controller called \"Controllers\\SPAController\" with the following contents: using System.Web.Mvc; namespace Backend.Controllers { public class SPAController : Controller { public ActionResult JQuery() { return View(); } } } Also, create a directory Views\\SPA and add a JQuery.cshtml file: @{ Layout = null; } <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width\"> <title>JQuery</title> <link rel=\"stylesheet\" href=\"~/Content/spa/jquery/application.css\"/> </head> <body> <div id=\"wrapper\"> <article> <header> <h2>Azure</h2> <h1>Mobile Apps</h1> <form id=\"add-item\"> <button type=\"submit\" id=\"refresh\">Refresh</button> <button type=\"submit\">Add</button> <div> <input type=\"text\" id=\"new-item-text\" placeholder=\"Enter new task\"/> </div> </form> </header> <ul id=\"todo-items\"></ul> <p id=\"summary\">Initializing...</p> </article> <footer> <ul id=\"errorlog\"></ul> </footer> </div> <script src=\"https://code.jquery.com/jquery-2.2.1.min.js\"></script> <script src=\"https://zumo.blob.core.windows.net/sdk/azure-mobile-apps-client.min.js\"></script> <script src=\"~/Content/spa/jquery/application.js\"></script> </body> </html> This will be accessed via the /SPA/JQuery path. Unlike other (mostly component based) applications, jQuery uses mark-up extensively, so we are really setting up the basics of the application. You can find the CSS on the GitHub repository as its contents are not germane to the discussion here. The main item to note in the scripts section is the path of the Azure Mobile Apps client. This is loaded from the ZUMO CDN. You can also download the JavaScript file (both minified and non-minified versions exist), and include it in your Scripts directory if you prefer a local copy. The ~/Content/spa/jquery/application.js file contains our JavaScript code. It starts with a standard IIFE (immediately invoked function expression) - a common method of wrapping code such that variables don't leak into the global namespace: (function () { \"use strict\"; $(onBrowserReady); /* Rest of the application */ })(); We cannot create a client definition until the \"DOMContentLoaded\" event has fired, which ensures that all libraries (including those slower libraries loaded from a CDN) have been loaded and executed. This, in turn, ensures that the WindowsAzure.MobileServiceClient object is available. The onBrowserReady() function will be called when this happens: var client, table; /** * Event handler, called when the browser has loaded all scripts * @event */ function onBrowserReady() { // Create a connection reference to our Azure Mobile Apps backend client = new WindowsAzure.MobileServiceClient(location.origin); // Create a table reference table = client.getTable('todoitem'); // Refresh the todoItems refreshDisplay(); // Wire up the UI event handler for the add item $('#add-item').submit(addItemHandler); $('#refresh').on('click', refreshDisplay); } Once the browser is ready for action, we create a MobileServiceClient , get a reference to the table, then wire up the rest of the UI that we need to handle. You may notice that the API that the JavaScript SDK for Azure Mobile Apps exposes is very similar to the .NET SDK. This is intentional across all the Azure Mobile Apps SDK. If you know the API surface of one version, it's likely you can take that knowledge to the other SDKs. You only have to learn the new programming language.","title":"The JavaScript SDK and jQuery"},{"location":"chapter6/jquery/#reading-table-data","text":"We can see this more clearly when working with the refreshDisplay() function: /** * Refresh the items within a page */ function refreshDisplay() { updateSummaryMessage('Loading data from Azure'); // Execute a query for uncompleted items and process table .where({ complete: false }) // Set up the query .read() // Send query and read results .then(createTodoItemList, handleError); } /** * Create the DOM for a single todo item * @param {Object} item the Todo item * @param {string} item.id the ID of the item * @param {bool} item.complete true if the item is complete * @param {string} item.text the text value * @returns {jQuery} jQuery DOM object */ function createTodoItem(item) { return $('<li>') .attr('data-todoitem-id', item.id) .append($('<button class=\"item-delete\">Delete</button>')) .append($('<input type=\"checkbox\" class=\"item-complete\">') .prop('checked', item.complete)) .append($('<div>') .append($('<input class=\"item-text\">').val(item.text))); } /** * Create a list of Todo Items * @param {TodoItem[]} items an array of items * @returns {void} */ function createTodoItemList(items) { // Cycle through each item received from Azure and add items to the item list var listItems = $.map(items, createTodoItem); $('#todo-items').empty().append(listItems).toggle(listItems.length > 0); updateSummaryMessage('<strong>' + items.length + '</strong> item(s)'); // Wire up event handlers $('.item-delete').on('click', deleteItemHandler); $('.item-text').on('change', updateItemTextHandler); $('.item-complete').on('change', updateItemCompleteHandler); } In the refreshDisplay() function, we can see the parallel with the LINQ language within the .NET world. JavaScript does not have a LINQ library, so Azure Mobile Apps provides a simplified version of LINQ (called QueryJS ) for use with Azure Mobile Apps. Once you have set up the appropriate query, calling .read() will actually execute the HTTP GET to obtain the results. The other two functions fill in the list of tasks with the appropriate HTML trimmings. There is no concept of \"offline-sync\" in the Azure Mobile Apps JavaScript SDK as there is not an equivalent of the SQLite database available. You can perform paging with .skip(n) and .take(n) , include the total number of records that would be returned without paging with .includeTotalCount() , order the returned results and filter by function instead of a specific value. For example, let's say you had an orders table and you wanted to produce a paged list of results where the state was OPEN (a constant) that belonged to a specific user, ordered by their completion date?: function filter (userId, state) { return this.owner === userId && this.state == state; } function processResults (results) { totalCount = results.totalCount; // Handle results here } table .where(filter, filteredUser, constants.OPEN) .orderBy('completionDate') .skip(startRecord) .take(pageSize) .includeTotalCount() .read() .then(processResults, handleError); You can get just the total count using .take(0).includeTotalCount() . QueryJS is extremely effective at (and optimized for) selecting the exact data you need to do live displays without extra data being transferred.","title":"Reading Table Data"},{"location":"chapter6/jquery/#modifying-data","text":"In a similar way to the .InsertAsync() , .UpdateAsync() and .DeleteAsync() methods in the .NET SDK, there are methods for inserting, modifying and deleting data: /** * Given a sub-element of an LI, find the TodoItem ID associated with the list member * * @param {DOMElement} el the form element * @returns {string} the ID of the TodoItem */ function getTodoItemId(el) { return $(el).closest('li').attr('data-todoitem-id'); } /** * Event handler for when the user enters some text and clicks on Add * @param {Event} event the event that caused the request * @returns {void} */ function addItemHandler(event) { var textbox = $('#new-item-text'), itemText = textbox.val(); updateSummaryMessage('Adding New Item'); if (itemText !== '') { table.insert({ text: itemText, complete: false }).then(refreshDisplay, handleError); } textbox.val('').focus(); event.preventDefault(); } /** * Event handler for when the user clicks on Delete next to a todo item * @param {Event} event the event that caused the request * @returns {void} */ function deleteItemHandler(event) { var itemId = getTodoItemId(event.currentTarget); updateSummaryMessage('Deleting Item in Azure'); table .del({ id: itemId }) // Async send the deletion to backend .then(refreshDisplay, handleError); // Update the UI event.preventDefault(); } /** * Event handler for when the user updates the text of a todo item * @param {Event} event the event that caused the request * @returns {void} */ function updateItemTextHandler(event) { var itemId = getTodoItemId(event.currentTarget), newText = $(event.currentTarget).val(); updateSummaryMessage('Updating Item in Azure'); table .update({ id: itemId, text: newText }) // Async send the update to backend .then(refreshDisplay, handleError); // Update the UI event.preventDefault(); } /** * Event handler for when the user updates the completed checkbox of a todo item * @param {Event} event the event that caused the request * @returns {void} */ function updateItemCompleteHandler(event) { var itemId = getTodoItemId(event.currentTarget), isComplete = $(event.currentTarget).prop('checked'); updateSummaryMessage('Updating Item in Azure'); table .update({ id: itemId, complete: isComplete }) // Async send the update to backend .then(refreshDisplay, handleError); // Update the UI } We insert data by passing the object to insert to table.insert() , modify with table.update() and delete with table.del() . These functions all operate using promises. Once the promise returns, we call the refreshDisplay() method to refresh the data. In the case of the .insert() and .update() functions, the promise is called with the updated item (containing the extra fields that the server adds), allowing the code to update a cache if necessary and avoiding a round-trip for the search.","title":"Modifying Data"},{"location":"chapter6/mvc/","text":"At some point, you will likely want to pair your mobile application with a web interface. This may be because you have a simplified mobile app whereas you may have a more fully featured app within the web. For example, I see this design featured prominently in fitness apps. The mobile app is a news feed and recording device, whereas the web interface contains all the fitness analytics. You may also have some sort of administrative interface that provides an alternate view of the data. Whatever the reason you decide to support web and mobile together, you will need to convert your Azure Mobile App backend to a fully-fledged ASP.NET MVC application. Fortunately, the process of merging Azure Mobile Apps with an existing ASP.NET MVC application is simple. Doing the reverse (merging MVC into Azure Mobile Apps) is considerably more complex. Start by creating a new ASP.NET application with File -> New Project. Select the ASP.NET Web Application (.NET Framework) project template. Then select th MVC template. Change the Authentication to No Authentication . Click OK to create the project. Run your project to ensure it is working correctly. Why is merging MVC into Azure Mobile Apps so hard? ASP.NET requires a large number of NuGet packages to implement MVC. These are provided for you when you start from the appropriate template, but you will need to add them yourself when you start from the Azure Mobile Apps template. Now that you have an MVC project, let's add Azure Mobile Apps to it. Start by adding the following two NuGet packages to your project: Microsoft.Azure.Mobile.Server.Quickstart Microsoft.Owin.Host.SystemWeb The Microsoft.Azure.Mobile.Server.Quickstart NuGet package contains dependencies for all the other Azure Mobile Apps SDK requirements. If you want the big long list instead, add the following: AutoMapper EntityFramework Microsoft.AspNet.WebApi.Client Microsoft.AspNet.WebApi.Core Microsoft.AspNet.WebApi.Owin Microsoft.Azure.Mobile.Server Microsoft.Azure.Mobile.Server.Authentication Microsoft.Azure.Mobile.Server.Notifications Microsoft.Azure.NotificationHubs Microsoft.Data.Edm Microsoft.Owin Microsoft.Owin.Security Microsoft.WindowsAzure.ConfigurationManager Owin System.IdentityModel.Tokens.Jwt (v4.0.x - do not install v5.x) System.Spatial Custom Authentication If you are using custom authentication, then you need to produce the entire login flow for both web and mobile sides. There is no assistance provided with the platform. You will also need to add the Microsoft.Azure.Mobile.Server.Login package to your project. Using the Quickstart package is a serious time saver over having to type in 16 package names. The SystemWeb package enables the use of the Owin Startup.cs class. This is used to bootstrap the Azure Mobile Apps configuration. Upgrading to .NET 4.6 If you want to run your ASP.NET service under .NET Framework 4.6, you can upgrade just about everything. However, the System.IdentityModel.Tokens.Jwt package should not be upgraded - leave it on the latest v4.x release. Do not upgrade AutoMapper beyond v3.3.1 if you are using the MappedEntityDomainManager class. You can then copy the the following files from your original Azure Mobile Apps server project to the new project. App_Start\\Startup.MobileApp.cs Controllers\\*.cs DataObjects\\*.cs Models\\MobileServiceContext.cs Finally, adjust or create the Startup.cs file as follows: using Microsoft.Owin; using Owin; [assembly: OwinStartup(typeof(Backend.Startup))] namespace Backend { public partial class Startup { public void Configuration(IAppBuilder app) { ConfigureMobileApp(app); } } } Note the addition of the ConfigureMobileApp() call. If you are starting from the suggested template, this file does not exist and you will need to create it. Finally, you must update the Web.config file. Firstly, in the <configSections> tag, add the following to support Entity Framework: <section name=\"entityFramework\" type=\"System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\" requirePermission=\"false\" /> Add the following <connectionStrings> section: <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"Data Source=(localdb)\\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\\aspnet-Backend.mdf;Initial Catalog=aspnet-Backend-20160720081828;Integrated Security=True;MultipleActiveResultSets=True\" providerName=\"System.Data.SqlClient\" /> </connectionStrings> Add the following entries to the <appSettings> section: <appSettings> <add key=\"webpages:Enabled\" value=\"false\" /> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> <add key=\"SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"ValidAudience\" value=\"https://chapter6.azurewebsites.net/\" /> <add key=\"ValidIssuer\" value=\"https://chapter6.azurewebsites.net/\" /> </appSettings> The first appSetting key should already be present. These changes can be copied from the Web.config file from your Azure Mobile Apps project. Sharing the Database \u00b6 Underneath the covers, Azure Mobile Apps uses EntityFramework to access the database. It requires certain adjustments to the models, as we discussed in Chapter 3 . However, you can still use the same Entity Framework context to access the database. There are some caveats that must be followed, however: Inserts must set the fields that are not managed by the database (such as Id ). Deletes must set the Deleted column if using Soft Delete, instead of directly deleting records. Before you get started, you have to enable EF code-first migrations. If you don't, you will get an error about a duplicate clustered index in your application for each table based on EntityData. To enable migrations: Open the Package Manager Console in Visual Studio Run Enable-Migrations Adjust the created Migrations\\Configuration.cs constructor as follows: csharp public Configuration() { AutomaticMigrationsEnabled = false; SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator()); } In your App_Start\\Startup.MobileApp.cs , comment out or remove your Database.SetInitializer() call, and replace with: csharp var migrator = new DbMigrator(new Migrations.Configuration()); migrator.Update(); You can also remove the MobileServiceInitializer class in the same file, if you wish. Run Add-Migration initial in the Package Manager Console. This is an abbreviated set of instructions from our work in Chapter 3 . As an example, let's create a default view for handling our TodoItem controller. In MVC, you need a Model, View and Controller class. The Model will be handled by our existing DataObjects\\TodoItem.cs class. The Controller and View will be new classes. Let's take a look at the replacement HomeController.cs class first: using System.Linq; using System.Web.Mvc; using Backend.Models; namespace Backend.Controllers { public class HomeController : Controller { private MobileServiceContext context; public HomeController() { context = new MobileServiceContext(); } public ActionResult Index() { var list = context.TodoItems.ToList(); return View(list); } [HttpPost] [ValidateAntiForgeryToken] public async Task<ActionResult> Create([Bind(Include = \"Text\")]TodoItem item) { try { if (ModelState.IsValid) { item.Id = Guid.NewGuid().ToString(\"N\"); context.TodoItems.Add(item); await context.SaveChangesAsync(); } } catch (DataException) { ModelState.AddModelError(\"\", \"Unable to save changes.\"); } return RedirectToAction(\"Index\"); } } } I am using the existing MobileServiceContext as the Entity Framework context. The list of tasks is taken directly from the DbSet<> that was established for the mobile backend table controller. I also have a method for creating a new todo item within the controller. If you look for a tutorial on implementing CRUD in ASP.NET MVC, it's likely you will see code similar to this. Info I've removed some additional views from this backend (About and Contact) plus the links in the layout partial view. This is just to make the code cleaner. You can leave them in if you desire. The view in Views\\Home\\Index.cshtml is similarly changed: @{ ViewBag.Title = \"Home Page\"; } @model IEnumerable<Backend.DataObjects.TodoItem> <div class=\"row\" style=\"margin-top: 8px;\"> <div class=\"col-md-1\"></div> <div class=\"col-md-10\"> @using (Html.BeginForm(\"Create\", \"Home\", FormMethod.Post)) { @Html.AntiForgeryToken() <input type=\"text\" name=\"Text\" placeholder=\"Enter TodoItem Text...\"/> <input type=\"submit\" value=\"Add Todo Item\"/> } </div> <div class=\"col-md-1\"></div> </div> <div class=\"row\" style=\"margin-top: 8px;\"> <div class=\"col-md-1\"></div> <div class=\"col-md-10\"> <div class=\"table-responsive\"> <table class=\"table table-striped table-bordered table-hover table-condensed\"> <thead> <tr> <th>#</th> <th>Text</th> <th>Complete</th> </tr> </thead> <tbody> @foreach (var item in Model) { <tr> <td>@Html.DisplayFor(modelItem => item.Id)</td> <td>@Html.DisplayFor(modelItem => item.Text)</td> <td>@Html.DisplayFor(modelItem => item.Complete)</td> </tr> } </tbody> </table> </div> </div> <div class=\"col-md-1\"></div> </div> The HTML classes are from Bootstrap - a common CSS framework. The first row div encapsulates the form for adding a new todo item, and the second row div encapsulates the list. If you enter some text into the box and click the submit button, it will be added to the database. Sharing Authentication \u00b6 Custom Authentication This section only pertains to using the standard authentication techniques provided with Azure App Service. It does not pertain to custom authentication. If you are using custom authentication, then you need to produce the entire login flow for both web and mobile sides. There is no assistance provided with the platform. As one would suspect, setting up App Service Authentication , then adding an [Authorize] attribute to the HomeController is a good starting point for making our application authenticated. However, it isn't enough. If you just do this, you will got something like the following: In order to properly capture the login flow, we have to redirect a missing authentication to the appropriate authentication endpoint. In ASP.NET, this functionality is configured within the Web.config file. Locate the <system.web> section and add the following: <system.web> <compilation debug=\"true\" targetFramework=\"4.5.2\"/> <httpRuntime targetFramework=\"4.5.2\"/> <authentication mode=\"Forms\"> <forms loginUrl=\"/.auth/login/aad\" timeout=\"2880\"/> </authentication> </system.web> The <compilation> and <httpRuntime> values should already be present. The <authentication> section is new. Once you deploy this code to Azure App Service, you will be redirected to your authentication provider. Once the authentication process is complete, you will receive a page indicating successful authentication, with a link back to the website. Use a private browsing window for testing One of the major problems with using Azure AD for authentication as a developer is that the Azure Portal authentication uses the same providers. This can cause problems in your app. Always use a private browsing window and/or a different browser for testing your code. Using Anti-Forgery Tokens \u00b6 One of the gotchas for using ASP.NET MVC with Azure App Service Authentication is that the Anti-Forgery Token no longer works as advertised. If you try to use the anti-forgery token on POST operations (and the associated [ValidateAntiForgeryToken] within your controller), you will receive the following exception: A claim of type 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier' or 'http://schemas.microsoft.com/accesscontrolservice/2010/07/claims/identityprovider' was not present on the provided ClaimsIdentity. To enable anti-forgery token support with claims-based authentication, please verify that the configured claims provider is providing both of these claims on the ClaimsIdentity instances it generates. If the configured claims provider instead uses a different claim type as a unique identifier, it can be configured by setting the static property AntiForgeryConfig.UniqueClaimTypeIdentifier. Unfortunately, the logic here is wrong. Check the source code and you will see that both listed claims must be present to not throw the exception. The Azure App Service Authentication claims do not include the identityprovider claim. With the advent of .NET Core, I do not expect this bug to be fixed. The workaround is to explicitly specify the identifier to use somewhere in your application startup: AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier; I place this in the MVC specific App_Start\\RouteConfig.cs file: namespace Backend { public class RouteConfig { public static void RegisterRoutes(RouteCollection routes) { routes.IgnoreRoute(\"{resource}.axd/{*pathInfo}\"); routes.MapRoute( name: \"Default\", url: \"{controller}/{action}/{id}\", defaults: new { controller = \"Home\", action = \"Index\", id = UrlParameter.Optional } ); AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier; } } } This will force the use of the (singular) claim rather than requiring both claims to be present, thus allowing you to use the anti-forgery token.","title":"MVC Applications"},{"location":"chapter6/mvc/#sharing-the-database","text":"Underneath the covers, Azure Mobile Apps uses EntityFramework to access the database. It requires certain adjustments to the models, as we discussed in Chapter 3 . However, you can still use the same Entity Framework context to access the database. There are some caveats that must be followed, however: Inserts must set the fields that are not managed by the database (such as Id ). Deletes must set the Deleted column if using Soft Delete, instead of directly deleting records. Before you get started, you have to enable EF code-first migrations. If you don't, you will get an error about a duplicate clustered index in your application for each table based on EntityData. To enable migrations: Open the Package Manager Console in Visual Studio Run Enable-Migrations Adjust the created Migrations\\Configuration.cs constructor as follows: csharp public Configuration() { AutomaticMigrationsEnabled = false; SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator()); } In your App_Start\\Startup.MobileApp.cs , comment out or remove your Database.SetInitializer() call, and replace with: csharp var migrator = new DbMigrator(new Migrations.Configuration()); migrator.Update(); You can also remove the MobileServiceInitializer class in the same file, if you wish. Run Add-Migration initial in the Package Manager Console. This is an abbreviated set of instructions from our work in Chapter 3 . As an example, let's create a default view for handling our TodoItem controller. In MVC, you need a Model, View and Controller class. The Model will be handled by our existing DataObjects\\TodoItem.cs class. The Controller and View will be new classes. Let's take a look at the replacement HomeController.cs class first: using System.Linq; using System.Web.Mvc; using Backend.Models; namespace Backend.Controllers { public class HomeController : Controller { private MobileServiceContext context; public HomeController() { context = new MobileServiceContext(); } public ActionResult Index() { var list = context.TodoItems.ToList(); return View(list); } [HttpPost] [ValidateAntiForgeryToken] public async Task<ActionResult> Create([Bind(Include = \"Text\")]TodoItem item) { try { if (ModelState.IsValid) { item.Id = Guid.NewGuid().ToString(\"N\"); context.TodoItems.Add(item); await context.SaveChangesAsync(); } } catch (DataException) { ModelState.AddModelError(\"\", \"Unable to save changes.\"); } return RedirectToAction(\"Index\"); } } } I am using the existing MobileServiceContext as the Entity Framework context. The list of tasks is taken directly from the DbSet<> that was established for the mobile backend table controller. I also have a method for creating a new todo item within the controller. If you look for a tutorial on implementing CRUD in ASP.NET MVC, it's likely you will see code similar to this. Info I've removed some additional views from this backend (About and Contact) plus the links in the layout partial view. This is just to make the code cleaner. You can leave them in if you desire. The view in Views\\Home\\Index.cshtml is similarly changed: @{ ViewBag.Title = \"Home Page\"; } @model IEnumerable<Backend.DataObjects.TodoItem> <div class=\"row\" style=\"margin-top: 8px;\"> <div class=\"col-md-1\"></div> <div class=\"col-md-10\"> @using (Html.BeginForm(\"Create\", \"Home\", FormMethod.Post)) { @Html.AntiForgeryToken() <input type=\"text\" name=\"Text\" placeholder=\"Enter TodoItem Text...\"/> <input type=\"submit\" value=\"Add Todo Item\"/> } </div> <div class=\"col-md-1\"></div> </div> <div class=\"row\" style=\"margin-top: 8px;\"> <div class=\"col-md-1\"></div> <div class=\"col-md-10\"> <div class=\"table-responsive\"> <table class=\"table table-striped table-bordered table-hover table-condensed\"> <thead> <tr> <th>#</th> <th>Text</th> <th>Complete</th> </tr> </thead> <tbody> @foreach (var item in Model) { <tr> <td>@Html.DisplayFor(modelItem => item.Id)</td> <td>@Html.DisplayFor(modelItem => item.Text)</td> <td>@Html.DisplayFor(modelItem => item.Complete)</td> </tr> } </tbody> </table> </div> </div> <div class=\"col-md-1\"></div> </div> The HTML classes are from Bootstrap - a common CSS framework. The first row div encapsulates the form for adding a new todo item, and the second row div encapsulates the list. If you enter some text into the box and click the submit button, it will be added to the database.","title":"Sharing the Database"},{"location":"chapter6/mvc/#sharing-authentication","text":"Custom Authentication This section only pertains to using the standard authentication techniques provided with Azure App Service. It does not pertain to custom authentication. If you are using custom authentication, then you need to produce the entire login flow for both web and mobile sides. There is no assistance provided with the platform. As one would suspect, setting up App Service Authentication , then adding an [Authorize] attribute to the HomeController is a good starting point for making our application authenticated. However, it isn't enough. If you just do this, you will got something like the following: In order to properly capture the login flow, we have to redirect a missing authentication to the appropriate authentication endpoint. In ASP.NET, this functionality is configured within the Web.config file. Locate the <system.web> section and add the following: <system.web> <compilation debug=\"true\" targetFramework=\"4.5.2\"/> <httpRuntime targetFramework=\"4.5.2\"/> <authentication mode=\"Forms\"> <forms loginUrl=\"/.auth/login/aad\" timeout=\"2880\"/> </authentication> </system.web> The <compilation> and <httpRuntime> values should already be present. The <authentication> section is new. Once you deploy this code to Azure App Service, you will be redirected to your authentication provider. Once the authentication process is complete, you will receive a page indicating successful authentication, with a link back to the website. Use a private browsing window for testing One of the major problems with using Azure AD for authentication as a developer is that the Azure Portal authentication uses the same providers. This can cause problems in your app. Always use a private browsing window and/or a different browser for testing your code.","title":"Sharing Authentication"},{"location":"chapter6/mvc/#using-anti-forgery-tokens","text":"One of the gotchas for using ASP.NET MVC with Azure App Service Authentication is that the Anti-Forgery Token no longer works as advertised. If you try to use the anti-forgery token on POST operations (and the associated [ValidateAntiForgeryToken] within your controller), you will receive the following exception: A claim of type 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier' or 'http://schemas.microsoft.com/accesscontrolservice/2010/07/claims/identityprovider' was not present on the provided ClaimsIdentity. To enable anti-forgery token support with claims-based authentication, please verify that the configured claims provider is providing both of these claims on the ClaimsIdentity instances it generates. If the configured claims provider instead uses a different claim type as a unique identifier, it can be configured by setting the static property AntiForgeryConfig.UniqueClaimTypeIdentifier. Unfortunately, the logic here is wrong. Check the source code and you will see that both listed claims must be present to not throw the exception. The Azure App Service Authentication claims do not include the identityprovider claim. With the advent of .NET Core, I do not expect this bug to be fixed. The workaround is to explicitly specify the identifier to use somewhere in your application startup: AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier; I place this in the MVC specific App_Start\\RouteConfig.cs file: namespace Backend { public class RouteConfig { public static void RegisterRoutes(RouteCollection routes) { routes.IgnoreRoute(\"{resource}.axd/{*pathInfo}\"); routes.MapRoute( name: \"Default\", url: \"{controller}/{action}/{id}\", defaults: new { controller = \"Home\", action = \"Index\", id = UrlParameter.Optional } ); AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier; } } } This will force the use of the (singular) claim rather than requiring both claims to be present, thus allowing you to use the anti-forgery token.","title":"Using Anti-Forgery Tokens"},{"location":"chapter6/react/","text":"React Applications \u00b6 There have been two major shifts in the way that JavaScript applications are built in the past few years. The first is in the user of transpilers for compiling another variant of JavaScript into something that a browser can use. There are languages that add whole new feature sets to the language, like CoffeeScript , TypeScript and Dart , plus others that extend JavaScript to include unsupported features of the JavaScript language early, like BabelJS which provides access to the latest specifications for JavaScript on the current set of browsers. The process of converting one of these languages into more normal JavaScript is called transpiling. The resultant code is rarely readable. The other major shift is in the use of bundlers. In prior iterations of the JavaScript language, there was no module system. Developers included a number of <script> tags to load the code. As JavaScript became more of a development platform, the size of the code grew and the need for a module system also grew. There are three in general use - RequireJS , CommonJS and ES2015 modules . In general, developers will use a bundler like Browserify or Webpack to bundle their application source together so that it loads faster and they can take care of alternate module specifications like CommonJS and ES2015 modules. (RequireJS doesn't require bundling). This brings us to React applications. React is a newcomer to the JavaScript framework scene. It provides a component model for your application. It is generally paired with a Flux state manager like Redux . Thus, most React applications can more properly be called React/Redux applications. While React can be used with just straight JavaScript and script tags, it is more normal to use ES2015 syntax and a bundler like Webpack to distribute the application. Setting Up \u00b6 In this example, we are going to walk through the set up of an application that uses Webpack and the BabelJS transpiler to build our web application into our MVC framework. MVC Controller & View \u00b6 As with the jQuery and Angular examples, we need a controller (within the Controllers\\SPAController.cs file): public ActionResult React() { return View(); } We also need a suitable View. This is similar in construction to the Angular view, but it requires some different elements, as is normal when you change a JavaScript framework. This is Views\\SPA\\React.cshtml : @{ Layout = null; } <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Azure Mobile Apps Quickstart</title> <link rel=\"stylesheet\" href=\"~/Content/spa/react/app.css\" /> </head> <body> <div id=\"app\"></div> <script src=\"~/Content/spa/react/vendor.bundle.js\"></script> <script src=\"~/Content/spa/react/app.js\"></script> </body> </html> This view is much simpler than the Angular view because none of the markup for the actual application is contained within the view. All markup is done by the React components in our code. There are three files referenced in this file that do not yet exist. The JavaScript files will be created by the Webpack bundler, and the app.css file is the regular CSS file from the normal Quickstart application. Setting up a Webpack project \u00b6 There is some work to do in order to set up a reasonable Webpack-enabled project. Let's first of all set up a basic React application in a new directory called ReactClient . Here is the ReactClient\\app.jsx file that will serve as our entry-point into the application: import React from 'react'; import ReactDOM from 'react-dom'; ReactDOM.render( <h1>In a React Application</h1> , document.getElementById('app') ); Once we have the build correctly done, the application will insert the <h1>In a React Application</h1> inside the <div id=\"app\"></div> from the main page. This is the most basic of React applications, but we can already see some things we may not have seen before. The import statements bring in ES2015 modules for use. Also, you can embed pseudo-HTML into a React application rendering method by using JSX . When the application is transpiled with BabelJS, this pseudo-HTML get compiled into normal JavaScript. There are three other files we want to produce that are build related. The first is the .babelrc file. This contains instructions to the BabelJS transpiler that tells it what specifications need to be transpiled: { \"presets\": [ \"es2015\", \"react\" ], \"plugins\": [ \"transform-class-properties\", \"transform-object-rest-spread\" ] } Our application will be written in ES2015 syntax with React extensions (those are presets) and I write with a couple of specifications that may or may not make it into the next JavaScript specification, but are useful - Class Properties and Object Rest Spread . Although these are considered \"experimental\", they make your life much easier as a developer. The second file is webpack.config.js . This is a JavaScript file that tells the Webpack system how to build the application. Here is a suitable file for this application: 'use strict'; /* global __dirname */ var path = require('path'), webpack = require('webpack'); var configuration = { devtool: 'source-map', entry: { app: [ path.join(__dirname, 'ReactClient/app.jsx') ], vendor: [ 'react', 'react-dom' ] }, module: { loaders: [ { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ } ] }, output: { path: path.join(__dirname, 'Content/spa/react'), publicPath: '/', filename: 'app.js' }, plugins: [ new webpack.optimize.CommonsChunkPlugin('vendor', 'vendor.bundle.js'), new webpack.optimize.DedupePlugin() ], resolve: { modulesDirectories: [ 'node_modules' ], extensions: [ '', '.js', '.jsx' ] }, target: 'web' }; module.exports = configuration; There are three areas of the configuration object that are interesting. The entry block contains the entry-point for the application, but it also contains a vendor array. This array is the list of modules that will be placed into the vendor.bundle.js file. When we add a new external module, we need to add it to the list so that it is included in the bundle. The module section contains instructions on how to transpile the source files. The single entry tells Webpack to run all JavaScript and JSX files through the BabelJS transpiler. Finally, the output section tells Webpack where to store the output files. Our final file is package.json . This is a fairly standard file for loading modules from the NPM repository. You can create one using npm init --yes , then use npm install modulename --save to install the individual modules. Alternatively, you can create the package.json file and then run npm install to install the modules that are named in the file. { \"version\": \"0.0.1\", \"name\": \"react-quickstart\", \"private\": true, \"main\": \"ReactClient/app.jsx\", \"scripts\": { \"build\": \"webpack -p\" }, \"devDependencies\": {}, \"dependencies\": { \"babel-core\": \"^6.21.0\", \"babel-loader\": \"^6.2.10\", \"babel-plugin-transform-class-properties\": \"^6.19.0\", \"babel-plugin-transform-object-rest-spread\": \"^6.20.2\", \"babel-preset-es2015\": \"^6.18.0\", \"babel-preset-react\": \"^6.16.0\", \"react\": \"^15.4.1\", \"react-dom\": \"^15.4.1\", \"webpack\": \"^1.14.0\" } } Run npm install in the Backend directory once you have saved this file. It will download all the modules needed. Visual Studio Extensions There are a number of Visual Studio extensions that can assist with this work. Check out the Appendix for more details on these. You should definitely install the NPM Task Runner extension before continuing though. Both webpack.config.js and package.json should be at the top of the directory tree, in the Backend folder. The .babeljs file should be in the ReactClient directory. You should exclude any files that are not going to be distributed from the publication process. This includes all three build files and the contents of the ReactClient directory. Right-click on each object and select Properties... . Set the Build Action to None and the Copy to Output Directory to Do not copy . Add node_modules to .gitignore You should ensure you add the node\\_modules directory to your .gitignore file or it will be checked into source code. The node\\_modules is normally very large and you can easily regenerate it by running npm install . Building the Webpack files automatically \u00b6 If you have installed the NPM Task Runner , you can build the entire JavaScript application within Visual Studio. Open the Task Runner Explorer ( View -> Other Windows -> Task Runner Explorer ). There will be a package.json section with Defaults and Custom sections within. In the Defaults section, right-click on install and select Run . This will install all the npm packages you are missing. The next step is to do the first build. Right-click on build in the Custom section and select Run . This will generate the vendor.bundle.js and app.js files (plus their map equivalents). Map Files Since there is no direct correlation between what is running in the browser and what your source code looks like when using transpilers and bundlers, these tools produce map files. This is a (generally much larger) file that tells the browser what line represents a piece of code in the browser. They are not distributed (nor required) in a production build. As a one-time activity, we need to add the generated files to the project. In the Solution Explorer , click on the Show All Files button (which is to the left of the spanner icon). Expand Content -> spa -> react and you will see four files which have dotted icons: Right-click on each file and select Include in Project . Ensure their Build Action * is set to Content** in the Properties pane. Assuming all went well, you can now set up the build to happen automatically. Right-click on build in the Custom section, then select Bindings > Before Build . This will generate the files on each build (before the build occurs). If you publish the project as this point, you will be able to browse to /SPA/React and see the following: Integrating Azure Mobile Apps \u00b6 There are three parts to adding Azure Mobile Apps to a Redux: the store, the actions and the reducer. The store is a central part of a redux application and you don't need to do anything special. Here is the ReactClient/redux/store.js code: import { createStore, combineReducers, applyMiddleware } from 'redux'; import createLogger from 'redux-logger'; import thunkMiddleware from 'redux-thunk'; import promiseMiddleware from 'redux-promise'; import * as reducers from './reducers'; import * as todoActions from './actions/todo'; const appReducers = combineReducers({ ...reducers }); const reduxStore = applyMiddleware( thunkMiddleware, promiseMiddleware, createLogger() )(createStore); export const store = reduxStore(appReducers); // Dispatch a refresh action store.dispatch(todoActions.refreshTodoItems()); The only real addition is that we dispatch an action (which initiates a change in the store) when we start up. This action is for refreshing the content from the mobile backend. Our reducer converts the actions into changes in the store. This is the ReactClient/reducers/todo.js : import constants from '../constants'; const initialState = { todoItems: [], network: 0, error: \"\" }; export default function todoReducers(state = initialState, action) { switch (action.type) { case constants.todo.addItem: return Object.assign({}, state, { todoItems: state.todoItems.concat(action.item) }); case constants.todo.removeItem: return Object.assign({}, state, { todoItems: state.todoItems.filter(item => item.id !== action.id) }); case constants.todo.updateItem: return Object.assign({}, state, { todoItems: state.todoItems.map(item => { return (item.id === action.item.id) ? action.item : item; }) }); case constants.todo.replaceItems: return Object.assign({}, state, { todoItems: action.items }); case constants.todo.network: return Object.assign({}, state, { network: state.network + action.counter }); case constants.todo.error: return Object.assign({}, state, { error: action.error }); default: return state; } }; There is no network activity here because all the store changes are being done against a cached copy of the data. You should not have to go to the cloud to fetch the data, except when refreshing the cache. All the actual network activity happens inside the ReactClient/actions/todo.js : import * as WindowsAzure from 'azure-mobile-apps-client'; import constants from '../constants'; const ZUMOAPPURL = location.origin; const zumoClient = new WindowsAzure.MobileServiceClient(ZUMOAPPURL); const todoTable = zumoClient.getTable('todoitem'); function networkProcess(counter) { return { type: constants.todo.network, counter: counter }; } function zumoError(error) { return { type: constants.todo.error, error: error }; } export function addTodoItem(item) { return (dispatch) => { dispatch({ type: constants.todo.addItem, item: item }); const success = (item) => { dispatch({ type: constants.todo.updateItem, item: item }); }; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.insert(item).done(success, failure); dispatch(networkProcess(-1)); }; } export function removeTodoItem(id) { return (dispatch) => { dispatch({ type: constants.todo.removeItem, id: id }); const success = () => {}; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.del({ id: id }).done(success, failure); dispatch(networkProcess(-1)); }; } export function updateTodoItem(item) { return (dispatch) => { dispatch({ type: constants.todo.updateItem, item: item }); const success = (item) => { dispatch({ type: constants.todo.updateItem, item: item }); }; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.update(item).done(success, failure); dispatch(networkProcess(-1)); }; } export function refreshTodoItems() { return (dispatch) => { const success = (data) => { dispatch({ type: constants.todo.replaceItems, items: data }); }; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.read().then(success, failure); dispatch(networkProcess(-1)); } } Each action that is called by the client code kicks off a call into the standard Azure Mobile Apps JavaScript SDK to do the relevant change in the mobile backend. When that returns, another action is dispatched to make the change in the store via the reducer. In addition, we keep track of how many network requests are happening and we also handle errors via the zumoError() action. I have not included all the code in this chapter as much of it is fairly standard React/Redux, but it is available on the GitHub repository for the book. Remember that you will have to update the package.json file and run npm install to install the additional libraries. Updating Webpack for Azure Mobile Apps JavaScript SDK \u00b6 The Azure Mobile Apps JavaScript SDK requires an additional loader for Webpack to handle it. Install the json-loader module using npm install --save json-loader , then add the following to the loaders section of the webpack.config.js : module: { loaders: [ { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ }, { test: /\\.json$/, loader: 'json' } ] }, Once the additional line is added, Webpack will handle the azure-mobile-apps-client without problems. There is a lot more to be said about the capabilities of React, Redux, BabelJS and Webpack that is beyond the scope of this book. I hope you will take the opportunity to try the vibrant JavaScript community out as you build single page applications.","title":"React Applications"},{"location":"chapter6/react/#react-applications","text":"There have been two major shifts in the way that JavaScript applications are built in the past few years. The first is in the user of transpilers for compiling another variant of JavaScript into something that a browser can use. There are languages that add whole new feature sets to the language, like CoffeeScript , TypeScript and Dart , plus others that extend JavaScript to include unsupported features of the JavaScript language early, like BabelJS which provides access to the latest specifications for JavaScript on the current set of browsers. The process of converting one of these languages into more normal JavaScript is called transpiling. The resultant code is rarely readable. The other major shift is in the use of bundlers. In prior iterations of the JavaScript language, there was no module system. Developers included a number of <script> tags to load the code. As JavaScript became more of a development platform, the size of the code grew and the need for a module system also grew. There are three in general use - RequireJS , CommonJS and ES2015 modules . In general, developers will use a bundler like Browserify or Webpack to bundle their application source together so that it loads faster and they can take care of alternate module specifications like CommonJS and ES2015 modules. (RequireJS doesn't require bundling). This brings us to React applications. React is a newcomer to the JavaScript framework scene. It provides a component model for your application. It is generally paired with a Flux state manager like Redux . Thus, most React applications can more properly be called React/Redux applications. While React can be used with just straight JavaScript and script tags, it is more normal to use ES2015 syntax and a bundler like Webpack to distribute the application.","title":"React Applications"},{"location":"chapter6/react/#setting-up","text":"In this example, we are going to walk through the set up of an application that uses Webpack and the BabelJS transpiler to build our web application into our MVC framework.","title":"Setting Up"},{"location":"chapter6/react/#mvc-controller-view","text":"As with the jQuery and Angular examples, we need a controller (within the Controllers\\SPAController.cs file): public ActionResult React() { return View(); } We also need a suitable View. This is similar in construction to the Angular view, but it requires some different elements, as is normal when you change a JavaScript framework. This is Views\\SPA\\React.cshtml : @{ Layout = null; } <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>Azure Mobile Apps Quickstart</title> <link rel=\"stylesheet\" href=\"~/Content/spa/react/app.css\" /> </head> <body> <div id=\"app\"></div> <script src=\"~/Content/spa/react/vendor.bundle.js\"></script> <script src=\"~/Content/spa/react/app.js\"></script> </body> </html> This view is much simpler than the Angular view because none of the markup for the actual application is contained within the view. All markup is done by the React components in our code. There are three files referenced in this file that do not yet exist. The JavaScript files will be created by the Webpack bundler, and the app.css file is the regular CSS file from the normal Quickstart application.","title":"MVC Controller &amp; View"},{"location":"chapter6/react/#setting-up-a-webpack-project","text":"There is some work to do in order to set up a reasonable Webpack-enabled project. Let's first of all set up a basic React application in a new directory called ReactClient . Here is the ReactClient\\app.jsx file that will serve as our entry-point into the application: import React from 'react'; import ReactDOM from 'react-dom'; ReactDOM.render( <h1>In a React Application</h1> , document.getElementById('app') ); Once we have the build correctly done, the application will insert the <h1>In a React Application</h1> inside the <div id=\"app\"></div> from the main page. This is the most basic of React applications, but we can already see some things we may not have seen before. The import statements bring in ES2015 modules for use. Also, you can embed pseudo-HTML into a React application rendering method by using JSX . When the application is transpiled with BabelJS, this pseudo-HTML get compiled into normal JavaScript. There are three other files we want to produce that are build related. The first is the .babelrc file. This contains instructions to the BabelJS transpiler that tells it what specifications need to be transpiled: { \"presets\": [ \"es2015\", \"react\" ], \"plugins\": [ \"transform-class-properties\", \"transform-object-rest-spread\" ] } Our application will be written in ES2015 syntax with React extensions (those are presets) and I write with a couple of specifications that may or may not make it into the next JavaScript specification, but are useful - Class Properties and Object Rest Spread . Although these are considered \"experimental\", they make your life much easier as a developer. The second file is webpack.config.js . This is a JavaScript file that tells the Webpack system how to build the application. Here is a suitable file for this application: 'use strict'; /* global __dirname */ var path = require('path'), webpack = require('webpack'); var configuration = { devtool: 'source-map', entry: { app: [ path.join(__dirname, 'ReactClient/app.jsx') ], vendor: [ 'react', 'react-dom' ] }, module: { loaders: [ { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ } ] }, output: { path: path.join(__dirname, 'Content/spa/react'), publicPath: '/', filename: 'app.js' }, plugins: [ new webpack.optimize.CommonsChunkPlugin('vendor', 'vendor.bundle.js'), new webpack.optimize.DedupePlugin() ], resolve: { modulesDirectories: [ 'node_modules' ], extensions: [ '', '.js', '.jsx' ] }, target: 'web' }; module.exports = configuration; There are three areas of the configuration object that are interesting. The entry block contains the entry-point for the application, but it also contains a vendor array. This array is the list of modules that will be placed into the vendor.bundle.js file. When we add a new external module, we need to add it to the list so that it is included in the bundle. The module section contains instructions on how to transpile the source files. The single entry tells Webpack to run all JavaScript and JSX files through the BabelJS transpiler. Finally, the output section tells Webpack where to store the output files. Our final file is package.json . This is a fairly standard file for loading modules from the NPM repository. You can create one using npm init --yes , then use npm install modulename --save to install the individual modules. Alternatively, you can create the package.json file and then run npm install to install the modules that are named in the file. { \"version\": \"0.0.1\", \"name\": \"react-quickstart\", \"private\": true, \"main\": \"ReactClient/app.jsx\", \"scripts\": { \"build\": \"webpack -p\" }, \"devDependencies\": {}, \"dependencies\": { \"babel-core\": \"^6.21.0\", \"babel-loader\": \"^6.2.10\", \"babel-plugin-transform-class-properties\": \"^6.19.0\", \"babel-plugin-transform-object-rest-spread\": \"^6.20.2\", \"babel-preset-es2015\": \"^6.18.0\", \"babel-preset-react\": \"^6.16.0\", \"react\": \"^15.4.1\", \"react-dom\": \"^15.4.1\", \"webpack\": \"^1.14.0\" } } Run npm install in the Backend directory once you have saved this file. It will download all the modules needed. Visual Studio Extensions There are a number of Visual Studio extensions that can assist with this work. Check out the Appendix for more details on these. You should definitely install the NPM Task Runner extension before continuing though. Both webpack.config.js and package.json should be at the top of the directory tree, in the Backend folder. The .babeljs file should be in the ReactClient directory. You should exclude any files that are not going to be distributed from the publication process. This includes all three build files and the contents of the ReactClient directory. Right-click on each object and select Properties... . Set the Build Action to None and the Copy to Output Directory to Do not copy . Add node_modules to .gitignore You should ensure you add the node\\_modules directory to your .gitignore file or it will be checked into source code. The node\\_modules is normally very large and you can easily regenerate it by running npm install .","title":"Setting up a Webpack project"},{"location":"chapter6/react/#building-the-webpack-files-automatically","text":"If you have installed the NPM Task Runner , you can build the entire JavaScript application within Visual Studio. Open the Task Runner Explorer ( View -> Other Windows -> Task Runner Explorer ). There will be a package.json section with Defaults and Custom sections within. In the Defaults section, right-click on install and select Run . This will install all the npm packages you are missing. The next step is to do the first build. Right-click on build in the Custom section and select Run . This will generate the vendor.bundle.js and app.js files (plus their map equivalents). Map Files Since there is no direct correlation between what is running in the browser and what your source code looks like when using transpilers and bundlers, these tools produce map files. This is a (generally much larger) file that tells the browser what line represents a piece of code in the browser. They are not distributed (nor required) in a production build. As a one-time activity, we need to add the generated files to the project. In the Solution Explorer , click on the Show All Files button (which is to the left of the spanner icon). Expand Content -> spa -> react and you will see four files which have dotted icons: Right-click on each file and select Include in Project . Ensure their Build Action * is set to Content** in the Properties pane. Assuming all went well, you can now set up the build to happen automatically. Right-click on build in the Custom section, then select Bindings > Before Build . This will generate the files on each build (before the build occurs). If you publish the project as this point, you will be able to browse to /SPA/React and see the following:","title":"Building the Webpack files automatically"},{"location":"chapter6/react/#integrating-azure-mobile-apps","text":"There are three parts to adding Azure Mobile Apps to a Redux: the store, the actions and the reducer. The store is a central part of a redux application and you don't need to do anything special. Here is the ReactClient/redux/store.js code: import { createStore, combineReducers, applyMiddleware } from 'redux'; import createLogger from 'redux-logger'; import thunkMiddleware from 'redux-thunk'; import promiseMiddleware from 'redux-promise'; import * as reducers from './reducers'; import * as todoActions from './actions/todo'; const appReducers = combineReducers({ ...reducers }); const reduxStore = applyMiddleware( thunkMiddleware, promiseMiddleware, createLogger() )(createStore); export const store = reduxStore(appReducers); // Dispatch a refresh action store.dispatch(todoActions.refreshTodoItems()); The only real addition is that we dispatch an action (which initiates a change in the store) when we start up. This action is for refreshing the content from the mobile backend. Our reducer converts the actions into changes in the store. This is the ReactClient/reducers/todo.js : import constants from '../constants'; const initialState = { todoItems: [], network: 0, error: \"\" }; export default function todoReducers(state = initialState, action) { switch (action.type) { case constants.todo.addItem: return Object.assign({}, state, { todoItems: state.todoItems.concat(action.item) }); case constants.todo.removeItem: return Object.assign({}, state, { todoItems: state.todoItems.filter(item => item.id !== action.id) }); case constants.todo.updateItem: return Object.assign({}, state, { todoItems: state.todoItems.map(item => { return (item.id === action.item.id) ? action.item : item; }) }); case constants.todo.replaceItems: return Object.assign({}, state, { todoItems: action.items }); case constants.todo.network: return Object.assign({}, state, { network: state.network + action.counter }); case constants.todo.error: return Object.assign({}, state, { error: action.error }); default: return state; } }; There is no network activity here because all the store changes are being done against a cached copy of the data. You should not have to go to the cloud to fetch the data, except when refreshing the cache. All the actual network activity happens inside the ReactClient/actions/todo.js : import * as WindowsAzure from 'azure-mobile-apps-client'; import constants from '../constants'; const ZUMOAPPURL = location.origin; const zumoClient = new WindowsAzure.MobileServiceClient(ZUMOAPPURL); const todoTable = zumoClient.getTable('todoitem'); function networkProcess(counter) { return { type: constants.todo.network, counter: counter }; } function zumoError(error) { return { type: constants.todo.error, error: error }; } export function addTodoItem(item) { return (dispatch) => { dispatch({ type: constants.todo.addItem, item: item }); const success = (item) => { dispatch({ type: constants.todo.updateItem, item: item }); }; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.insert(item).done(success, failure); dispatch(networkProcess(-1)); }; } export function removeTodoItem(id) { return (dispatch) => { dispatch({ type: constants.todo.removeItem, id: id }); const success = () => {}; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.del({ id: id }).done(success, failure); dispatch(networkProcess(-1)); }; } export function updateTodoItem(item) { return (dispatch) => { dispatch({ type: constants.todo.updateItem, item: item }); const success = (item) => { dispatch({ type: constants.todo.updateItem, item: item }); }; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.update(item).done(success, failure); dispatch(networkProcess(-1)); }; } export function refreshTodoItems() { return (dispatch) => { const success = (data) => { dispatch({ type: constants.todo.replaceItems, items: data }); }; const failure = (error) => { dispatch(zumoError(error)); }; dispatch(networkProcess(1)); todoTable.read().then(success, failure); dispatch(networkProcess(-1)); } } Each action that is called by the client code kicks off a call into the standard Azure Mobile Apps JavaScript SDK to do the relevant change in the mobile backend. When that returns, another action is dispatched to make the change in the store via the reducer. In addition, we keep track of how many network requests are happening and we also handle errors via the zumoError() action. I have not included all the code in this chapter as much of it is fairly standard React/Redux, but it is available on the GitHub repository for the book. Remember that you will have to update the package.json file and run npm install to install the additional libraries.","title":"Integrating Azure Mobile Apps"},{"location":"chapter6/react/#updating-webpack-for-azure-mobile-apps-javascript-sdk","text":"The Azure Mobile Apps JavaScript SDK requires an additional loader for Webpack to handle it. Install the json-loader module using npm install --save json-loader , then add the following to the loaders section of the webpack.config.js : module: { loaders: [ { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ }, { test: /\\.json$/, loader: 'json' } ] }, Once the additional line is added, Webpack will handle the azure-mobile-apps-client without problems. There is a lot more to be said about the capabilities of React, Redux, BabelJS and Webpack that is beyond the scope of this book. I hope you will take the opportunity to try the vibrant JavaScript community out as you build single page applications.","title":"Updating Webpack for Azure Mobile Apps JavaScript SDK"},{"location":"chapter7/media/","text":"Azure Media Services \u00b6 One of the common use cases for mobile applications involves video streaming. In the consumer space, this can include applications like Hulu or Netflix along with video reviews and new segments in apps like CNN and CNET. In the enterprise space, we see video learning and employee broadcast solutions. Whatever their source, they have some basic functionality in common: The video asset is uploaded and converted (also known as encoding) to a streaming format. A live video channel can be provided for multiplex streaming to a large audience. The encoded video is provided to clients for download with a suitable web endpoint. Additional services extract information from the video for search capabilities. Many enterprises wrap such functionality in a combined web and mobile site to provide streaming video for eLearning. We are going to look at what it takes to produce the mobile side of such a site in this section. The Video Search Application with Media Services \u00b6 We are going to produce a media services mobile application for this section, based on our last example for Azure Search. In the new example, this is the approximate flow of the application: The administrator will upload an MP4 video via the Visual Studio Storage Explorer. This will be automatically picked up by an Azure Function that encode the video, placing the encoded video into a download area. The next Azure Function will pick up that video and use Cognitive Services on it to extract information that can be searched and insert that information into the Azure Search instance. Finally, a third Azure Function will insert the data about the video into a database so that it can be picked up by Azure Mobile Apps. We are going to use three distinct operations here because encoding and cognitive services are asynchronous - we want to kick them off and let them complete in their own time. On the client side, we will use the Azure Search instance to find apps, display the information held within Azure Mobile Apps, and allow the user to stream the video using the video player. As you can see, there are many more services in use in this example than our previous examples: Azure Media Services is used for video encoding and streaming endpoints. Azure Logic Apps are used for workflow automation. Azure Functions are used for automation. Cognitive Services are used to extract information from the videos. Azure App Service is used to act as a coordinator for the mobile app. Azure Storage is used to store the individual video assets and for some queuing capabilities. SQL Azure is used as the backing store for the Azure Mobile Apps data store. This is now a fairly typical mobile application. We are using 8 different Azure services in a composite manner to provide the facilities needed by our application. Creating the Mobile Encoding flow \u00b6 When I look at the architecture for our mobile backend, I see two distinct parts. The first is the backend flow that processes the incoming videos. As videos are uploaded, they need to be injected into a queue. From there, a series of processes are kicked off to process the incoming video. First, the video is encoded; then data is extracted from the video for search purposes; finally, the video is added to the SQL database so it can be searched. The other flow is from the mobile client - it connects to the App Service and makes requests based on what it needs to do. In this case, we have a set of data tables for providing data about the video and a few custom APIs for handling search and video streaming. Let's take each of these in turn. The configuration of most of the services have already been discussed, so I will not go over them and only provide the options I used. This includes Storage, Search, SQL Azure, and Functions. Creating pre-requisite services \u00b6 Before I start with the new services, I need an Azure Storage account, an [Azure Search] instance and an Azure Function App . I've covered all these items in previous sections, so I won't go into them here. The configuration is as follows: My Azure Storage account called zumomediach7.core.windows.net as General Purpose storage with LRS replication. My Azure Functions app called zumomediach7-functions.azurewebsites.net in the Consumption Plan . I'm using my zumomediach7 storage account. My SQL Azure service is called zumomediach7.database.windows.net . My SQL Azure database is called videosearch in the B Basic pricing plan. My Azure App Service is created via the Mobile App template and called zumomediach7.azurewebsites.net . It has an B1 Basic app service plan associated with it. In addition, I've linked the SQL Azure database and storage accounts to the App Service via the Data Connections menu option. Our resource group looks quite extensive now: Configuration for the services is as follows: Azure Storage has a container for incoming videos called incoming . Azure App Service has a basic TableController which is based on the following DTO model: using Microsoft.Azure.Mobile.Server; namespace Backend.DataObjects { public class Video : EntityData { public string Filename { get; set; } public string VideoUri { get; set; } } } This encompasses information from the majority of the book thus far. If you are uncertain on how to perform any of this configuration, review the appropriate sections of the book: Chapter 1 covers creating a Mobile App. Chapter 4 covers Azure Functions. Azure Search is there One of the things I've added into this project that I don't describe is the integration with Azure Search. I use this to integrate Cognitive Services with the solution so that I can search for videos based on their content (audio, video or metadata). You can use this as a research project. Creating an Azure Media Services account \u00b6 So far, we've done a lot of infrastructure work. We've generated an Azure Mobile App that our mobile app can use to retrieve information about the videos, generated an Azure Search instance with a suitable index, and a storage account for processing the videos. We now want to move onto the meat of this section - working with video. In order to do that, we will need an Azure Media Services account. Creating an Azure Media Services account is very similar to other Azure resources. Log in to the [Azure Portal] and open the resource group you are using to hold all the resources for this application. Click + ADD to add a resource to the resource group. Search for Media Services , select it, then click on Create . Fill in the form: Select a name for your service. It needs to be unique within the service. Select your existing storage account (note the limitations on the replication policy if you use your own). Ensure the region matches your storage account and other resources. Click Create . The Media Services accounts may take a couple of minutes to create. Do not continue until the deployment is complete. Testing tools for Media Services If you intend to do any development in Azure Media Services, you should download and become familiar with the Azure Media Services Explorer . this is a test tool for Windows that allows you to upload, download, process, encode and package assets ith Azure Media Services. You should also grow a collection of test videos. A great starting point are these videos from TechSlides . You could stop here and do all the work manually. If you wish to check out the full set of tutorials, follow the official documentation: Uploading Assets Encoding Assets Publish Assets The Encoding Pipeline \u00b6 There is an excellent sample that uses Azure Functions and Azure Logic Apps as a media workflow. The Azure Functions do the actual processing, using the Azure Media Services SDK to communicate with the Media Services resource, and the Logic App (below) is used to control the workflow and ensure it works properly. To create this flow, first create the Azure Functions required by the flow in the Function App. There are five Functions that are required: check-job-status create-empty-asset publish-asset submit-job sync-asset Start by creating the shared and presets folders. You can do this using the App Service Editor , which is located in the Function app settings . Just create each file and then copy-and-paste the contents into the file. The source code for each function is in the referenced project . Create the Function from the GenericWebhook-CSharp template. Then add the project.json file, which is needed to load the Media Services SDK from NuGet. Once you save the project.json file, let the NuGet restore happen before continuing. You can check the Log window to ensure it is complete. Finally, copy-and-paste the code for the run.csx file. Next, create a Logic App : Close the Function App to return to your Resource Group. Click Add at the top of the blade. Search for and select Logic App . Click Create . Give it a name (like zumobook-logicapp ) and ensure the location is the same as all your other resources, then click Create . After deployment, we can set up the logic app. Click the newly created logic app to open the Logic Apps Designer . The first thing you want to add is a Trigger - something that triggers the execution of the workflow. You can upload a file to OneDrive or Dropbox, for example. In this example, I'm going to use my OneDrive IncomingVideos folder: Find and click OneDrive . You may have to click SEE MORE to find it. Sign in to create a connection to OneDrive. You will also have to authorize Logic Apps to access your information. Click the folder icon to Select a Folder. Click > next to Root, then IncomingVideos . It will be listed as /IncomingVideos . Use the folder picker Some of the triggers will encode the arguments. Use the folder picker rather than typing in the box if your trigger doesn't seem to fire successfully. Now that we have the trigger, we need to continue building the Logic App based on the diagram above. There are a few types of steps - a Function step (where an Azure Function is called via a Webhook): Click + New step , then Add an action . Find and click Azure Functions (you may have to click SEE MORE ). Click Azure Functions - Choose an Azure function . If you only have one Function App, it will be added automatically, otherwise, select the required Function App name. Click Azure Functions - {your Function App} . It will load the list of functions. Click the name of the function you wish to add as the step. The first one is create-empty-asset . Enter the Request Body based on the comment at the top of each Azure Function. For instance, the create-empty-asset should look like this: The Name can be added by clicking Add dynamic content , then finding the appropriate field. After create-empty-asset has been complete, you may want to click Save to save your work. Then continue by clicking + New step , then Add an action . The next step is a Create blob step. You can use the search box to find actions to perform. The Create blob step should look like this when you are finished. When you do configure this step in the Logic Apps Designer, you will note that the containerPath is not available from the dynamic content. When the Webhook returns, it provides a JSON response. The JSON response is documented at the top of the code of each Azure Function. To enter this value, switch to the Code view , find the Create_blob action, look for the queries section, then insert the following value: Once you have entered the value, click Save , then Designer to switch back to the designer view. Continue to add sync-asset as an action. The request body will have to be set within the code view as it relies on the output of create-empty-asset . Set the body section to: \"body\": { \"assetId\": \"@{body('create-empty-asset')['assetId']}\" } Use the template to create everything for you! The sample has a \"Deploy to Azure\" button that allows you to create all the functions and the logic app in one swoop. It's great to understand how Logic Apps are put together, but if you would rather get on with it, just use the shortcut. Linking the submit-job next, set the body in the Code view as follows: \"body\": { \"assetId\": \"@{body('create-empty-asset')['assetId']}\", \"mesPreset\": \"Adaptive Streaming\" } The next step is an \"Until\" step. You are not limited to just a straight step-flow with Logic Apps. You can do loops and conditional execution as well. In this case, the submit-job Azure Function kicks off an encoding job for the incoming video. However, the process to encode that video can take some time. Even a small video can take upwards of 15 minutes to encode because of queuing and process limitations. The Add a do until step is in the \"More\" section after you click + New step . Start by clicking on Add an action within the Until loop. Add the check-job-status Azure Function with a request body: \"body\": { \"jobId\": \"@{body('submit-job')['jobId']}\" } While you are in the code view, set the \"expression\" field for the Until loop to the following: \"expression\": \"@equals(body('check-job-status')['isRunning'], 'False')\", Check the template if you get lost! You can configure everything within the Code view , so if you get lost, just use copy-and-paste to configure each step within the logic app. After the Until loop, we can add a Condition to check if the isSuccessful field returned by the latest invocation of check-job-status was true. Click on Edit in advanced mode and enter the condition @equals(body('check-job-status')['isSuccessful'], 'True') . You now have two sections - a YES and a NO section. My NO section uses \"Outlook.com - Send an email\" to send me an email. I use the File name field in the body to indicate what file was problematic. On the YES side, I am going to add multiple steps. Firstly, I will add an Azure Function for publish-asset with a body: \"body\": { \"assetId\": \"@{body('submit-job')['mes']['assetId']}\" } Technically, this is now a complete encoding pipeline. However, I also want to put the asset into the database so that my client can download it. In the canonical example, the URL of the encoded video is published at @{body('publish-asset')['playerUrl']} . I can pass that into a new Azure Function that inserts it into the database. I can create a new function from directly within the Logic App. However, there are a number of problems with that. Firstly, it creates a Node.js function and I like C#. Secondly, the code editor leaves a lot to be desired. It's a small text box with no Intellisense. Use the Save button to save your Logic App, then close the Logic Apps Designer and switch over to your Azure Function App. Additional Resources Created If you have created your Logic App correctly, you will note additional resources have been created for the connections to the Azure Blob storage, OneDrive and potentially Outlook. These are part of your Logic App and should not be configured separately. Use the GenericWebHook-CSharp template to create a Function called insert-into-database . The code for the Webhook is as follows: /* This function check a job status. Input: { \"fileName\": \"some-name\", \"url\": \"some-url\" } Output: { \"dbId\": \"some-guid\" // The new object reference } */ #r \"Newtonsoft.Json\" #r \"System.Data\" using System; using System.Configuration; using System.Data.SqlClient; using System.Net; using Newtonsoft.Json; public static async Task<object> Run(HttpRequestMessage req, TraceWriter log) { log.Info($\"Webhook was triggered!\"); string jsonContent = await req.Content.ReadAsStringAsync(); dynamic data = JsonConvert.DeserializeObject(jsonContent); if (data.url == null || data.fileName == null) { return req.CreateResponse(HttpStatusCode.BadRequest, new { error = \"Please pass all properties in the input object\" }); } var connectionString = ConfigurationManager.ConnectionStrings[\"MS_TableConnectionString\"].ConnectionString; log.Info($\"Using Connection String {connectionString}\"); var dbId = Guid.NewGuid().ToString(\"N\"); try { using (var sqlConnection = new SqlConnection(connectionString)) { using (var sqlCommand = sqlConnection.CreateCommand()) { log.Info(\"Initiating SQL Connection\"); sqlConnection.Open(); log.Info(\"Executing SQL Statement\"); sqlCommand.CommandText = $\"INSERT INTO [dbo].[Videos] ([Id], [Deleted], [Filename], [VideoUri]) VALUES ('{dbId}', 0, '{data.fileName}', '{data.url}')\"; var rowsAffected = sqlCommand.ExecuteNonQuery(); log.Info($\"{rowsAffected} rows inserted.\"); sqlConnection.Close(); } } } catch (Exception ex) { return req.CreateResponse(HttpStatusCode.BadRequest, new { error = ex.Message }); } return req.CreateResponse(HttpStatusCode.OK, new { greeting = $\"{dbId}\" }); } This inserts a record into the Videos table with the filename and URI specified. I can now add this function to the YES column in my Logic App by specifying the following body in the Code view: \"body\": { \"fileName\": \"@{triggerOutputs()['headers']['x-ms-file-name']}\", \"url\": \"@{body('publish-asset')['pathUrl']}\" } There are three URLs that are returned by the previous step: pathUrl is the path to the assets (but not the filename). smoothUrl is the real-time streaming endpoint. playerUrl is a web-page with an embedded player. You need to use the corresponding URL for your implementation. If you are unsure, then store both the pathUrl and smoothUrl in the database (which will require a modification of the model). The playerUrl can be computed on the client if you need it. Before we can try this pipeline out, the other resources must be specified as Application Settings inside the Function App: AMSAccount is the name of your Media Services resource. AMSKey is the primary key for your Media Services resource. MediaServicesStorageAccountName is the name of your Azure Storage resource. MediaServicesStorageAccountKey is the primary key for your Azure Storage resource. MS_TableConnectionString is the connection string to your video database (from your App Service). Once these are set, you are ready to test your logic app. Go to the Logic Apps Designer and click Run . This allows you to monitor the progress of the workflow live. Then drop a video file in the /IncomingVideos folder of your OneDrive connection and watch the process. It's likely that something will go wrong the first time. In the case of Azure Functions, the error will be displayed: If the error is in a Logic App provided trigger, then consult the Diagnostics and Log search menu items under Monitoring. For the Azure Functions triggers, it is more informative to consult the error logs in the Function App. Open the Monitor tab to check the logs for the latest run. Also, you can create a test run with the appropriate input object and/or place more logging in the Azure Function. I faked this error by removing the AMSAccount application setting. If you have copied the source code directly, it's likely that any errors will be in the app settings. Insert-to-Database Failures The insert-to-database function and the logic app will fail because the database is not created until the first request by a client. You can either pre-create the database or use the client that is developed before you try out the encoding pipeline. The Video Mobile App \u00b6 Now that the backend has been brought online and we can populate it with videos, it's time to turn our attention to the client app. I've started with an app very similar to the Task List. The models are slightly different (since the data set is different), but ultimately, the app provides a list of videos to play. You can find the starting project on GitHub . Use the starting point to create the database In the last section, I mentioned that one of the functions would not work because the database was not created until the first client request. You can use the starting point for the project to create the necessary database. Create all the backend resources, then run the client to create the database, then test out the encoding pipeline. You can integrate any video player that supports a streaming endpoint, and there are several to choose from - each with their own complexities for integration. For simplicity, I am going to integrate the Azure Media Services Player - a web-based streaming media player which I will integrate into a WebView within the page. Let's start by hooking up a new view in the shared project. The new view is called Pages/VideoDetail.xaml : <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"VideoApp.Pages.VideoDetail\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\"> <StackLayout> <WebView x:Name=\"browser\" HorizontalOptions=\"FillAndExpand\" VerticalOptions=\"FillAndExpand\" /> </StackLayout> </ContentPage> This page creates a WebView that occupies the entire page. There is a backing C# source file as well that implements the viewer: using System; using System.Collections.Generic; using System.Linq; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace VideoApp.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class VideoDetail : ContentPage { public VideoDetail (Models.Video video) { InitializeComponent (); var htmlSource = new HtmlWebViewSource(); var sourceInfo = @\" <html> <head> <title>Test</title> <link href=\"\"https://amp.azure.net/libs/amp/1.8.3/skins/amp-default/azuremediaplayer.min.css\"\" rel=\"\"stylesheet\"\"> <script src=\"\"https://amp.azure.net/libs/amp/1.8.3/azuremediaplayer.min.js\"\"></script> </head> <body> <video id=\"\"azuremediaplayer\"\" class=\"\"azuremediaplayer amp-default-skin amp-big-play-centered\"\" tabindex=\"\"0\"\"></video> <script> var myOptions = { \"\"nativeControlsForTouch\"\": false, controls: true, autoplay: true, width: \"\"640\"\", height: \"\"400\"\", }; myPlayer = amp(\"\"azuremediaplayer\"\", myOptions); myPlayer.src([ { src: \"\"{Binding Source}\"\", type: \"\"application/vnd.ms-sstr+xml\"\" } ]); </script> </body> </html> \"; htmlSource.Html = sourceInfo.Replace(\"{Binding Source}\", video.VideoUri); browser.Source = htmlSource; } } } The HTML and Javascript libraries that I use here are provided by Azure Media Services. You can find them as follows: Start Azure Media Services Explorer and connect to your Media Services account. Right-click a published video, then select Playback -> with Azure Media Player . A web-page will open. Click Code -> Get Player Code (under the video). The code will be displayed: I replaced the src object with something I can string-replace later on. The player is completely cross-platform. You do, however, have to specify the height and width. One of the advantages of using a native control is that it will set the height and width for you. Wrap Up \u00b6 Obviously, this isn't the prettiest app that has been produced. However, it is functional and it demonstrates the basic capabilities. We don't have to stop where we did, however. We could use Cognitive Services to extract the audio track and submit to an Azure Search facility. We could then allow searching of the Azure Search facility to come up with a list of videos that match based on the audio track. This is, quite frankly, something that still amazes me and something that we could not do without the Azure Cloud. The Cognitive Services integration is available as the advanced option in the sample for the media processing workflow. We could adjust the insert-into-database Function to extract metadata from the MP4 file. The MP4 file contains a whole host of information. This can be inserted into the database so you can display it. You could also add a JSON file in the upload to provide additional content that is inserted into the database. We could provide ratings and other controls on the list. This can be stored on the mobile backend as well to provide information to other users. Video media is one of those areas of development that is complex to understand and implement, but has so much potential in the mobile space.","title":"Media Services"},{"location":"chapter7/media/#azure-media-services","text":"One of the common use cases for mobile applications involves video streaming. In the consumer space, this can include applications like Hulu or Netflix along with video reviews and new segments in apps like CNN and CNET. In the enterprise space, we see video learning and employee broadcast solutions. Whatever their source, they have some basic functionality in common: The video asset is uploaded and converted (also known as encoding) to a streaming format. A live video channel can be provided for multiplex streaming to a large audience. The encoded video is provided to clients for download with a suitable web endpoint. Additional services extract information from the video for search capabilities. Many enterprises wrap such functionality in a combined web and mobile site to provide streaming video for eLearning. We are going to look at what it takes to produce the mobile side of such a site in this section.","title":"Azure Media Services"},{"location":"chapter7/media/#the-video-search-application-with-media-services","text":"We are going to produce a media services mobile application for this section, based on our last example for Azure Search. In the new example, this is the approximate flow of the application: The administrator will upload an MP4 video via the Visual Studio Storage Explorer. This will be automatically picked up by an Azure Function that encode the video, placing the encoded video into a download area. The next Azure Function will pick up that video and use Cognitive Services on it to extract information that can be searched and insert that information into the Azure Search instance. Finally, a third Azure Function will insert the data about the video into a database so that it can be picked up by Azure Mobile Apps. We are going to use three distinct operations here because encoding and cognitive services are asynchronous - we want to kick them off and let them complete in their own time. On the client side, we will use the Azure Search instance to find apps, display the information held within Azure Mobile Apps, and allow the user to stream the video using the video player. As you can see, there are many more services in use in this example than our previous examples: Azure Media Services is used for video encoding and streaming endpoints. Azure Logic Apps are used for workflow automation. Azure Functions are used for automation. Cognitive Services are used to extract information from the videos. Azure App Service is used to act as a coordinator for the mobile app. Azure Storage is used to store the individual video assets and for some queuing capabilities. SQL Azure is used as the backing store for the Azure Mobile Apps data store. This is now a fairly typical mobile application. We are using 8 different Azure services in a composite manner to provide the facilities needed by our application.","title":"The Video Search Application with Media Services"},{"location":"chapter7/media/#creating-the-mobile-encoding-flow","text":"When I look at the architecture for our mobile backend, I see two distinct parts. The first is the backend flow that processes the incoming videos. As videos are uploaded, they need to be injected into a queue. From there, a series of processes are kicked off to process the incoming video. First, the video is encoded; then data is extracted from the video for search purposes; finally, the video is added to the SQL database so it can be searched. The other flow is from the mobile client - it connects to the App Service and makes requests based on what it needs to do. In this case, we have a set of data tables for providing data about the video and a few custom APIs for handling search and video streaming. Let's take each of these in turn. The configuration of most of the services have already been discussed, so I will not go over them and only provide the options I used. This includes Storage, Search, SQL Azure, and Functions.","title":"Creating the Mobile Encoding flow"},{"location":"chapter7/media/#creating-pre-requisite-services","text":"Before I start with the new services, I need an Azure Storage account, an [Azure Search] instance and an Azure Function App . I've covered all these items in previous sections, so I won't go into them here. The configuration is as follows: My Azure Storage account called zumomediach7.core.windows.net as General Purpose storage with LRS replication. My Azure Functions app called zumomediach7-functions.azurewebsites.net in the Consumption Plan . I'm using my zumomediach7 storage account. My SQL Azure service is called zumomediach7.database.windows.net . My SQL Azure database is called videosearch in the B Basic pricing plan. My Azure App Service is created via the Mobile App template and called zumomediach7.azurewebsites.net . It has an B1 Basic app service plan associated with it. In addition, I've linked the SQL Azure database and storage accounts to the App Service via the Data Connections menu option. Our resource group looks quite extensive now: Configuration for the services is as follows: Azure Storage has a container for incoming videos called incoming . Azure App Service has a basic TableController which is based on the following DTO model: using Microsoft.Azure.Mobile.Server; namespace Backend.DataObjects { public class Video : EntityData { public string Filename { get; set; } public string VideoUri { get; set; } } } This encompasses information from the majority of the book thus far. If you are uncertain on how to perform any of this configuration, review the appropriate sections of the book: Chapter 1 covers creating a Mobile App. Chapter 4 covers Azure Functions. Azure Search is there One of the things I've added into this project that I don't describe is the integration with Azure Search. I use this to integrate Cognitive Services with the solution so that I can search for videos based on their content (audio, video or metadata). You can use this as a research project.","title":"Creating pre-requisite services"},{"location":"chapter7/media/#creating-an-azure-media-services-account","text":"So far, we've done a lot of infrastructure work. We've generated an Azure Mobile App that our mobile app can use to retrieve information about the videos, generated an Azure Search instance with a suitable index, and a storage account for processing the videos. We now want to move onto the meat of this section - working with video. In order to do that, we will need an Azure Media Services account. Creating an Azure Media Services account is very similar to other Azure resources. Log in to the [Azure Portal] and open the resource group you are using to hold all the resources for this application. Click + ADD to add a resource to the resource group. Search for Media Services , select it, then click on Create . Fill in the form: Select a name for your service. It needs to be unique within the service. Select your existing storage account (note the limitations on the replication policy if you use your own). Ensure the region matches your storage account and other resources. Click Create . The Media Services accounts may take a couple of minutes to create. Do not continue until the deployment is complete. Testing tools for Media Services If you intend to do any development in Azure Media Services, you should download and become familiar with the Azure Media Services Explorer . this is a test tool for Windows that allows you to upload, download, process, encode and package assets ith Azure Media Services. You should also grow a collection of test videos. A great starting point are these videos from TechSlides . You could stop here and do all the work manually. If you wish to check out the full set of tutorials, follow the official documentation: Uploading Assets Encoding Assets Publish Assets","title":"Creating an Azure Media Services account"},{"location":"chapter7/media/#the-encoding-pipeline","text":"There is an excellent sample that uses Azure Functions and Azure Logic Apps as a media workflow. The Azure Functions do the actual processing, using the Azure Media Services SDK to communicate with the Media Services resource, and the Logic App (below) is used to control the workflow and ensure it works properly. To create this flow, first create the Azure Functions required by the flow in the Function App. There are five Functions that are required: check-job-status create-empty-asset publish-asset submit-job sync-asset Start by creating the shared and presets folders. You can do this using the App Service Editor , which is located in the Function app settings . Just create each file and then copy-and-paste the contents into the file. The source code for each function is in the referenced project . Create the Function from the GenericWebhook-CSharp template. Then add the project.json file, which is needed to load the Media Services SDK from NuGet. Once you save the project.json file, let the NuGet restore happen before continuing. You can check the Log window to ensure it is complete. Finally, copy-and-paste the code for the run.csx file. Next, create a Logic App : Close the Function App to return to your Resource Group. Click Add at the top of the blade. Search for and select Logic App . Click Create . Give it a name (like zumobook-logicapp ) and ensure the location is the same as all your other resources, then click Create . After deployment, we can set up the logic app. Click the newly created logic app to open the Logic Apps Designer . The first thing you want to add is a Trigger - something that triggers the execution of the workflow. You can upload a file to OneDrive or Dropbox, for example. In this example, I'm going to use my OneDrive IncomingVideos folder: Find and click OneDrive . You may have to click SEE MORE to find it. Sign in to create a connection to OneDrive. You will also have to authorize Logic Apps to access your information. Click the folder icon to Select a Folder. Click > next to Root, then IncomingVideos . It will be listed as /IncomingVideos . Use the folder picker Some of the triggers will encode the arguments. Use the folder picker rather than typing in the box if your trigger doesn't seem to fire successfully. Now that we have the trigger, we need to continue building the Logic App based on the diagram above. There are a few types of steps - a Function step (where an Azure Function is called via a Webhook): Click + New step , then Add an action . Find and click Azure Functions (you may have to click SEE MORE ). Click Azure Functions - Choose an Azure function . If you only have one Function App, it will be added automatically, otherwise, select the required Function App name. Click Azure Functions - {your Function App} . It will load the list of functions. Click the name of the function you wish to add as the step. The first one is create-empty-asset . Enter the Request Body based on the comment at the top of each Azure Function. For instance, the create-empty-asset should look like this: The Name can be added by clicking Add dynamic content , then finding the appropriate field. After create-empty-asset has been complete, you may want to click Save to save your work. Then continue by clicking + New step , then Add an action . The next step is a Create blob step. You can use the search box to find actions to perform. The Create blob step should look like this when you are finished. When you do configure this step in the Logic Apps Designer, you will note that the containerPath is not available from the dynamic content. When the Webhook returns, it provides a JSON response. The JSON response is documented at the top of the code of each Azure Function. To enter this value, switch to the Code view , find the Create_blob action, look for the queries section, then insert the following value: Once you have entered the value, click Save , then Designer to switch back to the designer view. Continue to add sync-asset as an action. The request body will have to be set within the code view as it relies on the output of create-empty-asset . Set the body section to: \"body\": { \"assetId\": \"@{body('create-empty-asset')['assetId']}\" } Use the template to create everything for you! The sample has a \"Deploy to Azure\" button that allows you to create all the functions and the logic app in one swoop. It's great to understand how Logic Apps are put together, but if you would rather get on with it, just use the shortcut. Linking the submit-job next, set the body in the Code view as follows: \"body\": { \"assetId\": \"@{body('create-empty-asset')['assetId']}\", \"mesPreset\": \"Adaptive Streaming\" } The next step is an \"Until\" step. You are not limited to just a straight step-flow with Logic Apps. You can do loops and conditional execution as well. In this case, the submit-job Azure Function kicks off an encoding job for the incoming video. However, the process to encode that video can take some time. Even a small video can take upwards of 15 minutes to encode because of queuing and process limitations. The Add a do until step is in the \"More\" section after you click + New step . Start by clicking on Add an action within the Until loop. Add the check-job-status Azure Function with a request body: \"body\": { \"jobId\": \"@{body('submit-job')['jobId']}\" } While you are in the code view, set the \"expression\" field for the Until loop to the following: \"expression\": \"@equals(body('check-job-status')['isRunning'], 'False')\", Check the template if you get lost! You can configure everything within the Code view , so if you get lost, just use copy-and-paste to configure each step within the logic app. After the Until loop, we can add a Condition to check if the isSuccessful field returned by the latest invocation of check-job-status was true. Click on Edit in advanced mode and enter the condition @equals(body('check-job-status')['isSuccessful'], 'True') . You now have two sections - a YES and a NO section. My NO section uses \"Outlook.com - Send an email\" to send me an email. I use the File name field in the body to indicate what file was problematic. On the YES side, I am going to add multiple steps. Firstly, I will add an Azure Function for publish-asset with a body: \"body\": { \"assetId\": \"@{body('submit-job')['mes']['assetId']}\" } Technically, this is now a complete encoding pipeline. However, I also want to put the asset into the database so that my client can download it. In the canonical example, the URL of the encoded video is published at @{body('publish-asset')['playerUrl']} . I can pass that into a new Azure Function that inserts it into the database. I can create a new function from directly within the Logic App. However, there are a number of problems with that. Firstly, it creates a Node.js function and I like C#. Secondly, the code editor leaves a lot to be desired. It's a small text box with no Intellisense. Use the Save button to save your Logic App, then close the Logic Apps Designer and switch over to your Azure Function App. Additional Resources Created If you have created your Logic App correctly, you will note additional resources have been created for the connections to the Azure Blob storage, OneDrive and potentially Outlook. These are part of your Logic App and should not be configured separately. Use the GenericWebHook-CSharp template to create a Function called insert-into-database . The code for the Webhook is as follows: /* This function check a job status. Input: { \"fileName\": \"some-name\", \"url\": \"some-url\" } Output: { \"dbId\": \"some-guid\" // The new object reference } */ #r \"Newtonsoft.Json\" #r \"System.Data\" using System; using System.Configuration; using System.Data.SqlClient; using System.Net; using Newtonsoft.Json; public static async Task<object> Run(HttpRequestMessage req, TraceWriter log) { log.Info($\"Webhook was triggered!\"); string jsonContent = await req.Content.ReadAsStringAsync(); dynamic data = JsonConvert.DeserializeObject(jsonContent); if (data.url == null || data.fileName == null) { return req.CreateResponse(HttpStatusCode.BadRequest, new { error = \"Please pass all properties in the input object\" }); } var connectionString = ConfigurationManager.ConnectionStrings[\"MS_TableConnectionString\"].ConnectionString; log.Info($\"Using Connection String {connectionString}\"); var dbId = Guid.NewGuid().ToString(\"N\"); try { using (var sqlConnection = new SqlConnection(connectionString)) { using (var sqlCommand = sqlConnection.CreateCommand()) { log.Info(\"Initiating SQL Connection\"); sqlConnection.Open(); log.Info(\"Executing SQL Statement\"); sqlCommand.CommandText = $\"INSERT INTO [dbo].[Videos] ([Id], [Deleted], [Filename], [VideoUri]) VALUES ('{dbId}', 0, '{data.fileName}', '{data.url}')\"; var rowsAffected = sqlCommand.ExecuteNonQuery(); log.Info($\"{rowsAffected} rows inserted.\"); sqlConnection.Close(); } } } catch (Exception ex) { return req.CreateResponse(HttpStatusCode.BadRequest, new { error = ex.Message }); } return req.CreateResponse(HttpStatusCode.OK, new { greeting = $\"{dbId}\" }); } This inserts a record into the Videos table with the filename and URI specified. I can now add this function to the YES column in my Logic App by specifying the following body in the Code view: \"body\": { \"fileName\": \"@{triggerOutputs()['headers']['x-ms-file-name']}\", \"url\": \"@{body('publish-asset')['pathUrl']}\" } There are three URLs that are returned by the previous step: pathUrl is the path to the assets (but not the filename). smoothUrl is the real-time streaming endpoint. playerUrl is a web-page with an embedded player. You need to use the corresponding URL for your implementation. If you are unsure, then store both the pathUrl and smoothUrl in the database (which will require a modification of the model). The playerUrl can be computed on the client if you need it. Before we can try this pipeline out, the other resources must be specified as Application Settings inside the Function App: AMSAccount is the name of your Media Services resource. AMSKey is the primary key for your Media Services resource. MediaServicesStorageAccountName is the name of your Azure Storage resource. MediaServicesStorageAccountKey is the primary key for your Azure Storage resource. MS_TableConnectionString is the connection string to your video database (from your App Service). Once these are set, you are ready to test your logic app. Go to the Logic Apps Designer and click Run . This allows you to monitor the progress of the workflow live. Then drop a video file in the /IncomingVideos folder of your OneDrive connection and watch the process. It's likely that something will go wrong the first time. In the case of Azure Functions, the error will be displayed: If the error is in a Logic App provided trigger, then consult the Diagnostics and Log search menu items under Monitoring. For the Azure Functions triggers, it is more informative to consult the error logs in the Function App. Open the Monitor tab to check the logs for the latest run. Also, you can create a test run with the appropriate input object and/or place more logging in the Azure Function. I faked this error by removing the AMSAccount application setting. If you have copied the source code directly, it's likely that any errors will be in the app settings. Insert-to-Database Failures The insert-to-database function and the logic app will fail because the database is not created until the first request by a client. You can either pre-create the database or use the client that is developed before you try out the encoding pipeline.","title":"The Encoding Pipeline"},{"location":"chapter7/media/#the-video-mobile-app","text":"Now that the backend has been brought online and we can populate it with videos, it's time to turn our attention to the client app. I've started with an app very similar to the Task List. The models are slightly different (since the data set is different), but ultimately, the app provides a list of videos to play. You can find the starting project on GitHub . Use the starting point to create the database In the last section, I mentioned that one of the functions would not work because the database was not created until the first client request. You can use the starting point for the project to create the necessary database. Create all the backend resources, then run the client to create the database, then test out the encoding pipeline. You can integrate any video player that supports a streaming endpoint, and there are several to choose from - each with their own complexities for integration. For simplicity, I am going to integrate the Azure Media Services Player - a web-based streaming media player which I will integrate into a WebView within the page. Let's start by hooking up a new view in the shared project. The new view is called Pages/VideoDetail.xaml : <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"VideoApp.Pages.VideoDetail\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\"> <StackLayout> <WebView x:Name=\"browser\" HorizontalOptions=\"FillAndExpand\" VerticalOptions=\"FillAndExpand\" /> </StackLayout> </ContentPage> This page creates a WebView that occupies the entire page. There is a backing C# source file as well that implements the viewer: using System; using System.Collections.Generic; using System.Linq; using Xamarin.Forms; using Xamarin.Forms.Xaml; namespace VideoApp.Pages { [XamlCompilation(XamlCompilationOptions.Compile)] public partial class VideoDetail : ContentPage { public VideoDetail (Models.Video video) { InitializeComponent (); var htmlSource = new HtmlWebViewSource(); var sourceInfo = @\" <html> <head> <title>Test</title> <link href=\"\"https://amp.azure.net/libs/amp/1.8.3/skins/amp-default/azuremediaplayer.min.css\"\" rel=\"\"stylesheet\"\"> <script src=\"\"https://amp.azure.net/libs/amp/1.8.3/azuremediaplayer.min.js\"\"></script> </head> <body> <video id=\"\"azuremediaplayer\"\" class=\"\"azuremediaplayer amp-default-skin amp-big-play-centered\"\" tabindex=\"\"0\"\"></video> <script> var myOptions = { \"\"nativeControlsForTouch\"\": false, controls: true, autoplay: true, width: \"\"640\"\", height: \"\"400\"\", }; myPlayer = amp(\"\"azuremediaplayer\"\", myOptions); myPlayer.src([ { src: \"\"{Binding Source}\"\", type: \"\"application/vnd.ms-sstr+xml\"\" } ]); </script> </body> </html> \"; htmlSource.Html = sourceInfo.Replace(\"{Binding Source}\", video.VideoUri); browser.Source = htmlSource; } } } The HTML and Javascript libraries that I use here are provided by Azure Media Services. You can find them as follows: Start Azure Media Services Explorer and connect to your Media Services account. Right-click a published video, then select Playback -> with Azure Media Player . A web-page will open. Click Code -> Get Player Code (under the video). The code will be displayed: I replaced the src object with something I can string-replace later on. The player is completely cross-platform. You do, however, have to specify the height and width. One of the advantages of using a native control is that it will set the height and width for you.","title":"The Video Mobile App"},{"location":"chapter7/media/#wrap-up","text":"Obviously, this isn't the prettiest app that has been produced. However, it is functional and it demonstrates the basic capabilities. We don't have to stop where we did, however. We could use Cognitive Services to extract the audio track and submit to an Azure Search facility. We could then allow searching of the Azure Search facility to come up with a list of videos that match based on the audio track. This is, quite frankly, something that still amazes me and something that we could not do without the Azure Cloud. The Cognitive Services integration is available as the advanced option in the sample for the media processing workflow. We could adjust the insert-into-database Function to extract metadata from the MP4 file. The MP4 file contains a whole host of information. This can be inserted into the database so you can display it. You could also add a JSON file in the upload to provide additional content that is inserted into the database. We could provide ratings and other controls on the list. This can be stored on the mobile backend as well to provide information to other users. Video media is one of those areas of development that is complex to understand and implement, but has so much potential in the mobile space.","title":"Wrap Up"},{"location":"chapter7/search/","text":"Integrating Mobile Search \u00b6 Simple search capabilities can be handled by a little light LINQ usage and a small search bar. However, most catalog apps or video apps have search that seems almost magical in what it produces. It can handle multiple languages, bad spelling, the difference between singular and plural spellings. Once it has a set of matches, the order of the results is based on relevance to your search query and the app can highlight and provide additional search queries. All of this is provided by external search services. The most commonly used services are based on Lucene which is an open-source text search engine library from the Apache Project. Azure Search is no exception here. It provides a nice REST interface that can be used by your app to provide search results. In this chapters example, we are going to build something new and it doesn't use Azure Mobile Apps much at all. We are going to build a video search engine. We will have a number of videos that we will upload. Those videos will be processed by the backend and the audio and video content will be analyzed for searchable content. Our app will be able to search that content and get some matches back. To start, I've created a simple Xamarin Forms app with a single view (called Search). We'll update this app later on as we develop the code. For now, the code for the app is on the GitHub repository . Configuring Azure Search \u00b6 My project does not depend on Azure Mobile Apps this time (yet). Create a new Resource Group, then click on the + ADD button at the top of your resource group and add a new Azure Search resource. You will have to give it a name which becomes a part of the URL. All Azure Search resources are accessed through a https:// name .search.windows.net URL. Tip Since your users will not be typing the name in, this is a great time to use a GUID as the name - it fits the naming convention and is guaranteed to be unique. The only other decision of note is the Pricing Tier . You will be given the Standard tier, which covers 15 million documents per partition and up to 12 partitions, with up to 36 search units for scaling. It's an awesome production service. We are not exactly at that level of need yet. Fortunately, there is a free tier that covers a single scale unit, 50MB of storage and 10,000 documents - plenty for testing the service out. Click on the F Free option, then Select , followed by Create to create the resource. Creation of search resources is very quick - usually less than 15 seconds. Creating a Search Index \u00b6 Just like Azure Mobile Apps, there is no data in the service yet, so it's fairly useless. We need to create an index that can be searched. For right now, I've got a collection of videos. These documents are JSON objects that include the following fields: Id Title Image Rating Release Year Genre In Azure Search, the model for the objects going into the store need to have a type and you need to decide on some attributes. Exactly one field must be a \"key\" (we'll use the Id for this), and fields need to be marked Retrievable, Filterable, Sortable, Facetable and/or Searchable. Retrievable - the app can retrieve the field Sortable - it can be used to sort search results Filterable - it can be used in filter queries Searchable - it is a full-text search field The only one I've left out here is \"Facet-able\". This allows a field to be used in faceted navigation. This is a drill-down mechanism. If you have been on a web store like Amazon, you will have seen this feature. It's generally depicted as a \"Refine by\" field. For example, you may search for cars, but then want to limit the search to only convertibles, then only by red cars. Each of these refinements is a facet. If I added a \"genre\" to my fields, I could use faceted navigation. Back to my model, here it is: Field Name Type Attributes videoId Edm.String Key, Retrievable title Edm.String Retrievable, Sortable, Filterable, Searchable image Edm.String Retrievable rating Edm.Double Retrievable, Sortable, Filterable releaseYear Edm.Int32 Retrievable, Sortable, Filterable genre Collection(Edm.String) Retrievable, Filterable, Searchable, Facetable The types are based on OData v4 types . There are a large number of primitive types. Unfortunately, Azure Search only supports a subset of these . Warn You can't sort by the genre field because it is a collection. Collections cannot be marked sortable. You can create an index via the Azure Portal or with the REST interface. I'm going to define my \"videos\" index with JSON. Here is the JSON file: { \"name\": \"videos\", \"fields\": [ { \"name\": \"videoId\", \"type\": \"Edm.String\", \"key\": true, \"filterable\": false, \"searchable\": false, \"sortable\": false, \"facetable\": false }, { \"name\": \"title\", \"type\": \"Edm.String\", \"filterable\": true, \"sortable\": true, \"facetable\": false }, { \"name\": \"image\", \"type\": \"Edm.String\", \"filterable\": false, \"searchable\": false, \"sortable\": false, \"facetable\": false }, { \"name\": \"rating\", \"type\": \"Edm.Double\", \"filterable\": true, \"searchable\": false, \"sortable\": true, \"facetable\": false }, { \"name\": \"releaseYear\", \"type\": \"Edm.Int32\", \"filterable\": true, \"searchable\": false, \"sortable\": true, \"facetable\": false }, { \"name\": \"genre\", \"type\": \"Collection(Edm.String)\", \"filterable\": true, \"sortable\": false, \"facetable\": true } ] } Tip You can skip default values. Searchable and Retrievable default to true, so you only have to specify them if you want to turn that off. Similarly, key defaults to false, so you only have to specify the key on the field that needs it. To install this index, you will need the URI of the search service (which you already have) and the API Key. In the Azure Portal , open your Azure Search resource and click on Keys . You will see the primary and secondary ADMIN KEY . You can use either one. Open up Postman and issue a POST to URI/indexes?api-version=2016-09-01 . Set the Content-Type to application/json and add an api-key header set to the admin key. The content of the POST should be your JSON object. Info You can also install an index through the Azure Portal as a one time activity. I like using REST because it allows me to treat \"configuration as code\" and check my index definition into my source repository. This also opens up automated deployment options via PowerShell, for example. Click on the SEND button and you will see the return status of 201 Created and an OData v4 document in the response body giving the full definition of the index. You will receive a 400 series response with an error message if something goes wrong. It's likely to be either a duplicate index or a malformed JSON object. Free Limits You can create 1 free Azure Search instance and that supports 3 indices (or collections of documents). Populating a Search Index (the easy way) \u00b6 There are many ways of populating a search index. You can do what we are going to do - push data into an index. You can, however, also define an indexer that will crawl a data source periodically. Indexers are provided for Azure Blob and Table Storage, DocumentDB and SQL Server instances (through Azure SQL or SQL Server on a VM). I'm going to use a file from the Internet to populate the search index. I had to adjust it as the format for uploading is specific: { \"value\": [ { \"title\": \"Dawn of the Planet of the Apes\", \"image\": \"http://api.androidhive.info/json/movies/1.jpg\", \"rating\": 8.3, \"releaseYear\": 2014, \"genre\": [ \"Action\", \"Drama\", \"Sci-Fi\" ], \"@search.action\": \"upload\", \"videoId\": \"98ebe557-894c-48de-b61c-718f78b2adbb\" }, { \"title\": \"District 9\", \"image\": \"http://api.androidhive.info/json/movies/2.jpg\", \"rating\": 8, \"releaseYear\": 2009, \"genre\": [ \"Action\", \"Sci-Fi\", \"Thriller\" ], \"@search.action\": \"upload\", \"videoId\": \"e9d89038-386b-4aaa-a36c-530e2f3587c9\" }, ... ] } I can upload this document just like the creation of the index. The only difference is that I am POSTing to /indexes/videos/docs/index: If you see a 200 OK , then all the documents were uploaded and accepted. If you see a 207 message, then some of the documents were not uploaded. In that case, look at the response - each document will be referenced by the key field (in our case, the videoId field), the status field will be false and there will be an errorMessage field which contains the problem. You can correct the problem and upload just that document. There are four values for the @search.action field - normally, we will want to use \"upload\" for new documents and \"mergeOrUpload\" for subsequent updates. Testing Azure Search \u00b6 Now that we have a few documents indexed, we can test the search facility. Go to the Overview page of your search service. The indices are listed on the overview page and you can click on the videos index. You can then click on the Search Explorer to get into the test facility. Let's start with a fairly basic search for the word \"of\" across all searchable fields: We can also do boolean searches. For example, let's do the same search, but finding only comedies: Azure Search can accept a simple search string (as we have done here), an OData Filter using a restricted set of search criteria, or Lucene Search Syntax . The search explorer allows you to explore the various search mechanisms and their (sometimes peculiar) syntax. Other Service Considerations \u00b6 You will note the use of an API key for Azure Search. This can (and should) be regenerated at a regular interval. As a result, you will want a custom API that retrieves the current API key, perhaps only giving the API key to authenticated users. We covered custom APIs in an earlier chapter, so I won't cover that functionality here. Instead, the demonstration code will use a Settings.cs class in the client that contains the URI and API key for searching. Using Azure Search \u00b6 Before you can use Azure Search, you should generate a Query-Only API key for your Azure Search service. When we uploaded the documents to the search service for indexing (and if you intend to do any other administrative tasks through PowerShell or the REST API), you will use the Administrative API key. This key is found under the Keys menu item in the Azure Search resource in the Azure Portal. In the same place is a menu item called Manage query keys . The service creates one of these keys for you with an empty name. I like to create a query key for each version of the mobile software I release. I can thus retire keys that are no longer in use. To create a key: Click the + Add button. Enter a descriptive name (like \"iOS v1.0\", for example) Click Create . You can now copy and paste the key into your settings file. I have created a Settings.cs file in my shared project: using System; namespace VideoSearch { public static class Settings { public static string AzureSearchUri = \"https://zumbook.search.windows.net\"; /// <summary> /// Replace this with your API key from the Azure Search. You should /// never check in code with an API key in it - read the key from an /// Azure App Service App Setting and then provide it to your mobile /// clients via a custom API. /// </summary> public static string AzureSearchApiKey = \"88E95AB69AAAAB6FC5579E1CC40E7FC4\"; } } As we saw while we were testing the service, the search API is going to return a number of JSON objects. We can represent each return value with a model. Here is my Models/Movie.cs model: using System; using System.Collections.Generic; using Newtonsoft.Json; namespace VideoSearch.Models { public class Movie : SearchResult { [JsonProperty(PropertyName = \"videoId\")] public string Id { get; set; } public string Title { get; set; } public Uri Image { get; set; } public double Rating { get; set; } public int ReleaseYear { get; set; } [JsonProperty(PropertyName = \"genre\")] public List<string> Genres { get; set; } } } The Models/SearchResult.cs model adds the @search.score value that is returned in the search results: using Newtonsoft.Json; namespace VideoSearch.Models { public class SearchResult { [JsonProperty(PropertyName = \"@search.score\")] public double SearchScore { get; set; } } } Finally, the Models/MovieResults.cs class can be used to deserialize the entire JSON object that is returned by the server: using System.Collections.Generic; using Newtonsoft.Json; namespace VideoSearch.Models { public class MovieResults { [JsonProperty(PropertyName = \"@odata.context\")] public string Context { get; set; } [JsonProperty(PropertyName = \"value\")] public List<Movie> Movies { get; set; } } } I also provide a class called Services/SearchService.cs for handling search results. In this case, it will do the HTTP request to the specified server, sending the provided search string, and decode the response. It will throw an exception if the server produces an error: using System; using System.Collections.Generic; using System.Net.Http; using System.Net.Http.Headers; using System.Threading.Tasks; using Newtonsoft.Json; using VideoSearch.Models; namespace VideoSearch.Services { public class SearchService { private HttpClient _client; private string _apiVersion = \"2016-09-01\"; public SearchService() { this._client = new HttpClient(); } public async Task<List<Movie>> SearchMoviesAsync(string searchTerms) { var content = await SearchAsync(\"videos\", searchTerms); var movieResults = JsonConvert.DeserializeObject<MovieResults>(content); return movieResults.Movies; } private async Task<string> SearchAsync(string index, string searchTerms) { var uri = new UriBuilder($\"{Settings.AzureSearchUri}/indexes/{index}/docs\"); uri.Query = $\"api-version={_apiVersion}&search={Uri.EscapeDataString(searchTerms)}\"; var request = new HttpRequestMessage { RequestUri = uri.Uri, Method = HttpMethod.Get }; request.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\")); request.Headers.Add(\"api-key\", Settings.AzureSearchApiKey); var response = await _client.SendAsync(request); return await response.Content.ReadAsStringAsync(); } } } The SearchAsync() method is a basic HTTP GET method that returns a string. We add the appropriate headers and ensure the URI is the correct format. This service class can now be used to search for movies when we type something into the search box and initiate a search. This is done in the ViewModels\\Search.cs class in the shared project: public Command SearchCommand => _cmdSearch ?? (_cmdSearch = new Command(async () => await ExecuteSearchCommand())); private async Task ExecuteSearchCommand() { if (IsBusy) return; IsBusy = true; try { var results = await _service.SearchMoviesAsync(SearchString); SearchResults.ReplaceRange(results); } catch (Exception ex) { SearchResults.Clear(); await Application.Current.MainPage.DisplayAlert(\"Search Failed\", ex.Message, \"OK\"); } finally { IsBusy = false; } } The SearchString is a bindable string property that is bound to the Text field of the search bar. The SearchResults property is an ObservableRangeCollection that is bound to a list of items. This code creates a reference to our search service, then uses it to populate the search results PropertyName with the list of movies when the search is complete. We also needed a little bit of error handling for the edge case when the user types something in that isn't understood by the search service. This is likely to be rare, but we want to handle failures gracefully when they do happen. The associated view must also be adjusted so that the movies are displayed. This is done within a ListView object in a similar way to the way we did our task list examples: <ListView CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"False\" ItemsSource=\"{Binding SearchResults}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.BackgroundColor> <OnPlatform x:TypeArguments=\"Color\" Android=\"#2E2F30\" WinPhone=\"#F0F0F0\" iOS=\"#F0F0F0\" /> </ListView.BackgroundColor> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout Padding=\"10\" HorizontalOptions=\"FillAndExpand\" Orientation=\"Horizontal\" VerticalOptions=\"CenterAndExpand\"> <StackLayout.BackgroundColor> <OnPlatform x:TypeArguments=\"Color\" Android=\"Black\" WinPhone=\"White\" iOS=\"White\" /> </StackLayout.BackgroundColor> <Label HorizontalOptions=\"FillAndExpand\" Text=\"{Binding Title}\" TextColor=\"#272832\"> <Label.TextColor> <OnPlatform x:TypeArguments=\"Color\" Android=\"#F3F3F3\" WinPhone=\"#272832\" iOS=\"#272832\" /> </Label.TextColor> </Label> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> We can now run this application and start using the search service! In the example code , I've also included a Details panel. This should show you all the information within a movie title from the search results by clicking on the movie title. It's similar in construction to the task details pane from our task list example. SQL vs. DocumentDB vs. Azure Search \u00b6 You may be wondering at this point why you should use Azure Search as an addition to SQL databases or DocumentDB. After all, all three options store data, allow you to update that data, and provide access to JSON documents. So why would I not just place all my data in a searchable index and use that instead? The Azure Search facility is designed for full-text searching of data, not rapid updates from multiple sources. As a result of this focus, it has no ability to do offline synchronization or conflict resolution, and it is lazy about inserts (data inserted may not be available straight away). It is not a good repository if your common practice is to work with the full data set and it is most definitely not the right source if you have more than one client with the potential to update records at the same time. It's primarily driven by a need to provide fast search results. DocumentDB and SQL Azure (via Azure Mobile Apps) are are the inverse of this. They are designed for concurrent and rapid updates to the data, with built-in conflict handling, incremental offline sync and guaranteed atomic writes. These are features that your mobile clients need when you are writing data. Both DocumentDB and Azure Mobile Apps also have security features for limiting data retrieval - something Azure Search does not have. Use Azure Search when you want to do \"shopping cart\" or \"reference data search\" type functionality. These use cases have slow changing data sets that are changed behind the scenes by a single client. Use Azure Mobile Apps or DocumentDB when you want multiple clients to update the data set, or your mobile client updates have security concerns.","title":"Azure Search"},{"location":"chapter7/search/#integrating-mobile-search","text":"Simple search capabilities can be handled by a little light LINQ usage and a small search bar. However, most catalog apps or video apps have search that seems almost magical in what it produces. It can handle multiple languages, bad spelling, the difference between singular and plural spellings. Once it has a set of matches, the order of the results is based on relevance to your search query and the app can highlight and provide additional search queries. All of this is provided by external search services. The most commonly used services are based on Lucene which is an open-source text search engine library from the Apache Project. Azure Search is no exception here. It provides a nice REST interface that can be used by your app to provide search results. In this chapters example, we are going to build something new and it doesn't use Azure Mobile Apps much at all. We are going to build a video search engine. We will have a number of videos that we will upload. Those videos will be processed by the backend and the audio and video content will be analyzed for searchable content. Our app will be able to search that content and get some matches back. To start, I've created a simple Xamarin Forms app with a single view (called Search). We'll update this app later on as we develop the code. For now, the code for the app is on the GitHub repository .","title":"Integrating Mobile Search"},{"location":"chapter7/search/#configuring-azure-search","text":"My project does not depend on Azure Mobile Apps this time (yet). Create a new Resource Group, then click on the + ADD button at the top of your resource group and add a new Azure Search resource. You will have to give it a name which becomes a part of the URL. All Azure Search resources are accessed through a https:// name .search.windows.net URL. Tip Since your users will not be typing the name in, this is a great time to use a GUID as the name - it fits the naming convention and is guaranteed to be unique. The only other decision of note is the Pricing Tier . You will be given the Standard tier, which covers 15 million documents per partition and up to 12 partitions, with up to 36 search units for scaling. It's an awesome production service. We are not exactly at that level of need yet. Fortunately, there is a free tier that covers a single scale unit, 50MB of storage and 10,000 documents - plenty for testing the service out. Click on the F Free option, then Select , followed by Create to create the resource. Creation of search resources is very quick - usually less than 15 seconds.","title":"Configuring Azure Search"},{"location":"chapter7/search/#creating-a-search-index","text":"Just like Azure Mobile Apps, there is no data in the service yet, so it's fairly useless. We need to create an index that can be searched. For right now, I've got a collection of videos. These documents are JSON objects that include the following fields: Id Title Image Rating Release Year Genre In Azure Search, the model for the objects going into the store need to have a type and you need to decide on some attributes. Exactly one field must be a \"key\" (we'll use the Id for this), and fields need to be marked Retrievable, Filterable, Sortable, Facetable and/or Searchable. Retrievable - the app can retrieve the field Sortable - it can be used to sort search results Filterable - it can be used in filter queries Searchable - it is a full-text search field The only one I've left out here is \"Facet-able\". This allows a field to be used in faceted navigation. This is a drill-down mechanism. If you have been on a web store like Amazon, you will have seen this feature. It's generally depicted as a \"Refine by\" field. For example, you may search for cars, but then want to limit the search to only convertibles, then only by red cars. Each of these refinements is a facet. If I added a \"genre\" to my fields, I could use faceted navigation. Back to my model, here it is: Field Name Type Attributes videoId Edm.String Key, Retrievable title Edm.String Retrievable, Sortable, Filterable, Searchable image Edm.String Retrievable rating Edm.Double Retrievable, Sortable, Filterable releaseYear Edm.Int32 Retrievable, Sortable, Filterable genre Collection(Edm.String) Retrievable, Filterable, Searchable, Facetable The types are based on OData v4 types . There are a large number of primitive types. Unfortunately, Azure Search only supports a subset of these . Warn You can't sort by the genre field because it is a collection. Collections cannot be marked sortable. You can create an index via the Azure Portal or with the REST interface. I'm going to define my \"videos\" index with JSON. Here is the JSON file: { \"name\": \"videos\", \"fields\": [ { \"name\": \"videoId\", \"type\": \"Edm.String\", \"key\": true, \"filterable\": false, \"searchable\": false, \"sortable\": false, \"facetable\": false }, { \"name\": \"title\", \"type\": \"Edm.String\", \"filterable\": true, \"sortable\": true, \"facetable\": false }, { \"name\": \"image\", \"type\": \"Edm.String\", \"filterable\": false, \"searchable\": false, \"sortable\": false, \"facetable\": false }, { \"name\": \"rating\", \"type\": \"Edm.Double\", \"filterable\": true, \"searchable\": false, \"sortable\": true, \"facetable\": false }, { \"name\": \"releaseYear\", \"type\": \"Edm.Int32\", \"filterable\": true, \"searchable\": false, \"sortable\": true, \"facetable\": false }, { \"name\": \"genre\", \"type\": \"Collection(Edm.String)\", \"filterable\": true, \"sortable\": false, \"facetable\": true } ] } Tip You can skip default values. Searchable and Retrievable default to true, so you only have to specify them if you want to turn that off. Similarly, key defaults to false, so you only have to specify the key on the field that needs it. To install this index, you will need the URI of the search service (which you already have) and the API Key. In the Azure Portal , open your Azure Search resource and click on Keys . You will see the primary and secondary ADMIN KEY . You can use either one. Open up Postman and issue a POST to URI/indexes?api-version=2016-09-01 . Set the Content-Type to application/json and add an api-key header set to the admin key. The content of the POST should be your JSON object. Info You can also install an index through the Azure Portal as a one time activity. I like using REST because it allows me to treat \"configuration as code\" and check my index definition into my source repository. This also opens up automated deployment options via PowerShell, for example. Click on the SEND button and you will see the return status of 201 Created and an OData v4 document in the response body giving the full definition of the index. You will receive a 400 series response with an error message if something goes wrong. It's likely to be either a duplicate index or a malformed JSON object. Free Limits You can create 1 free Azure Search instance and that supports 3 indices (or collections of documents).","title":"Creating a Search Index"},{"location":"chapter7/search/#populating-a-search-index-the-easy-way","text":"There are many ways of populating a search index. You can do what we are going to do - push data into an index. You can, however, also define an indexer that will crawl a data source periodically. Indexers are provided for Azure Blob and Table Storage, DocumentDB and SQL Server instances (through Azure SQL or SQL Server on a VM). I'm going to use a file from the Internet to populate the search index. I had to adjust it as the format for uploading is specific: { \"value\": [ { \"title\": \"Dawn of the Planet of the Apes\", \"image\": \"http://api.androidhive.info/json/movies/1.jpg\", \"rating\": 8.3, \"releaseYear\": 2014, \"genre\": [ \"Action\", \"Drama\", \"Sci-Fi\" ], \"@search.action\": \"upload\", \"videoId\": \"98ebe557-894c-48de-b61c-718f78b2adbb\" }, { \"title\": \"District 9\", \"image\": \"http://api.androidhive.info/json/movies/2.jpg\", \"rating\": 8, \"releaseYear\": 2009, \"genre\": [ \"Action\", \"Sci-Fi\", \"Thriller\" ], \"@search.action\": \"upload\", \"videoId\": \"e9d89038-386b-4aaa-a36c-530e2f3587c9\" }, ... ] } I can upload this document just like the creation of the index. The only difference is that I am POSTing to /indexes/videos/docs/index: If you see a 200 OK , then all the documents were uploaded and accepted. If you see a 207 message, then some of the documents were not uploaded. In that case, look at the response - each document will be referenced by the key field (in our case, the videoId field), the status field will be false and there will be an errorMessage field which contains the problem. You can correct the problem and upload just that document. There are four values for the @search.action field - normally, we will want to use \"upload\" for new documents and \"mergeOrUpload\" for subsequent updates.","title":"Populating a Search Index (the easy way)"},{"location":"chapter7/search/#testing-azure-search","text":"Now that we have a few documents indexed, we can test the search facility. Go to the Overview page of your search service. The indices are listed on the overview page and you can click on the videos index. You can then click on the Search Explorer to get into the test facility. Let's start with a fairly basic search for the word \"of\" across all searchable fields: We can also do boolean searches. For example, let's do the same search, but finding only comedies: Azure Search can accept a simple search string (as we have done here), an OData Filter using a restricted set of search criteria, or Lucene Search Syntax . The search explorer allows you to explore the various search mechanisms and their (sometimes peculiar) syntax.","title":"Testing Azure Search"},{"location":"chapter7/search/#other-service-considerations","text":"You will note the use of an API key for Azure Search. This can (and should) be regenerated at a regular interval. As a result, you will want a custom API that retrieves the current API key, perhaps only giving the API key to authenticated users. We covered custom APIs in an earlier chapter, so I won't cover that functionality here. Instead, the demonstration code will use a Settings.cs class in the client that contains the URI and API key for searching.","title":"Other Service Considerations"},{"location":"chapter7/search/#using-azure-search","text":"Before you can use Azure Search, you should generate a Query-Only API key for your Azure Search service. When we uploaded the documents to the search service for indexing (and if you intend to do any other administrative tasks through PowerShell or the REST API), you will use the Administrative API key. This key is found under the Keys menu item in the Azure Search resource in the Azure Portal. In the same place is a menu item called Manage query keys . The service creates one of these keys for you with an empty name. I like to create a query key for each version of the mobile software I release. I can thus retire keys that are no longer in use. To create a key: Click the + Add button. Enter a descriptive name (like \"iOS v1.0\", for example) Click Create . You can now copy and paste the key into your settings file. I have created a Settings.cs file in my shared project: using System; namespace VideoSearch { public static class Settings { public static string AzureSearchUri = \"https://zumbook.search.windows.net\"; /// <summary> /// Replace this with your API key from the Azure Search. You should /// never check in code with an API key in it - read the key from an /// Azure App Service App Setting and then provide it to your mobile /// clients via a custom API. /// </summary> public static string AzureSearchApiKey = \"88E95AB69AAAAB6FC5579E1CC40E7FC4\"; } } As we saw while we were testing the service, the search API is going to return a number of JSON objects. We can represent each return value with a model. Here is my Models/Movie.cs model: using System; using System.Collections.Generic; using Newtonsoft.Json; namespace VideoSearch.Models { public class Movie : SearchResult { [JsonProperty(PropertyName = \"videoId\")] public string Id { get; set; } public string Title { get; set; } public Uri Image { get; set; } public double Rating { get; set; } public int ReleaseYear { get; set; } [JsonProperty(PropertyName = \"genre\")] public List<string> Genres { get; set; } } } The Models/SearchResult.cs model adds the @search.score value that is returned in the search results: using Newtonsoft.Json; namespace VideoSearch.Models { public class SearchResult { [JsonProperty(PropertyName = \"@search.score\")] public double SearchScore { get; set; } } } Finally, the Models/MovieResults.cs class can be used to deserialize the entire JSON object that is returned by the server: using System.Collections.Generic; using Newtonsoft.Json; namespace VideoSearch.Models { public class MovieResults { [JsonProperty(PropertyName = \"@odata.context\")] public string Context { get; set; } [JsonProperty(PropertyName = \"value\")] public List<Movie> Movies { get; set; } } } I also provide a class called Services/SearchService.cs for handling search results. In this case, it will do the HTTP request to the specified server, sending the provided search string, and decode the response. It will throw an exception if the server produces an error: using System; using System.Collections.Generic; using System.Net.Http; using System.Net.Http.Headers; using System.Threading.Tasks; using Newtonsoft.Json; using VideoSearch.Models; namespace VideoSearch.Services { public class SearchService { private HttpClient _client; private string _apiVersion = \"2016-09-01\"; public SearchService() { this._client = new HttpClient(); } public async Task<List<Movie>> SearchMoviesAsync(string searchTerms) { var content = await SearchAsync(\"videos\", searchTerms); var movieResults = JsonConvert.DeserializeObject<MovieResults>(content); return movieResults.Movies; } private async Task<string> SearchAsync(string index, string searchTerms) { var uri = new UriBuilder($\"{Settings.AzureSearchUri}/indexes/{index}/docs\"); uri.Query = $\"api-version={_apiVersion}&search={Uri.EscapeDataString(searchTerms)}\"; var request = new HttpRequestMessage { RequestUri = uri.Uri, Method = HttpMethod.Get }; request.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\")); request.Headers.Add(\"api-key\", Settings.AzureSearchApiKey); var response = await _client.SendAsync(request); return await response.Content.ReadAsStringAsync(); } } } The SearchAsync() method is a basic HTTP GET method that returns a string. We add the appropriate headers and ensure the URI is the correct format. This service class can now be used to search for movies when we type something into the search box and initiate a search. This is done in the ViewModels\\Search.cs class in the shared project: public Command SearchCommand => _cmdSearch ?? (_cmdSearch = new Command(async () => await ExecuteSearchCommand())); private async Task ExecuteSearchCommand() { if (IsBusy) return; IsBusy = true; try { var results = await _service.SearchMoviesAsync(SearchString); SearchResults.ReplaceRange(results); } catch (Exception ex) { SearchResults.Clear(); await Application.Current.MainPage.DisplayAlert(\"Search Failed\", ex.Message, \"OK\"); } finally { IsBusy = false; } } The SearchString is a bindable string property that is bound to the Text field of the search bar. The SearchResults property is an ObservableRangeCollection that is bound to a list of items. This code creates a reference to our search service, then uses it to populate the search results PropertyName with the list of movies when the search is complete. We also needed a little bit of error handling for the edge case when the user types something in that isn't understood by the search service. This is likely to be rare, but we want to handle failures gracefully when they do happen. The associated view must also be adjusted so that the movies are displayed. This is done within a ListView object in a similar way to the way we did our task list examples: <ListView CachingStrategy=\"RecycleElement\" IsPullToRefreshEnabled=\"False\" ItemsSource=\"{Binding SearchResults}\" RowHeight=\"50\" SelectedItem=\"{Binding SelectedItem, Mode=TwoWay}\"> <ListView.BackgroundColor> <OnPlatform x:TypeArguments=\"Color\" Android=\"#2E2F30\" WinPhone=\"#F0F0F0\" iOS=\"#F0F0F0\" /> </ListView.BackgroundColor> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout Padding=\"10\" HorizontalOptions=\"FillAndExpand\" Orientation=\"Horizontal\" VerticalOptions=\"CenterAndExpand\"> <StackLayout.BackgroundColor> <OnPlatform x:TypeArguments=\"Color\" Android=\"Black\" WinPhone=\"White\" iOS=\"White\" /> </StackLayout.BackgroundColor> <Label HorizontalOptions=\"FillAndExpand\" Text=\"{Binding Title}\" TextColor=\"#272832\"> <Label.TextColor> <OnPlatform x:TypeArguments=\"Color\" Android=\"#F3F3F3\" WinPhone=\"#272832\" iOS=\"#272832\" /> </Label.TextColor> </Label> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> We can now run this application and start using the search service! In the example code , I've also included a Details panel. This should show you all the information within a movie title from the search results by clicking on the movie title. It's similar in construction to the task details pane from our task list example.","title":"Using Azure Search"},{"location":"chapter7/search/#sql-vs-documentdb-vs-azure-search","text":"You may be wondering at this point why you should use Azure Search as an addition to SQL databases or DocumentDB. After all, all three options store data, allow you to update that data, and provide access to JSON documents. So why would I not just place all my data in a searchable index and use that instead? The Azure Search facility is designed for full-text searching of data, not rapid updates from multiple sources. As a result of this focus, it has no ability to do offline synchronization or conflict resolution, and it is lazy about inserts (data inserted may not be available straight away). It is not a good repository if your common practice is to work with the full data set and it is most definitely not the right source if you have more than one client with the potential to update records at the same time. It's primarily driven by a need to provide fast search results. DocumentDB and SQL Azure (via Azure Mobile Apps) are are the inverse of this. They are designed for concurrent and rapid updates to the data, with built-in conflict handling, incremental offline sync and guaranteed atomic writes. These are features that your mobile clients need when you are writing data. Both DocumentDB and Azure Mobile Apps also have security features for limiting data retrieval - something Azure Search does not have. Use Azure Search when you want to do \"shopping cart\" or \"reference data search\" type functionality. These use cases have slow changing data sets that are changed behind the scenes by a single client. Use Azure Mobile Apps or DocumentDB when you want multiple clients to update the data set, or your mobile client updates have security concerns.","title":"SQL vs. DocumentDB vs. Azure Search"},{"location":"chapter7/services/","text":"When developing mobile applications, certain cloud services - data access, authentication and push notifications - almost always make their way into the requirements for the mobile backend. However, there are a number of other Azure services that can also make an appearance. Azure has a number of \"platform as a service\" type services. We've built Azure Mobile Apps on top of some of them - App Service and Notification Hubs. PaaS, as it is known, is a microservice where you don't have to deal with the underlying operating system or scaling issues. While we won't cover all possible Azure services, it's worth mentioning a few of the more useful ones here. In addition, there are some things you can't get from Azure. We'll take a look at those later on. We will be covering Search and Video services within this chapter in more depth. Azure Services \u00b6 The following services are available with your Azure subscription. The all incur some kind of cost, although search has a free tier. Cognitive Services \u00b6 Machine Learning has been a hot topic in development circles recently. It's highly complex and very specific to the application being handled. Fortunately, some applications are relatively easily generalized. For example, if I want to speak to my app (for example, like Siri or Cortana), then I can use Cognitive Services to process the speech. There are SDKs for speech processing, textual language analytics and image processing. The image processing SDKs are particularly useful for mobile photo applications. Content Delivery Networks and Traffic Manager \u00b6 I hope your application is successful. If you enjoy worldwide success or your mobile backend just can't go down, you will want to augment the single region solution with multiple region availability. Perhaps you just want an automated failover to another region within the country, or perhaps you want to support other continents with closer capabilities. Whatever the reason, you will want to take a look at Traffic Manager , which routes a HTTP request to the closest service endpoint. In addition, you may want to make static assets available closer to the user. You can't do anything about dynamic content. However, static assets like images, videos or long-lived documents can be published to a Content Delivery Network or CDN. The Azure CDN uses multiple providers (Akamai and Verizon) so that you can choose the footprint and costs that you need. DocumentDB \u00b6 We have concentrated on integrating SQL services with offline-sync capabilities in this book. However, there is a whole different paradigm for storing data on the server, mostly known as NoSQL stores. These use JSON blobs to store their data. Their main advantage is that you don't have to think about the schema of the data. The Azure entry into this market is called DocumentDB . It's highly scalable, geo-replicated, and allows you to define business logic in JavaScript to be run on the server (as stored procedures and triggers). NoSQL databases tend to be prevalent in gaming, social data and IoT applications. SQL tends to be more prevalent in web and enterprise applications. Both types of data store have their place in the development world. It really depends on what sort of work load you are considering with your app. For assistance with the choice, check out the NoSQL vs. SQL article. Media services \u00b6 One of the major pieces of functionality that I see in a lot of mobile applications is video. It may be an advertisement, a training video, or a major video platform like Netflix or Hulu. Video is a complex subject and can take up a book just by itself. Fortunately, Azure Media Services simplifies the process by providing a scalable platform for video encoding and streaming to your mobile clients. It also integrates media analytics, the ability to use cognitive services, and content protection in the platform. We are going to go more in-depth in Media Services later on. Search \u00b6 If you have some sort of catalog (for example, a shopping cart or video library), then it's likely you will want to search within the catalog or library. This is where Azure Search comes in. It is a platform service that implements an easily consumed API for searching your catalog. This goes beyond basic SQL queries and gets into natural language processing, fuzzy search, proximity search, term boosting and regular expressions. It supports 56 languages and provides feedback to your user in terms of search suggestions, highlighting and faceted navigation. We are going to do a more in-depth study of Azure Search later in the chapter. Service Fabric \u00b6 Not all mobile applications are straight-forward view-based applications. Some are games, for example, or highly scalable interactive platforms. When you need custom microservices on the backend, then Azure Service Fabric will provide an architectural framework for developing the most complex, low-latency, data-intensive scenarios. In these cases, a simple CRUD model is never going to be enough architecturally for the job of powering your backend. However, such complexity comes at a cost. In this case, the cost is the developer time necessry to invest in a highly scalable custom backend for your data flows. If you think that you need more than basic CRUD style APIs because of latency concerns, then Service Fabric is the appropriate service. Non-Azure Services \u00b6 Please note that I do not receive compensation and do not endorse any of the companies or products listed in this section. Please do your research and ensure that the product will assist you. Customer Feedback \u00b6 You can get some information from your customers without asking. Crash analytics and page-use analytics can be driven by code within your application and submitted in real-time or batched to an application analytics package like Mobile Center or App Insights . You may, however, need to ask your customers to leave feedback. It could be as simple as a button click (like the Visual Studio happy face / sad face), or it may be more extensive, providing support capabilities, star ratings, and free-form text. You can use these insights to drive UX improvements or features of your mobile app. Some companies that provide mobile-based feedback systems include OpinionLab and Apptentive . In App Purchases and Mobile Advertising \u00b6 One of the big topics that mobile application developers face is how to monetize (i.e. get paid) their app. Mobile app buyers do not like paying a lot for an app, so supplementary income comes from in-app purchases and mobile advertising. In-app purchases (or In-app billing, as Android folks call it) are driven via an SDK provided by the mobile platform - iOS or Android . You will need to integrate these SDKs into the platform-specific project, and Xamarin has some tools to assist for iOS and Android . Mobile Advertising is another good area for investigation. There are a number of considerations when choosing a mobile advertising partner (which is the partner who will feed your app advertisements to display and pay you for their display). The good news for most mobile app developers is that you can use the social authentication provider as a hint as to which mobile ad provider is going to be appropriate: Facebook uses Facebook App Ads Google uses AdMob There are also other mobile ad providers, like OGMobi , Smaato and AdIquity , should you not be using one of these social auth providers. It is not uncommon to see top-20 lists of mobile ad networks, so do some research - in particular, pay attention to the \"fill rate\" for customers in your area and the performance of the network (which is generally referred to as eCPM or effective cost per mille - one thousand impressions). Your mobile ad performance will be less if the advertisements being offered by the network are not appropriate for your customers. In general, you will need to integrate the ad network SDK into your application and add view elements to your pages where the mobile ads will be displayed. Real-time Communications \u00b6 You might want to get an instant alert of a pending change to a table, or perhaps produce the next great chat app. Real-time Communications keeps a TCP or UDP channel open to the mobile client for communications, allowing you to send instant alerts to the connected mobile clients. This enables quite a few collaborative use cases. There are quite a few \"standards\" here: WebRTC WebSockets SignalR socket.io WebRTC uses UDP underneath, so it's better for lossy communication with low latency - for example, audio or video. WebSockets (and the frameworks that depend on it, like socket.io and SignalR) use TCP. This means higher latency, but lossless communication. Since we are developing an ASP.NET mobile backend and using C# for the mobile application, SignalR would be more appropriate to use. You can find a full example of integrating SignalR with Xamarin Forms on GitHub .","title":"Useful Azure Services"},{"location":"chapter7/services/#azure-services","text":"The following services are available with your Azure subscription. The all incur some kind of cost, although search has a free tier.","title":"Azure Services"},{"location":"chapter7/services/#cognitive-services","text":"Machine Learning has been a hot topic in development circles recently. It's highly complex and very specific to the application being handled. Fortunately, some applications are relatively easily generalized. For example, if I want to speak to my app (for example, like Siri or Cortana), then I can use Cognitive Services to process the speech. There are SDKs for speech processing, textual language analytics and image processing. The image processing SDKs are particularly useful for mobile photo applications.","title":"Cognitive Services"},{"location":"chapter7/services/#content-delivery-networks-and-traffic-manager","text":"I hope your application is successful. If you enjoy worldwide success or your mobile backend just can't go down, you will want to augment the single region solution with multiple region availability. Perhaps you just want an automated failover to another region within the country, or perhaps you want to support other continents with closer capabilities. Whatever the reason, you will want to take a look at Traffic Manager , which routes a HTTP request to the closest service endpoint. In addition, you may want to make static assets available closer to the user. You can't do anything about dynamic content. However, static assets like images, videos or long-lived documents can be published to a Content Delivery Network or CDN. The Azure CDN uses multiple providers (Akamai and Verizon) so that you can choose the footprint and costs that you need.","title":"Content Delivery Networks and Traffic Manager"},{"location":"chapter7/services/#documentdb","text":"We have concentrated on integrating SQL services with offline-sync capabilities in this book. However, there is a whole different paradigm for storing data on the server, mostly known as NoSQL stores. These use JSON blobs to store their data. Their main advantage is that you don't have to think about the schema of the data. The Azure entry into this market is called DocumentDB . It's highly scalable, geo-replicated, and allows you to define business logic in JavaScript to be run on the server (as stored procedures and triggers). NoSQL databases tend to be prevalent in gaming, social data and IoT applications. SQL tends to be more prevalent in web and enterprise applications. Both types of data store have their place in the development world. It really depends on what sort of work load you are considering with your app. For assistance with the choice, check out the NoSQL vs. SQL article.","title":"DocumentDB"},{"location":"chapter7/services/#media-services","text":"One of the major pieces of functionality that I see in a lot of mobile applications is video. It may be an advertisement, a training video, or a major video platform like Netflix or Hulu. Video is a complex subject and can take up a book just by itself. Fortunately, Azure Media Services simplifies the process by providing a scalable platform for video encoding and streaming to your mobile clients. It also integrates media analytics, the ability to use cognitive services, and content protection in the platform. We are going to go more in-depth in Media Services later on.","title":"Media services"},{"location":"chapter7/services/#search","text":"If you have some sort of catalog (for example, a shopping cart or video library), then it's likely you will want to search within the catalog or library. This is where Azure Search comes in. It is a platform service that implements an easily consumed API for searching your catalog. This goes beyond basic SQL queries and gets into natural language processing, fuzzy search, proximity search, term boosting and regular expressions. It supports 56 languages and provides feedback to your user in terms of search suggestions, highlighting and faceted navigation. We are going to do a more in-depth study of Azure Search later in the chapter.","title":"Search"},{"location":"chapter7/services/#service-fabric","text":"Not all mobile applications are straight-forward view-based applications. Some are games, for example, or highly scalable interactive platforms. When you need custom microservices on the backend, then Azure Service Fabric will provide an architectural framework for developing the most complex, low-latency, data-intensive scenarios. In these cases, a simple CRUD model is never going to be enough architecturally for the job of powering your backend. However, such complexity comes at a cost. In this case, the cost is the developer time necessry to invest in a highly scalable custom backend for your data flows. If you think that you need more than basic CRUD style APIs because of latency concerns, then Service Fabric is the appropriate service.","title":"Service Fabric"},{"location":"chapter7/services/#non-azure-services","text":"Please note that I do not receive compensation and do not endorse any of the companies or products listed in this section. Please do your research and ensure that the product will assist you.","title":"Non-Azure Services"},{"location":"chapter7/services/#customer-feedback","text":"You can get some information from your customers without asking. Crash analytics and page-use analytics can be driven by code within your application and submitted in real-time or batched to an application analytics package like Mobile Center or App Insights . You may, however, need to ask your customers to leave feedback. It could be as simple as a button click (like the Visual Studio happy face / sad face), or it may be more extensive, providing support capabilities, star ratings, and free-form text. You can use these insights to drive UX improvements or features of your mobile app. Some companies that provide mobile-based feedback systems include OpinionLab and Apptentive .","title":"Customer Feedback"},{"location":"chapter7/services/#in-app-purchases-and-mobile-advertising","text":"One of the big topics that mobile application developers face is how to monetize (i.e. get paid) their app. Mobile app buyers do not like paying a lot for an app, so supplementary income comes from in-app purchases and mobile advertising. In-app purchases (or In-app billing, as Android folks call it) are driven via an SDK provided by the mobile platform - iOS or Android . You will need to integrate these SDKs into the platform-specific project, and Xamarin has some tools to assist for iOS and Android . Mobile Advertising is another good area for investigation. There are a number of considerations when choosing a mobile advertising partner (which is the partner who will feed your app advertisements to display and pay you for their display). The good news for most mobile app developers is that you can use the social authentication provider as a hint as to which mobile ad provider is going to be appropriate: Facebook uses Facebook App Ads Google uses AdMob There are also other mobile ad providers, like OGMobi , Smaato and AdIquity , should you not be using one of these social auth providers. It is not uncommon to see top-20 lists of mobile ad networks, so do some research - in particular, pay attention to the \"fill rate\" for customers in your area and the performance of the network (which is generally referred to as eCPM or effective cost per mille - one thousand impressions). Your mobile ad performance will be less if the advertisements being offered by the network are not appropriate for your customers. In general, you will need to integrate the ad network SDK into your application and add view elements to your pages where the mobile ads will be displayed.","title":"In App Purchases and Mobile Advertising"},{"location":"chapter7/services/#real-time-communications","text":"You might want to get an instant alert of a pending change to a table, or perhaps produce the next great chat app. Real-time Communications keeps a TCP or UDP channel open to the mobile client for communications, allowing you to send instant alerts to the connected mobile clients. This enables quite a few collaborative use cases. There are quite a few \"standards\" here: WebRTC WebSockets SignalR socket.io WebRTC uses UDP underneath, so it's better for lossy communication with low latency - for example, audio or video. WebSockets (and the frameworks that depend on it, like socket.io and SignalR) use TCP. This means higher latency, but lossless communication. Since we are developing an ASP.NET mobile backend and using C# for the mobile application, SignalR would be more appropriate to use. You can find a full example of integrating SignalR with Xamarin Forms on GitHub .","title":"Real-time Communications"},{"location":"chapter8/developing/","text":"The Development Environment \u00b6 In these last two chapters, I want to go over some of the complexities of developing mobile applications when there is a cloud-enabled backend. Working with cloud services presents its own special challenges, especially when you use the features of the provider. In the case of Azure App Service, this means that dealing with App Service Authentication and App Service Push requires some special configuration. Working with Azure Mobile Apps Locally \u00b6 In general, you can use Azure Mobile Apps locally by running inside a local IIS container. If you are using Visual Studio, then this is set up for you. However, you will need to handle the SQL database connection. You have two choices for this. Firstly, you can use a SQL Azure instance and just point your server to that instance. Secondly, you can install SQL Express and use that instance instead. Both are perfectly viable options. I like to use SQL Express in early development, then switch over to a SQL Azure instance as I get closer to deployment. Switching over when you are close to deployment enables to you detect any problems in upgrades or the use of encrypted channels. Configuring SQL Express for Local Development \u00b6 Start by downloading and installing the Microsoft SQL Server Express edition. Azure Mobile Apps will work with just about any recent SQL Server Express edition. I personally recommend the SQL Server 2016 SP1 Express edition. The process by which SQL Server Express is installed varies by edition and version, so keep these tips in mind. Always elect the custom installation option. You need the Database Engine and the Management Tools (possibly via separate download) You do not need reporting or integration services. Use Mixed mode for authentication and set an sa password. If possible, place the data directories on a different disk. Once you have installed the database engine and management tools, you will need to create a user that has permissions to create databases: Run the SQL Server Management Studio and connect to your local SQL Express instance. Ensure the SQL Server and Windows Authentication mode is checked in the Properties > Security page. Expand Security > Logins in the Object Explorer . Right-click the Logins node and select New login... . a. Enter a unique login name b. Select SQL Server authentication . c. Enter a password, then enter the same password in Confirm password . d. Click OK . Right-click on your new login and select Properties . a. Click Server Roles under Select a page . b. Check the box next to the dbcreator role. c. Click OK . Close the SQL Server Management Studio. Ensure you record the username and password you selected. You may need to assign additional server roles or permissions depending on your specific database requirements. Your connection string will need to look like this: Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word; Replace the user ID and password with the user ID and password you just created. The database will be created for you, so ensure it is rememberable. To set the connection string, you will need to edit the Web.config file. At around line 12, you will see the default connection string. Simply replace it with your SQL Express connection string: <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"Data Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word;\" providerName=\"System.Data.SqlClient\" /> </connectionStrings> You should now be able to press F5 on your server and run it locally. Configuring SQL Azure for Local Development \u00b6 I have already discussed creating a SQL Azure instance. By default, however, the SQL Azure instance can only be used by other Azure resources as a security measure. There is a firewall that limits connectivity from the Internet to your database. To run your server locally while connecting to the SQL Azure instance, you need to do two things: Open the firewall for connections from your IP address. Update the connection string in your servers Web.config file. From your development system (the workstation that you will be using to run the service locally), open a browser and log into the Azure portal . You will note that there are two resources for your SQL database - a server and a database. Open the resource for the SQL server, then: Click the Firewall menu option. Click + Add client IP . Click Save , then OK . If you are using a different workstation to run the service, then you can enter an explicit IP address. To get the connection strings: From the SQL Server Overview page, click the SQL database. From the SQL database Overview page, click Show database connection strings . Copy the ADO.NET (SQL authentication) connection string. This will need to be copied into the Web.config in the same way as the SQL Express version (above). You will need to replace the {your_username} and {your_password} strings with the username and password of your SQL server. If you can't remember them, look in your App Service - they are available in the configured connection string under Application Settings . Once this done, you will be able to press F5 on your server and run it locally. Handling Cloud Services while Developing Locally \u00b6 At this point, you have done plenty of test runs in the cloud and you have also run both your client and the server locally. However,t here are certain problems when you are running the server locally but using the Azure App Service resources for pieces of your application. The two main resources that mobile developers use in Azure App Service are authentication and registration for push notifications. The basics for these facilities were covered in Chapter 2 and Chapter 5 respectively. There are some additional code requirements when you want to run your main server locally for debugging. Handling Authentication \u00b6 When dealing with authentication, you want the majority of your requests to go to your main server, but you want your authentication requests to go to the Azure App Service you are using for development. To set this scenario up, you need to configure your mobile client to connect to your Azure App Service and receive a token, then continue using the local server. You also need to configure your local server so that it can decode the token. The Azure Mobile Apps Client SDK has a setting called AlternateLoginUrl . You can specify the URL of the Azure App Server in this if you are operating against a local server. For example: namespace TaskList.Helpers { public static class Locations { //#if DEBUG // public static readonly string AppServiceUrl = \"http://localhost:17568/\"; // public static readonly string AlternateLoginHost = \"https://the-book.azurewebsites.net\"; //#else public static readonly string AppServiceUrl = \"https://the-book.azurewebsites.net\"; public static readonly string AlternateLoginHost = null; //#endif } } In this snippet (from the Chapter 2 test project), I get the AlternateLoginHost set only if I define the DEBUG setting in my build properties. When I create the MobileServiceClient , I can use this logic to set the AlternateLoginUrl : public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler()); if (Locations.AlternateLoginHost != null) client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost); } To set the DEBUG setting, I can right-click on the project and select Properties , then select Build in the resulting window. The Define DEBUG constant is available in this window: Since my location is only set when the DEBUG constant is set, this allows me to switch between the two modes. There are other ways to switch between the two modes if you also want to support debug mode against an Azure App Service based server. One of my favorites is to have an application setting that enabled \"local mode\". When I check that, I can specify a local URL. At that point, the AppServerUrl is set to what I enter and the AlternateLoginUrl becomes the original value of the AppServiceUrl - effectively swapping to the local server but preserving the auth configuration. Now that I've enabled the mobile client for local development, I also need to enable the local server. The local server should have the following code already within the App_Start\\Startup.MobileApps.cs file: MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings(); if (string.IsNullOrEmpty(settings.HostName)) { app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions { // This middleware is intended to be used locally for debugging. By default, HostName will // only have a value when running in an App Service application. SigningKey = ConfigurationManager.AppSettings[\"SigningKey\"], ValidAudiences = new[] { ConfigurationManager.AppSettings[\"ValidAudience\"] }, ValidIssuers = new[] { ConfigurationManager.AppSettings[\"ValidIssuer\"] }, TokenHandler = config.GetAppServiceTokenHandler() }); } This code says \"if the service is not running within Azure App Service, then validate the authentication tokens using the following information\". We need the information provided from the Azure App Service that is producing the tokens. To obtain this information, we need to peek into Kudu - a backend service for examining the information that Azure App Service uses to run the service. Log on to the Azure portal . Select your Azure App Service. In the left-hand menu for the server, type Advanced Tools in the search box. Click Advanced Tools , then click Go . In the new window, click Environment , then Environment variables . Scroll down (or use search) to find WEBSITE_AUTH_SIGNING_KEY . Your required values are: SigningKey is the WEBSITE_AUTH_SIGNING_KEY above. ValidAudience and ValidIssuer are both of the form https://{your-sitename}/ . Once you have these two values, you can insert them into the Web.config file for local development: <appSettings> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> <add key=\"SigningKey\" value=\"4CFD0AA0148455E90F076F58D401377DA9D2443CC5C2E64E36C2D5FC96A71E9C\" /> <add key=\"ValidAudience\" value=\"https://the-book.azurewebsites.net/\" /> <add key=\"ValidIssuer\" value=\"https://the-book.azurewebsites.net/\" /> </appSettings> These values will be overridden by the portal settings when operating within the Azure App Service. You can now run the local server and the mobile client locally while authenticating via the Azure-based App Service. Handling Push Notifications \u00b6 In chapter 5, I espoused using InvokeApiAsync<>() to register for push notifications, mostly because it enabled me to register with a Notification Hubs Installation object rather than just my registration ID. This enabled me, for example, to register with tags for better audience selection. If you look at how the InvokeApiAsync method is implemented: private async Task<string> InternalInvokeApiAsync(string apiName, string content, HttpMethod method, IDictionary<string, string> parameters, MobileServiceFeatures features, CancellationToken cancellationToken = default(CancellationToken)) { method = method ?? defaultHttpMethod; if (parameters != null && parameters.Count > 0) { features |= MobileServiceFeatures.AdditionalQueryParameters; } MobileServiceHttpResponse response = await this.HttpClient.RequestAsync(method, CreateAPIUriString(apiName, parameters), this.CurrentUser, content, false, features: features, cancellationToken: cancellationToken); return response.Content; } You will note it's just a HTTP request with some serialization and deserialization around it. I can replicate this by creatign a new HttpClient object and publishing the same serialized content. I'm not concerned with the actual response, so this is fairly easy. The adjusted code might look something like this: try { var registrationId = GcmClient.GetRegistrationId(RootView); var client = new HttpClient(new Uri(Locations.AlternateLoginHost ?? Locations.AppServiceUrl)); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"gcm\", PushChannel = registrationId }; installation.Tags.Add(\"topic:Sports\"); installation.Templates.Add(\"genericTemplate\", new PustTemplate { Body = \"{\\\"data\\\":{\\\"message\\\":\\\"$(messageParam)\\\"}}\" }); // Register with NH var json = JsonConvert.SerializeObject(installation).ToString(); var response = await client.PutAsync( $\"/push/installations/{client.InstallationId}\", new StringContent(json, Encoding.UTF8, \"application/json\")); if (!response.IsSuccessStatusCode) { throw new MobileServiceInvalidOperationException(\"Invalid Response\", null, response); } } catch (Exception ex) { Log.Error(\"DroidPlatformProvider\", $\"Could not register with NH: {ex.Message}\"); } This uses a new HttpClient that is created for the purposes of registering with Notification Hubs. I have to handle all the serialization of the objects myself because I'm not operating in the confines of the Azure Mobile Apps SDK any more. The code differences here are minimal. Another way to deal with this is to stub the /push/installations endpoint in your server during local debugging with a standard WebAPI controller. This allows you to verify that the data being sent to the server is correct (i.e. your client code is correct) without actually registering for push notifications. You will not receive push notifications until you publish your service to Azure App Service and properly configure the App Service Push feature. Your controller will be over-ridden by App Service Push when you do publish to Azure App Service. Debugging your Cloud Mobile Backend \u00b6 When you are running the server locally, you can easily set breakpoints, view the state of the service, and emit log events that are captured in the Output window of your Visual Studio IDE. When you are running the server within Azure App Service, you can still do all these things. However, they are slower because you are accessing the service over the Internet rather than on your local machine. They also take a little bit more to set up. Diagnostic Logging \u00b6 Diagnostic logging is my preferred way of isolating problems. This is mostly because you can see historical context - what happened in what order - and you don't need to have a debugger attached to the process to use it. This means that diagnostic logging should be provided even after your service has move to production. However, diagnostic logging does require you to decide what information is important and to emit events associated with that information. You can configure diagnostic logging within the Azure portal or within Visual Studio. To configure diagnostic logging in the Azure portal : Open the Azure portal and select your Azure App Service. In the left-hand menu search box, type Diagnostic logs . Click the Diagnostic Logs menu item. In the Diagnostic logs page, turn on the following items: a. Application Logging (Filesystem) (also set the Level to Verbose ) b. Web server logging (select File System ) c. Detailed error messages d. Failed request tracing Click Save You can view the stream of logs from the Azure portal by selecting your Azure App Service and then using the Log Stream menu option in the left-hand menu. The logs are also available as text files via Kudu ( Advanced Tools in the left-hand menu) or via FTP (the location and login ID are provided on the Diagnostic Logs screen). To configure diagnostic logging within Visual Studio: Use View > Server Explorer to show the Server Explorer. Expand the Azure node, then App Service , then your resource group. Right-click your Azure App Service and select View Settings . In the Configuration panel, select the following: a. Web Server Logging = On b. Detailed Error Messages = On c. Failed Request Tracing = On d. Application Logging (File System) = Verbose Click Save . You can view the stream of logs from Visual Studio by right-clicking your Azure App Service within the Server Explorer , then selecting View Streaming Logs . The logs appear in a section of the Output window called Microsoft Azure Logs - {your-site} . The first message received will indicate that visual Studio is connected to the log-streaming service. Using the Visual Studio Debugger \u00b6 Even though diagnostic logging will probably be your go-to troubleshooting tool in the long run, there are times when you need to set a breakpoint and step through the code during development. In my experience, this happens quite a lot, especially when there are differences between how the server reacts when running locally vs. when running within Azure. Before you attach a debugger to an App Service running within Azure, you need to turn on remote debugging. This should NOT be done on a production service. You can turn on remote debugging within Visual Studio or in the Azure portal. For Visual Studio: Use View > Server Explorer to show the Server Explorer. Expand the Azure node, then App Service , then your resource group. Right-click your Azure App Service and select View Settings . In the Configuration panel, set Remote Debugging = On. Click Save . In the Azure portal , the remote debugging option is set through the Application settings menu option. You will need to set the Remote Visual Studio version to be the same version as your copy of Visual Studio. To debug your service, deploy a Debug version of your server. Then right-click the App Service within the Server Explorer and select Attach Debugger . You can now set breakpoints and step through the code. Server Hangups Using the debugger is also a great way to ensure that your mobile client handles slow links and service interruptions gracefully. As developers, we tend to have pretty good cellular coverage and fast connections to the Internet, but your users probably won't be in that situation. Set a breakpoint on the GetAll() method of your controller, trigger a synchronization, and then let the service just sit there at the breakpoint. Your mobile client will time out and you can see the effect of a bad connection. One of the other significant things you can do with your server when remote debugging is turned on is to profile the server. Profiling is a good way of determining performance bottlenecks in your code. Azure App Service supports remote profiling within Visual Studio for the server component. Use the Xamarin Profiler to profile your mobile application.","title":"The Development Environment"},{"location":"chapter8/developing/#the-development-environment","text":"In these last two chapters, I want to go over some of the complexities of developing mobile applications when there is a cloud-enabled backend. Working with cloud services presents its own special challenges, especially when you use the features of the provider. In the case of Azure App Service, this means that dealing with App Service Authentication and App Service Push requires some special configuration.","title":"The Development Environment"},{"location":"chapter8/developing/#working-with-azure-mobile-apps-locally","text":"In general, you can use Azure Mobile Apps locally by running inside a local IIS container. If you are using Visual Studio, then this is set up for you. However, you will need to handle the SQL database connection. You have two choices for this. Firstly, you can use a SQL Azure instance and just point your server to that instance. Secondly, you can install SQL Express and use that instance instead. Both are perfectly viable options. I like to use SQL Express in early development, then switch over to a SQL Azure instance as I get closer to deployment. Switching over when you are close to deployment enables to you detect any problems in upgrades or the use of encrypted channels.","title":"Working with Azure Mobile Apps Locally"},{"location":"chapter8/developing/#configuring-sql-express-for-local-development","text":"Start by downloading and installing the Microsoft SQL Server Express edition. Azure Mobile Apps will work with just about any recent SQL Server Express edition. I personally recommend the SQL Server 2016 SP1 Express edition. The process by which SQL Server Express is installed varies by edition and version, so keep these tips in mind. Always elect the custom installation option. You need the Database Engine and the Management Tools (possibly via separate download) You do not need reporting or integration services. Use Mixed mode for authentication and set an sa password. If possible, place the data directories on a different disk. Once you have installed the database engine and management tools, you will need to create a user that has permissions to create databases: Run the SQL Server Management Studio and connect to your local SQL Express instance. Ensure the SQL Server and Windows Authentication mode is checked in the Properties > Security page. Expand Security > Logins in the Object Explorer . Right-click the Logins node and select New login... . a. Enter a unique login name b. Select SQL Server authentication . c. Enter a password, then enter the same password in Confirm password . d. Click OK . Right-click on your new login and select Properties . a. Click Server Roles under Select a page . b. Check the box next to the dbcreator role. c. Click OK . Close the SQL Server Management Studio. Ensure you record the username and password you selected. You may need to assign additional server roles or permissions depending on your specific database requirements. Your connection string will need to look like this: Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word; Replace the user ID and password with the user ID and password you just created. The database will be created for you, so ensure it is rememberable. To set the connection string, you will need to edit the Web.config file. At around line 12, you will see the default connection string. Simply replace it with your SQL Express connection string: <connectionStrings> <add name=\"MS_TableConnectionString\" connectionString=\"Data Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word;\" providerName=\"System.Data.SqlClient\" /> </connectionStrings> You should now be able to press F5 on your server and run it locally.","title":"Configuring SQL Express for Local Development"},{"location":"chapter8/developing/#configuring-sql-azure-for-local-development","text":"I have already discussed creating a SQL Azure instance. By default, however, the SQL Azure instance can only be used by other Azure resources as a security measure. There is a firewall that limits connectivity from the Internet to your database. To run your server locally while connecting to the SQL Azure instance, you need to do two things: Open the firewall for connections from your IP address. Update the connection string in your servers Web.config file. From your development system (the workstation that you will be using to run the service locally), open a browser and log into the Azure portal . You will note that there are two resources for your SQL database - a server and a database. Open the resource for the SQL server, then: Click the Firewall menu option. Click + Add client IP . Click Save , then OK . If you are using a different workstation to run the service, then you can enter an explicit IP address. To get the connection strings: From the SQL Server Overview page, click the SQL database. From the SQL database Overview page, click Show database connection strings . Copy the ADO.NET (SQL authentication) connection string. This will need to be copied into the Web.config in the same way as the SQL Express version (above). You will need to replace the {your_username} and {your_password} strings with the username and password of your SQL server. If you can't remember them, look in your App Service - they are available in the configured connection string under Application Settings . Once this done, you will be able to press F5 on your server and run it locally.","title":"Configuring SQL Azure for Local Development"},{"location":"chapter8/developing/#handling-cloud-services-while-developing-locally","text":"At this point, you have done plenty of test runs in the cloud and you have also run both your client and the server locally. However,t here are certain problems when you are running the server locally but using the Azure App Service resources for pieces of your application. The two main resources that mobile developers use in Azure App Service are authentication and registration for push notifications. The basics for these facilities were covered in Chapter 2 and Chapter 5 respectively. There are some additional code requirements when you want to run your main server locally for debugging.","title":"Handling Cloud Services while Developing Locally"},{"location":"chapter8/developing/#handling-authentication","text":"When dealing with authentication, you want the majority of your requests to go to your main server, but you want your authentication requests to go to the Azure App Service you are using for development. To set this scenario up, you need to configure your mobile client to connect to your Azure App Service and receive a token, then continue using the local server. You also need to configure your local server so that it can decode the token. The Azure Mobile Apps Client SDK has a setting called AlternateLoginUrl . You can specify the URL of the Azure App Server in this if you are operating against a local server. For example: namespace TaskList.Helpers { public static class Locations { //#if DEBUG // public static readonly string AppServiceUrl = \"http://localhost:17568/\"; // public static readonly string AlternateLoginHost = \"https://the-book.azurewebsites.net\"; //#else public static readonly string AppServiceUrl = \"https://the-book.azurewebsites.net\"; public static readonly string AlternateLoginHost = null; //#endif } } In this snippet (from the Chapter 2 test project), I get the AlternateLoginHost set only if I define the DEBUG setting in my build properties. When I create the MobileServiceClient , I can use this logic to set the AlternateLoginUrl : public AzureCloudService() { client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler()); if (Locations.AlternateLoginHost != null) client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost); } To set the DEBUG setting, I can right-click on the project and select Properties , then select Build in the resulting window. The Define DEBUG constant is available in this window: Since my location is only set when the DEBUG constant is set, this allows me to switch between the two modes. There are other ways to switch between the two modes if you also want to support debug mode against an Azure App Service based server. One of my favorites is to have an application setting that enabled \"local mode\". When I check that, I can specify a local URL. At that point, the AppServerUrl is set to what I enter and the AlternateLoginUrl becomes the original value of the AppServiceUrl - effectively swapping to the local server but preserving the auth configuration. Now that I've enabled the mobile client for local development, I also need to enable the local server. The local server should have the following code already within the App_Start\\Startup.MobileApps.cs file: MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings(); if (string.IsNullOrEmpty(settings.HostName)) { app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions { // This middleware is intended to be used locally for debugging. By default, HostName will // only have a value when running in an App Service application. SigningKey = ConfigurationManager.AppSettings[\"SigningKey\"], ValidAudiences = new[] { ConfigurationManager.AppSettings[\"ValidAudience\"] }, ValidIssuers = new[] { ConfigurationManager.AppSettings[\"ValidIssuer\"] }, TokenHandler = config.GetAppServiceTokenHandler() }); } This code says \"if the service is not running within Azure App Service, then validate the authentication tokens using the following information\". We need the information provided from the Azure App Service that is producing the tokens. To obtain this information, we need to peek into Kudu - a backend service for examining the information that Azure App Service uses to run the service. Log on to the Azure portal . Select your Azure App Service. In the left-hand menu for the server, type Advanced Tools in the search box. Click Advanced Tools , then click Go . In the new window, click Environment , then Environment variables . Scroll down (or use search) to find WEBSITE_AUTH_SIGNING_KEY . Your required values are: SigningKey is the WEBSITE_AUTH_SIGNING_KEY above. ValidAudience and ValidIssuer are both of the form https://{your-sitename}/ . Once you have these two values, you can insert them into the Web.config file for local development: <appSettings> <add key=\"PreserveLoginUrl\" value=\"true\" /> <add key=\"MS_SigningKey\" value=\"Overridden by portal settings\" /> <add key=\"EMA_RuntimeUrl\" value=\"Overridden by portal settings\" /> <add key=\"MS_NotificationHubName\" value=\"Overridden by portal settings\" /> <add key=\"SigningKey\" value=\"4CFD0AA0148455E90F076F58D401377DA9D2443CC5C2E64E36C2D5FC96A71E9C\" /> <add key=\"ValidAudience\" value=\"https://the-book.azurewebsites.net/\" /> <add key=\"ValidIssuer\" value=\"https://the-book.azurewebsites.net/\" /> </appSettings> These values will be overridden by the portal settings when operating within the Azure App Service. You can now run the local server and the mobile client locally while authenticating via the Azure-based App Service.","title":"Handling Authentication"},{"location":"chapter8/developing/#handling-push-notifications","text":"In chapter 5, I espoused using InvokeApiAsync<>() to register for push notifications, mostly because it enabled me to register with a Notification Hubs Installation object rather than just my registration ID. This enabled me, for example, to register with tags for better audience selection. If you look at how the InvokeApiAsync method is implemented: private async Task<string> InternalInvokeApiAsync(string apiName, string content, HttpMethod method, IDictionary<string, string> parameters, MobileServiceFeatures features, CancellationToken cancellationToken = default(CancellationToken)) { method = method ?? defaultHttpMethod; if (parameters != null && parameters.Count > 0) { features |= MobileServiceFeatures.AdditionalQueryParameters; } MobileServiceHttpResponse response = await this.HttpClient.RequestAsync(method, CreateAPIUriString(apiName, parameters), this.CurrentUser, content, false, features: features, cancellationToken: cancellationToken); return response.Content; } You will note it's just a HTTP request with some serialization and deserialization around it. I can replicate this by creatign a new HttpClient object and publishing the same serialized content. I'm not concerned with the actual response, so this is fairly easy. The adjusted code might look something like this: try { var registrationId = GcmClient.GetRegistrationId(RootView); var client = new HttpClient(new Uri(Locations.AlternateLoginHost ?? Locations.AppServiceUrl)); var installation = new DeviceInstallation { InstallationId = client.InstallationId, Platform = \"gcm\", PushChannel = registrationId }; installation.Tags.Add(\"topic:Sports\"); installation.Templates.Add(\"genericTemplate\", new PustTemplate { Body = \"{\\\"data\\\":{\\\"message\\\":\\\"$(messageParam)\\\"}}\" }); // Register with NH var json = JsonConvert.SerializeObject(installation).ToString(); var response = await client.PutAsync( $\"/push/installations/{client.InstallationId}\", new StringContent(json, Encoding.UTF8, \"application/json\")); if (!response.IsSuccessStatusCode) { throw new MobileServiceInvalidOperationException(\"Invalid Response\", null, response); } } catch (Exception ex) { Log.Error(\"DroidPlatformProvider\", $\"Could not register with NH: {ex.Message}\"); } This uses a new HttpClient that is created for the purposes of registering with Notification Hubs. I have to handle all the serialization of the objects myself because I'm not operating in the confines of the Azure Mobile Apps SDK any more. The code differences here are minimal. Another way to deal with this is to stub the /push/installations endpoint in your server during local debugging with a standard WebAPI controller. This allows you to verify that the data being sent to the server is correct (i.e. your client code is correct) without actually registering for push notifications. You will not receive push notifications until you publish your service to Azure App Service and properly configure the App Service Push feature. Your controller will be over-ridden by App Service Push when you do publish to Azure App Service.","title":"Handling Push Notifications"},{"location":"chapter8/developing/#debugging-your-cloud-mobile-backend","text":"When you are running the server locally, you can easily set breakpoints, view the state of the service, and emit log events that are captured in the Output window of your Visual Studio IDE. When you are running the server within Azure App Service, you can still do all these things. However, they are slower because you are accessing the service over the Internet rather than on your local machine. They also take a little bit more to set up.","title":"Debugging your Cloud Mobile Backend"},{"location":"chapter8/developing/#diagnostic-logging","text":"Diagnostic logging is my preferred way of isolating problems. This is mostly because you can see historical context - what happened in what order - and you don't need to have a debugger attached to the process to use it. This means that diagnostic logging should be provided even after your service has move to production. However, diagnostic logging does require you to decide what information is important and to emit events associated with that information. You can configure diagnostic logging within the Azure portal or within Visual Studio. To configure diagnostic logging in the Azure portal : Open the Azure portal and select your Azure App Service. In the left-hand menu search box, type Diagnostic logs . Click the Diagnostic Logs menu item. In the Diagnostic logs page, turn on the following items: a. Application Logging (Filesystem) (also set the Level to Verbose ) b. Web server logging (select File System ) c. Detailed error messages d. Failed request tracing Click Save You can view the stream of logs from the Azure portal by selecting your Azure App Service and then using the Log Stream menu option in the left-hand menu. The logs are also available as text files via Kudu ( Advanced Tools in the left-hand menu) or via FTP (the location and login ID are provided on the Diagnostic Logs screen). To configure diagnostic logging within Visual Studio: Use View > Server Explorer to show the Server Explorer. Expand the Azure node, then App Service , then your resource group. Right-click your Azure App Service and select View Settings . In the Configuration panel, select the following: a. Web Server Logging = On b. Detailed Error Messages = On c. Failed Request Tracing = On d. Application Logging (File System) = Verbose Click Save . You can view the stream of logs from Visual Studio by right-clicking your Azure App Service within the Server Explorer , then selecting View Streaming Logs . The logs appear in a section of the Output window called Microsoft Azure Logs - {your-site} . The first message received will indicate that visual Studio is connected to the log-streaming service.","title":"Diagnostic Logging"},{"location":"chapter8/developing/#using-the-visual-studio-debugger","text":"Even though diagnostic logging will probably be your go-to troubleshooting tool in the long run, there are times when you need to set a breakpoint and step through the code during development. In my experience, this happens quite a lot, especially when there are differences between how the server reacts when running locally vs. when running within Azure. Before you attach a debugger to an App Service running within Azure, you need to turn on remote debugging. This should NOT be done on a production service. You can turn on remote debugging within Visual Studio or in the Azure portal. For Visual Studio: Use View > Server Explorer to show the Server Explorer. Expand the Azure node, then App Service , then your resource group. Right-click your Azure App Service and select View Settings . In the Configuration panel, set Remote Debugging = On. Click Save . In the Azure portal , the remote debugging option is set through the Application settings menu option. You will need to set the Remote Visual Studio version to be the same version as your copy of Visual Studio. To debug your service, deploy a Debug version of your server. Then right-click the App Service within the Server Explorer and select Attach Debugger . You can now set breakpoints and step through the code. Server Hangups Using the debugger is also a great way to ensure that your mobile client handles slow links and service interruptions gracefully. As developers, we tend to have pretty good cellular coverage and fast connections to the Internet, but your users probably won't be in that situation. Set a breakpoint on the GetAll() method of your controller, trigger a synchronization, and then let the service just sit there at the breakpoint. Your mobile client will time out and you can see the effect of a bad connection. One of the other significant things you can do with your server when remote debugging is turned on is to profile the server. Profiling is a good way of determining performance bottlenecks in your code. Azure App Service supports remote profiling within Visual Studio for the server component. Use the Xamarin Profiler to profile your mobile application.","title":"Using the Visual Studio Debugger"},{"location":"chapter8/testing/","text":"Testing your Mobile Application \u00b6 There is nothing that causes more problems than when a developer works on testing. Testing a cross-platform client-server application across all the permutations that are possible is hard work. You will spend more time on developing tests than on writing code. Much of what is asked, however, is not required. That is primarily because most people want to test the entire stack. There are generally minimal custom code in the backend, so that can significantly reduce the amount of tests you write. In this section, we will look at what it takes to do unit tests for your mobile backend and the mobile app, together with an end-to-end testing capability that allows you to test your application on many devices at once. Testing your Mobile Backend \u00b6 Most of the code within the mobile backend is pulled from libraries - ASP.NET, Entity Framework and Azure Mobile Apps. These libraries are already tested before release and there is not much you can do about bugs other than reporting them (although Azure Mobile Apps does accept fixes as well). As a result, you should concentrate your testing on the following areas: Filters, Transforms and Actions associated with your table controllers. Custom APIs. You should also do \"end-to-end\" testing. This is where you use UI testing to test both the client and the server at the same time. End to end testing is a much better test of the overall functionality of your server. In addition, your mobile backend will come under a lot of strain after you go to production. You should plan on a load test prior to each major release in a staging environment that is identical to your production environment. We'll cover this later in the book. Unit Testing \u00b6 Let's take a simple example of an app that we developed back in Chapter 3. We used the data connections to develop a personal todo store - one in which the users ID is associated with each submitted record and the user could only see their own records. The table controller looked like the following: namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request, enableSoftDelete: true); } public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; public void ValidateOwner(string id) { var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault<TodoItem>(); if (result == null) { throw new HttpResponseException(HttpStatusCode.NotFound); } } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query().PerUserFilter(UserId); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) { return new SingleResult<TodoItem>(Lookup(id).Queryable.PerUserFilter(UserId)); } // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) { ValidateOwner(id); return UpdateAsync(id, patch); } // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { item.UserId = UserId; TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) { ValidateOwner(id); return DeleteAsync(id); } } } In addition, we have a LINQ extension method for handling the PerUserFilter : using Backend.DataObjects; using System.Linq; namespace Backend.Extensions { public static class PerUserFilterExtension { public static IQueryable<TodoItem> PerUserFilter(this IQueryable<TodoItem> query, string userid) { return query.Where(item => item.UserId.Equals(userid)); } } } In my minimalist testing suggestion, I would test the following: The LINQ Extension PerUserFilter . The UserId property. The ValidateOwner method. The other methods are straight out of the standard table controller. I would defer unit testing of these until the end-to-end tests. Unit tests should be short and should be idempotent. The test should be able to be run multiple times and always return the same result. Since our service is defined to be run out of a stateful SQL database, it cannot be defined to be idempotent. However, the individual parts we are operating can be idempotent. Unit tests are generally defined to be a separate project within the Visual Studio solution. By convention, they are named by appending .Tests to the project they are testing. My project is called Backend , so the test project is called Backend.Tests . To create the test project: Open the solution in Visual Studio. Right-click the solution, choose Add -> New Project... . Select Installed > Visual C# > Test in the project type tree. Select xUnit Test Project as the project type. Enter Backend.Tests as the name, then click OK . xUnit vs. MSTest vs. Others Most version of Visual Studio support a specific type of test called MSTest . However, Visual Studio 2017 has integrated xUnit testing as well. xUnit is cross-platform whereas MSTest is PC only. I will be using xUnit for this project. If you are using a version of Visual Studio earlier than VS2017, you will not have the xUnit Test Project available. However, you can simulate the same project type manually . In addition, there are other test frameworks available. We will only be covering xUnit here. Generally, copy the folder format from the main project to the test project. For example, the PerUserFilterExtension.cs file is in an Extensions folder within the main project. I'm going to create an Extensions folder within the test project and create a PerUserFilterExtensionTests.cs file with the tests in it. To create the tests: Right-click the Extensions folder, and select Add -> New Item... . Select Installed > Visual C# Items > Test in the project type tree. Select xUnit Test , and enter PerUserFilterExtensionTests.cs as the name. Click Add . Add your Project under Test as a Reference You will need to add your project under test (in this case, the Backend project) as a reference to the test project. You will get this code generated: using System; using System.Linq; using Xunit; namespace Backend.Tests.Extensions { public class PerUserFilterExtensionTests { [Fact] public void TestMethod1() { } } } We are going to replace the TestMethod1() method with our unit tests. XUnit tests are designated with the [Fact] attribute. You do some work on the class to test specific conditions, then assert that the results are valid. In the case of this class, for instance, we want to test that the result is correct under the following conditions: A valid string is provided. A zero-length string is provided. Null is provided. Under no conditions should the extension method throw an exception. That means three tests, coded thusly: using Backend.DataObjects; using Backend.Extensions; using System; using System.Collections.Generic; using System.Linq; using Xunit; namespace Backend.Tests.Extensions { public class PerUserFilterExtensionTests { [Fact] public void UserId_Is_Valid() { List<TodoItem> items = new List<TodoItem> { new TodoItem { UserId = \"test\", Text = \"Task 1\", Complete = false }, new TodoItem { UserId = \"test2\", Text = \"Task 2\", Complete = true }, new TodoItem { UserId = \"test\", Text = \"Task 3\", Complete = false } }; var result = items.AsQueryable<TodoItem>().PerUserFilter(\"test\"); Assert.NotNull(result); Assert.Equal(2, result.Count()); } [Fact] public void UserId_Is_Empty() { List<TodoItem> items = new List<TodoItem> { new TodoItem { UserId = \"test\", Text = \"Task 1\", Complete = false }, new TodoItem { UserId = \"test2\", Text = \"Task 2\", Complete = true }, new TodoItem { UserId = \"test\", Text = \"Task 3\", Complete = false } }; var result = items.AsQueryable<TodoItem>().PerUserFilter(String.Empty); Assert.NotNull(result); Assert.Equal(0, result.Count()); } [Fact] public void UserId_Is_Null() { List<TodoItem> items = new List<TodoItem> { new TodoItem { UserId = \"test\", Text = \"Task 1\", Complete = false }, new TodoItem { UserId = \"test2\", Text = \"Task 2\", Complete = true }, new TodoItem { UserId = \"test\", Text = \"Task 3\", Complete = false } }; var result = items.AsQueryable<TodoItem>().PerUserFilter(null); Assert.NotNull(result); Assert.Equal(0, result.Count()); } } } Use the same .NET Framework Version You will note that your tests will not compile at this point. That is because the server is dependent on .NET Framework 4.6 and the test project is created with .NET Framework 4.5. Both test and main project must be configured to use the same version of the .NET Framework. Right-click the test project, select Properties , then change the version of the .NET Framework to match your main project. Save and re-build your test project. Visual Studio has a couple of methods of running the tests. Visual Studio 2017 has in-built support for the xUnit test runner. You may have to download an extension or run them manually in earlier versions of Visual Studio. My favorite way of running the tests is to open the Test Explorer using Test -> Windows -> Test Explorer , then click Run All . You can then right-click the Test Explorer tab and select Float to float it as a window. This allows you to enlarge the window so you can see the tests after they have run: As you can see, my tests all passed. I can run these tests as many times as necessary as they do not depend on external requirements. This is not generally the case with table controllers. The table controller takes a dependency on a domain manager (most normally, the EntityDomainManager ). The EntityDomainManager is configured to use a database via a connection string. Thus, we need to do things differently for testing table controllers even if we only test the unique functionality. Let's take a look at the tests for the UserId property. The UserId property contains the contents of the NameIdentifier claim. My tests for this are: A correct set of claims are provided. An incomplete set of claims (without a NameIdentifier) are provided. No claims are provided. The first and last are the typical authenticated and anonymous access tests. The first should provide the username in the NameIdentifier, and the latter should throw an error. The middle test is an important one for us. What do you want to happen if the user is authenticated, but the NameIdentifier claim was not provided? It's bad form for us to return a 500 Internal Server Error, even though that would be appropriate here. Instead I want to assume that the user id is blank so that everything keeps on working. (One can argue that this is not correct either!) Install the same NuGet packages Unlike the scaffolded project for Azure Mobile Apps or ASP.NET MVC, no additional packages are added to the test project, which means you will need to figure out which packages you need to simulate the requirements for the test. Don't guess. Look at the packages that are in your project under test and duplicate them. Right-click the solution and select Manage NuGet Packages to get a good idea of what your test package is missing. Under the Installed list, you can tell what packages are required and which projects have them installed. Mock the ClaimsIdentity to test authentication: using System.Security.Claims; namespace Backend.Tests.Utilities { public class TestPrincipal : ClaimsPrincipal { public TestPrincipal(params Claim[] claims) : base(new TestIdentity(claims)) { } } public class TestIdentity : ClaimsIdentity { public TestIdentity(params Claim[] claims) : base(claims) { } } } My (incorrect - deliberately) test looks like the following: using Backend.Controllers; using Backend.Tests.Utilities; using System.Security.Claims; using System.Threading; using Xunit; namespace Backend.Tests.Controllers { public class TodoItemControllerTests { [Fact] public void UserId_With_Correct_Claims() { var controller = new TodoItemController(); controller.User = new TestPrincipal( new Claim(\"name\", \"testuser\"), new Claim(\"sub\", \"foo\") ); var result = controller.UserId; Assert.NotNull(result); Assert.Equal(\"testuser\", result); } [Fact] public void UserId_With_Incomplete_Claims() { var controller = new TodoItemController(); controller.User = new TestPrincipal( new Claim(\"sub\", \"foo\") ); var result = controller.UserId; Assert.Null(result); } [Fact] public void UserId_With_Null_Claims() { var controller = new TodoItemController(); controller.User = null; var ex = Assert.Throws<HttpResponseException>(() => { var result = controller.UserId; }); Assert.Equal(HttpStatusCode.Unauthorized, ex.Response.StatusCode); } } } The UserId_With_Null_Claims test is an interesting recipe for testing that the right exception is thrown. In this case, I expect the methods to return a 401 Unauthorized response to the client. Of course, the [Authorize] tag will do this for my well before my code is hit, but it's good to be accurate. If I run the tests, I get the following: What I want to do is run that test again, but attach a debugger. To do this, set a breakpoint on the property in the TodoItemController . Then right-click the failing test and select Debug Selected Tests . This runs the test with a debugger connected. The breakpoint you set will be hit and you will be able to inspect the code state while it is running. The first test is failing because ClaimTypes.NameIdentifier is not \"name\". I re-wrote the test as follows: [Fact] public void UserId_With_Correct_Claims() { var controller = new TodoItemController(); controller.User = new TestPrincipal( new Claim(ClaimTypes.NameIdentifier, \"testuser\"), new Claim(\"sub\", \"foo\") ); var result = controller.UserId; Assert.NotNull(result); Assert.Equal(\"testuser\", result); } This test will now pass. The other two tests are actually the result of incorrect code. I've adjusted the code accordingly: public string UserId { get { if (User == null) { throw new HttpResponseException(HttpStatusCode.Unauthorized); } var principal = User as ClaimsPrincipal; Claim cl = principal.Claims.FirstOrDefault(c => c.Type == ClaimTypes.NameIdentifier); return cl?.Value; } } This is a little longer than the original one-liner, but it's more accurate. This means that when I've forgotten what this particular method does in six months time, it will still do the right thing in all conditions. Use Test-Driven Development There is a whole school of thought on how to develop using testing as the driver known as Test Driven Development or TDD. In this school of thought, you write the tests first, ensuring you have 100% of the cases covered. Your code is correct when the tests pass. This method provides for very rapid development, but you do spend most of your time developing tests rather than code. The other big class of testing to do is against custom APIs. You can test these the same way. For example, the standard scaffolding for an Azure Mobile Apps server contains a ValuesController.cs , which I have modified: using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; namespace Backend.Controllers { // Use the MobileAppController attribute for each ApiController you want to use // from your mobile clients [MobileAppController] public class ValuesController : ApiController { // GET api/values public string Get(string user) { return $\"Hello {user}!\"; } } } I can test this with the following: using Backend.Controllers; using Xunit; namespace Backend.Tests.Controllers { public class ValuesControllerTests { [Fact] public void Get_Name_Works() { var controller = new ValuesController(); var result = controller.Get(\"adrian\"); Assert.Equal(\"Hello adrian!\", result); } } } As with all other testing, ensure you think about all the things that could happen here, and test them all. Ensure that the appropriate response is always returned and that you are never leave your server or a connected client in a bad state. A big example of this in the case of mobile apps: If a response is meant to be a JSON encoded version of an object on your client, ensure it can be deserialized to that object under all conditions. Testing your Mobile Client \u00b6 Testing your mobile client will generally be a multi-part affair: Implement mock data services and test the UI in isolation. Implement unit tests for the non-UI components. Do end-to-end tests to ensure both client and server work together. Unit tests for non-UI code is the same as the server-side code. You need to write the tests in a unit test framework like xUnit or MSTest . Use Test-driven development to ensure that your code is living up to its contract. Using Mock Data Services \u00b6 Unfortunately, setting up repeatable unit tests becomes increasingly difficult in a client-server application such as a cloud-enabled mobile app. For these aspects, you want to mock the data services. If you have followed along from the beginning, we've actually done a lot of the hard work for this. Create an Interface that represents the interface to the data service. Create a concrete implementation of that interface. Use a dependency injection service to inject the concrete implementation. The whole idea here is that changing just one line of code will enable you to update from the mock implementation to the cloud implementation. This allows you to develop the UI independently of the backend communication code, and allows you to do repeatable UI tests later on. Let's take a look at an example. In my Chapter8 project , I've got the Xamarin Forms application from the very first chapter . In the shared TaskList project, there is an Abstractions folder that contains the definitions for ICloudService : namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; } } There is also a definition for ICloudTable : using System.Collections.Generic; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); } } The important part of this is this. The only place where the concrete edition, AzureCloudService() , is mentioned is in the App.cs file: using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { CloudService = new AzureCloudService(); MainPage = new NavigationPage(new Pages.EntryPage()); } } } Everywhere else uses the ICloudService interface and does not mention the concrete version. The application sets up the cloud service and every other class uses it. This allows us to set up a mock cloud service as follows. First, let's define the MockCloudService : using System.Collections.Generic; using TaskList.Abstractions; namespace TaskList.Services { public class MockCloudService : ICloudService { public Dictionary<string, object> tables = new Dictionary<string, object>(); public ICloudTable<T> GetTable<T>() where T : TableData { var tableName = typeof(T).Name; if (!tables.ContainsKey(tableName)) { var table = new MockCloudTable<T>(); tables[tableName] = table; } return (ICloudTable<T>)tables[tableName]; } } } It's very similar to the AzureCloudService class, but there is no MobileServiceClient . Instead, we store the cloud table instances in a dictionary to ensure successive calls to GetTable<>() return the same singleton reference. We aren't using the backend service. Similarly, we use a Dictionary<> to hold the items instead of the backend service in the MockCloudTable class: using Microsoft.WindowsAzure.MobileServices; using System; using System.Collections.Generic; using System.Text; using System.Threading.Tasks; using TaskList.Abstractions; #pragma warning disable CS1998 // Async method lacks 'await' operators and will run synchronously namespace TaskList.Services { public class MockCloudTable<T> : ICloudTable<T> where T : TableData { private Dictionary<string, T> items = new Dictionary<string, T>(); private int currentVersion = 1; public async Task<T> CreateItemAsync(T item) { item.Id = Guid.NewGuid().ToString(\"N\"); item.CreatedAt = DateTimeOffset.Now; item.UpdatedAt = DateTimeOffset.Now; item.Version = ToVersionString(currentVersion++); items.Add(item.Id, item); return item; } public async Task DeleteItemAsync(T item) { if (item.Id == null) { throw new NullReferenceException(); } if (items.ContainsKey(item.Id)) { items.Remove(item.Id); } else { throw new MobileServiceInvalidOperationException(\"Not Found\", null, null); } } public async Task<ICollection<T>> ReadAllItemsAsync() { List<T> allItems = new List<T>(items.Values); return allItems; } public async Task<T> ReadItemAsync(string id) { if (items.ContainsKey(id)) { return items[id]; } else { throw new MobileServiceInvalidOperationException(\"Not Found\", null, null); } } public async Task<T> UpdateItemAsync(T item) { if (item.Id == null) { throw new NullReferenceException(); } if (items.ContainsKey(item.Id)) { item.UpdatedAt = DateTimeOffset.Now; item.Version = ToVersionString(currentVersion++); items[item.Id] = item; return item; } else { throw new MobileServiceInvalidOperationException(\"Not Found\", null, null); } } private byte[] ToVersionString(int i) { byte[] b = BitConverter.GetBytes(i); string str = Convert.ToBase64String(b); return Encoding.ASCII.GetBytes(str); } } } The mock service is instantiated within the App.cs file: using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { #if USE_MOCK_SERVICES CloudService = new MockCloudService(); #else CloudService = new AzureCloudService(); #endif MainPage = new NavigationPage(new Pages.EntryPage()); } } } Finally, I need to actually define USE_MOCK_SERVICES somewhere. Right-click the project and select Properties . Click Build . Add the USE_MOCK_SERVICES to the Conditional compilation symbols (which is a semi-colon separated list). Save the properties then rebuild the project you modified. You can run this version without any backend at all. It will not persist the data, but that's the point of mock data services. Use a new Configuration Another, more advanced, way of accomplishing this is to set up a new configuration. You can see the configuration is \"Active (Debug)\". You can add another configuration to this list called \"Mock Services\" by using the Build > Configuration Manager... . When you select that configuration, the mock services will automatically be brought in. UI Testing \u00b6 The mock services are a tool to enable UI unit testing. UI testing is unit testing for your UI. These are small tests that are executed on a real device and check to see if your main UI flows work as expected. There are actually a few ways of creating tests. I'm going to produce a simple test. In the test, I will simulate clicking on the entry button and ensuring that the task list page is produced. This test can then be run against one or more devices. Let's start by creating a TaskList.Tests project: Right-click the solution and select Add -> New Project... In the Installed > Visual C# > Cross-Platform node of the tree, select UI Test App (Xamarin.UITest | Cross-Platform) . Give it a snappy name, like TaskList.Tests , then click OK . Wait for the project to be created. Right-click the References node in the newly created project and select Add Reference... . Click Projects in the left hand side-bar. Click TaskList.Android . Ensure there is a checked box next to the TaskList.Android project. Click OK . We are only going to test the Android edition of the project in this walkthrough, mostly because I do most of my work on a PC. The same methodology can be used for iOS, however. Use NUnit v2.x Xamarin.UITest does not support NUnit 3.x - make sure you do not upgrade the NUnit framework beyond the latest v2.x release when updating your NuGet packages. The project contains two source files - AppInitializer.cs and Tests.cs . This latter file is where we are going to write the tests. The most efficient way of writing tests is to use the Xamarin Test Recorder . The workflow for using Xamarin Test Recorder is: If needed, export the mobile client so that it can be used on a device. Start Xamarin Test Recorder Specify the application to be tested and the device to run the aplication on. Interact with the application. The Test Recorder will create a C# test method. Incorporate the test into a Xamarin UITest project. We already have the UITest project, so let's walk through the process of creating a simple test case for the process of creating a new task. Open the Tests.cs file. Note the \"lightning\" icon next to each [TestFixture] attribute: Switch to a Release build in the configuration manager (or on the toolbar), then right-click the TaskList.Android project and select Build to build the project. Now that you have a working app, click the lightning icon next to the TestFixture for the Android platform, then select Record new test -> Build TaskList.Android project . This will start your project in the selected device - normally an emulator. It will also create a NewTest() method. In the emulator, click on the Login button, followed by Add New Item, followed by Save. When you are done, switch back to the Visual Studio instance, click on the spanner next at the bottom of the file and select Stop Recording . The following code will be generated: [Test] public void NewTest() { app.Tap(x => x.Text(\"Login\")); app.Tap(x => x.Text(\"Add New Item\")); app.Tap(x => x.Text(\"Save\")); } You can take screen shots, by adding app.Screenshot(\"description\"); in between each step: [Test] public void NewTest() { app.Tap(x => x.Text(\"Login\")); app.Screenshot(\"Logged in - initial list of items\"); app.Tap(x => x.Text(\"Add New Item\")); app.Screenshot(\"Empty detail record\"); app.Tap(x => x.Text(\"Save\")); app.Screenshot(\"Back at list of items\"); } Platform Support Xamarin Test Recorder supports iOS and Android. You cannot record an iOS UITest on the PC. If you want to use one platform for recording tests, use a Mac. You can run this as long as you adjust the AppInitializer.cs file as follows: if (platform == Platform.Android) { return ConfigureApp .Android .InstalledApp(\"tasklist.tasklist\") .StartApp(); } Replace the tasklist.tasklist with the package name of your app. You can retrieve this in the Properties > Android Manifest page for the TaskList.Android app. Use the Test Explorer to run the test. You will see that the clicks are performed as expected. Let's take this a little further. Let's say that rather than just clicking a few times, we wanted to ensure that the text box was filled with the text that is expected via an assertion. To uniquely identify a view, we need to add an AutomationId to the view. Adjust the TaskDetail.xaml file in the shared project as follows: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.TaskDetail\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout Padding=\"10\" Spacing=\"10\"> <Label Text=\"What should I be doing?\" /> <Entry AutomationId=\"entrytext\" Text=\"{Binding Item.Text}\" /> <Label Text=\"Completed?\" /> <Switch IsToggled=\"{Binding Item.Complete}\" /> <StackLayout VerticalOptions=\"CenterAndExpand\" /> <StackLayout Orientation=\"Vertical\" VerticalOptions=\"End\"> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"#A6E55E\" Command=\"{Binding SaveCommand}\" Text=\"Save\" TextColor=\"White\" /> <Button BackgroundColor=\"Red\" Command=\"{Binding DeleteCommand}\" Text=\"Delete\" TextColor=\"White\" /> </StackLayout> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> Note line 10. I've explicitly added an AutomationId to the entry text. I can now adjust the test to use this: [Test] public void NewTest() { app.Tap(x => x.Text(\"Login\")); app.Screenshot(\"Logged in - initial list of items\"); app.Tap(x => x.Text(\"Add New Item\")); app.Screenshot(\"Empty detail record\"); AppResult[] results = app.Query(\"entrytext\"); Assert.AreEqual(1, results.Length); Assert.AreEqual(\"New Item\", results[0].Text); app.Tap(x => x.Text(\"Save\")); app.Screenshot(\"Back at list of items\"); } Run tests on Visual Studio Mobile Center Although this process is good, it requires that you own a large number of devices and you manually test. Visual Studio Mobile Center (introduced below) has a test facility that allows you to run the same tests on hundreds of real devices and get reports of failures. Combine this with crash and analytics reporting and you will have a robust release testing mechanism for your product. You can now create a complete set of tests: Unit tests for the custom code in the mobile backend. Unit tests with a mocked backend for the mobile client. UI tests with a mocked backend for the mobile client. End to End UI tests with a test backend and known dataset. When you are in the normal development cycle, you should be doing the first three on a very regular basis throughout the day. You should perform end to end tests with the known test backend across a wide variety of devices before every release to the public app store. Distributing your Mobile Client to Beta Users \u00b6 There are a number of services that will distribute your client applications to your beta users. Since I am working primarily within Azure and the Microsoft family of products, I'm going to use Visual Studio Mobile Center . As of writing, Visual Studio Mobile Center is in preview, but already supports a large number of highly desirable features. One of these features is beta distribution. To create an application: Sign in to Visual Studio Mobile Center . You will need to create an account if you have not already done so. Click Add new -> Add new app . Give your app a name and select the OS (iOS or Android) and Platform (Xamarin). Click Add new app at the bottom of the page. Create two apps if you distribute both iOS and Android apps If you are distributing both an iOS and an Android app, you will need to create two apps - one for the iOS edition and one for the Android edition. At this point, the cloud infrastructure necessary to handle the mobile client app is created. We can move on to the distribution phase. We need to create a group and invite some beta users to our group before doing a distribution: Click the Distribute icon (it looks like three arrow pointing up if your menu is collapsed). Click New Group . Name your group something like Beta Testers . Enter a comma-delimited list of email addresses to invite users to join the group. Click Create Group . Collect and register UDID for iOS Distribution Another quirk of the iOS ecosystem. You will need to register the UDID for each iOS beta tester. Apple does not allow software to be installed on non-registered devices except via the App Store. A group called Collaborators is created and maintained by default. This contains a list of people whom you are collaborating with via Visual Studio Mobile Center. The APK or IPA file must be generated first. Open your solution in Visual Studio 2017. Select the Release , Any CPU , TaskList.Android configuration in the menu bar (for Android). Right-click the TaskList.Android project and select Rebuild . Once the build is complete, click Build -> Archive... . Once the archive is created, click Open Folder . The process is similar for iOS. You must build for a device and will only be able to distribute to registered devices. Now that we have an APK file: Return to the Visual Studio Mobile Center and select your app. Click Distribute in the left-hand sidebar. Click the group you created. Click Distribute new release . Drag and drop the APK file from the open folder to the landing area (or click upload and find your APK file). Wait for the APK file to be uploaded. A thin blue bar above the landing area is used to indicate progress. While you wait, type in some release note into the box provided. Click Next . Click Next again, as the group is already selected. Click Distribute . Your beta testers will now get an email telling them there is a new app available and how to download it.","title":"Testing your Application"},{"location":"chapter8/testing/#testing-your-mobile-application","text":"There is nothing that causes more problems than when a developer works on testing. Testing a cross-platform client-server application across all the permutations that are possible is hard work. You will spend more time on developing tests than on writing code. Much of what is asked, however, is not required. That is primarily because most people want to test the entire stack. There are generally minimal custom code in the backend, so that can significantly reduce the amount of tests you write. In this section, we will look at what it takes to do unit tests for your mobile backend and the mobile app, together with an end-to-end testing capability that allows you to test your application on many devices at once.","title":"Testing your Mobile Application"},{"location":"chapter8/testing/#testing-your-mobile-backend","text":"Most of the code within the mobile backend is pulled from libraries - ASP.NET, Entity Framework and Azure Mobile Apps. These libraries are already tested before release and there is not much you can do about bugs other than reporting them (although Azure Mobile Apps does accept fixes as well). As a result, you should concentrate your testing on the following areas: Filters, Transforms and Actions associated with your table controllers. Custom APIs. You should also do \"end-to-end\" testing. This is where you use UI testing to test both the client and the server at the same time. End to end testing is a much better test of the overall functionality of your server. In addition, your mobile backend will come under a lot of strain after you go to production. You should plan on a load test prior to each major release in a staging environment that is identical to your production environment. We'll cover this later in the book.","title":"Testing your Mobile Backend"},{"location":"chapter8/testing/#unit-testing","text":"Let's take a simple example of an app that we developed back in Chapter 3. We used the data connections to develop a personal todo store - one in which the users ID is associated with each submitted record and the user could only see their own records. The table controller looked like the following: namespace Backend.Controllers { public class TodoItemController : TableController<TodoItem> { protected override void Initialize(HttpControllerContext controllerContext) { base.Initialize(controllerContext); MobileServiceContext context = new MobileServiceContext(); DomainManager = new EntityDomainManager<TodoItem>(context, Request, enableSoftDelete: true); } public string UserId => ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value; public void ValidateOwner(string id) { var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault<TodoItem>(); if (result == null) { throw new HttpResponseException(HttpStatusCode.NotFound); } } // GET tables/TodoItem public IQueryable<TodoItem> GetAllTodoItems() { return Query().PerUserFilter(UserId); } // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public SingleResult<TodoItem> GetTodoItem(string id) { return new SingleResult<TodoItem>(Lookup(id).Queryable.PerUserFilter(UserId)); } // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task<TodoItem> PatchTodoItem(string id, Delta<TodoItem> patch) { ValidateOwner(id); return UpdateAsync(id, patch); } // POST tables/TodoItem public async Task<IHttpActionResult> PostTodoItem(TodoItem item) { item.UserId = UserId; TodoItem current = await InsertAsync(item); return CreatedAtRoute(\"Tables\", new { id = current.Id }, current); } // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959 public Task DeleteTodoItem(string id) { ValidateOwner(id); return DeleteAsync(id); } } } In addition, we have a LINQ extension method for handling the PerUserFilter : using Backend.DataObjects; using System.Linq; namespace Backend.Extensions { public static class PerUserFilterExtension { public static IQueryable<TodoItem> PerUserFilter(this IQueryable<TodoItem> query, string userid) { return query.Where(item => item.UserId.Equals(userid)); } } } In my minimalist testing suggestion, I would test the following: The LINQ Extension PerUserFilter . The UserId property. The ValidateOwner method. The other methods are straight out of the standard table controller. I would defer unit testing of these until the end-to-end tests. Unit tests should be short and should be idempotent. The test should be able to be run multiple times and always return the same result. Since our service is defined to be run out of a stateful SQL database, it cannot be defined to be idempotent. However, the individual parts we are operating can be idempotent. Unit tests are generally defined to be a separate project within the Visual Studio solution. By convention, they are named by appending .Tests to the project they are testing. My project is called Backend , so the test project is called Backend.Tests . To create the test project: Open the solution in Visual Studio. Right-click the solution, choose Add -> New Project... . Select Installed > Visual C# > Test in the project type tree. Select xUnit Test Project as the project type. Enter Backend.Tests as the name, then click OK . xUnit vs. MSTest vs. Others Most version of Visual Studio support a specific type of test called MSTest . However, Visual Studio 2017 has integrated xUnit testing as well. xUnit is cross-platform whereas MSTest is PC only. I will be using xUnit for this project. If you are using a version of Visual Studio earlier than VS2017, you will not have the xUnit Test Project available. However, you can simulate the same project type manually . In addition, there are other test frameworks available. We will only be covering xUnit here. Generally, copy the folder format from the main project to the test project. For example, the PerUserFilterExtension.cs file is in an Extensions folder within the main project. I'm going to create an Extensions folder within the test project and create a PerUserFilterExtensionTests.cs file with the tests in it. To create the tests: Right-click the Extensions folder, and select Add -> New Item... . Select Installed > Visual C# Items > Test in the project type tree. Select xUnit Test , and enter PerUserFilterExtensionTests.cs as the name. Click Add . Add your Project under Test as a Reference You will need to add your project under test (in this case, the Backend project) as a reference to the test project. You will get this code generated: using System; using System.Linq; using Xunit; namespace Backend.Tests.Extensions { public class PerUserFilterExtensionTests { [Fact] public void TestMethod1() { } } } We are going to replace the TestMethod1() method with our unit tests. XUnit tests are designated with the [Fact] attribute. You do some work on the class to test specific conditions, then assert that the results are valid. In the case of this class, for instance, we want to test that the result is correct under the following conditions: A valid string is provided. A zero-length string is provided. Null is provided. Under no conditions should the extension method throw an exception. That means three tests, coded thusly: using Backend.DataObjects; using Backend.Extensions; using System; using System.Collections.Generic; using System.Linq; using Xunit; namespace Backend.Tests.Extensions { public class PerUserFilterExtensionTests { [Fact] public void UserId_Is_Valid() { List<TodoItem> items = new List<TodoItem> { new TodoItem { UserId = \"test\", Text = \"Task 1\", Complete = false }, new TodoItem { UserId = \"test2\", Text = \"Task 2\", Complete = true }, new TodoItem { UserId = \"test\", Text = \"Task 3\", Complete = false } }; var result = items.AsQueryable<TodoItem>().PerUserFilter(\"test\"); Assert.NotNull(result); Assert.Equal(2, result.Count()); } [Fact] public void UserId_Is_Empty() { List<TodoItem> items = new List<TodoItem> { new TodoItem { UserId = \"test\", Text = \"Task 1\", Complete = false }, new TodoItem { UserId = \"test2\", Text = \"Task 2\", Complete = true }, new TodoItem { UserId = \"test\", Text = \"Task 3\", Complete = false } }; var result = items.AsQueryable<TodoItem>().PerUserFilter(String.Empty); Assert.NotNull(result); Assert.Equal(0, result.Count()); } [Fact] public void UserId_Is_Null() { List<TodoItem> items = new List<TodoItem> { new TodoItem { UserId = \"test\", Text = \"Task 1\", Complete = false }, new TodoItem { UserId = \"test2\", Text = \"Task 2\", Complete = true }, new TodoItem { UserId = \"test\", Text = \"Task 3\", Complete = false } }; var result = items.AsQueryable<TodoItem>().PerUserFilter(null); Assert.NotNull(result); Assert.Equal(0, result.Count()); } } } Use the same .NET Framework Version You will note that your tests will not compile at this point. That is because the server is dependent on .NET Framework 4.6 and the test project is created with .NET Framework 4.5. Both test and main project must be configured to use the same version of the .NET Framework. Right-click the test project, select Properties , then change the version of the .NET Framework to match your main project. Save and re-build your test project. Visual Studio has a couple of methods of running the tests. Visual Studio 2017 has in-built support for the xUnit test runner. You may have to download an extension or run them manually in earlier versions of Visual Studio. My favorite way of running the tests is to open the Test Explorer using Test -> Windows -> Test Explorer , then click Run All . You can then right-click the Test Explorer tab and select Float to float it as a window. This allows you to enlarge the window so you can see the tests after they have run: As you can see, my tests all passed. I can run these tests as many times as necessary as they do not depend on external requirements. This is not generally the case with table controllers. The table controller takes a dependency on a domain manager (most normally, the EntityDomainManager ). The EntityDomainManager is configured to use a database via a connection string. Thus, we need to do things differently for testing table controllers even if we only test the unique functionality. Let's take a look at the tests for the UserId property. The UserId property contains the contents of the NameIdentifier claim. My tests for this are: A correct set of claims are provided. An incomplete set of claims (without a NameIdentifier) are provided. No claims are provided. The first and last are the typical authenticated and anonymous access tests. The first should provide the username in the NameIdentifier, and the latter should throw an error. The middle test is an important one for us. What do you want to happen if the user is authenticated, but the NameIdentifier claim was not provided? It's bad form for us to return a 500 Internal Server Error, even though that would be appropriate here. Instead I want to assume that the user id is blank so that everything keeps on working. (One can argue that this is not correct either!) Install the same NuGet packages Unlike the scaffolded project for Azure Mobile Apps or ASP.NET MVC, no additional packages are added to the test project, which means you will need to figure out which packages you need to simulate the requirements for the test. Don't guess. Look at the packages that are in your project under test and duplicate them. Right-click the solution and select Manage NuGet Packages to get a good idea of what your test package is missing. Under the Installed list, you can tell what packages are required and which projects have them installed. Mock the ClaimsIdentity to test authentication: using System.Security.Claims; namespace Backend.Tests.Utilities { public class TestPrincipal : ClaimsPrincipal { public TestPrincipal(params Claim[] claims) : base(new TestIdentity(claims)) { } } public class TestIdentity : ClaimsIdentity { public TestIdentity(params Claim[] claims) : base(claims) { } } } My (incorrect - deliberately) test looks like the following: using Backend.Controllers; using Backend.Tests.Utilities; using System.Security.Claims; using System.Threading; using Xunit; namespace Backend.Tests.Controllers { public class TodoItemControllerTests { [Fact] public void UserId_With_Correct_Claims() { var controller = new TodoItemController(); controller.User = new TestPrincipal( new Claim(\"name\", \"testuser\"), new Claim(\"sub\", \"foo\") ); var result = controller.UserId; Assert.NotNull(result); Assert.Equal(\"testuser\", result); } [Fact] public void UserId_With_Incomplete_Claims() { var controller = new TodoItemController(); controller.User = new TestPrincipal( new Claim(\"sub\", \"foo\") ); var result = controller.UserId; Assert.Null(result); } [Fact] public void UserId_With_Null_Claims() { var controller = new TodoItemController(); controller.User = null; var ex = Assert.Throws<HttpResponseException>(() => { var result = controller.UserId; }); Assert.Equal(HttpStatusCode.Unauthorized, ex.Response.StatusCode); } } } The UserId_With_Null_Claims test is an interesting recipe for testing that the right exception is thrown. In this case, I expect the methods to return a 401 Unauthorized response to the client. Of course, the [Authorize] tag will do this for my well before my code is hit, but it's good to be accurate. If I run the tests, I get the following: What I want to do is run that test again, but attach a debugger. To do this, set a breakpoint on the property in the TodoItemController . Then right-click the failing test and select Debug Selected Tests . This runs the test with a debugger connected. The breakpoint you set will be hit and you will be able to inspect the code state while it is running. The first test is failing because ClaimTypes.NameIdentifier is not \"name\". I re-wrote the test as follows: [Fact] public void UserId_With_Correct_Claims() { var controller = new TodoItemController(); controller.User = new TestPrincipal( new Claim(ClaimTypes.NameIdentifier, \"testuser\"), new Claim(\"sub\", \"foo\") ); var result = controller.UserId; Assert.NotNull(result); Assert.Equal(\"testuser\", result); } This test will now pass. The other two tests are actually the result of incorrect code. I've adjusted the code accordingly: public string UserId { get { if (User == null) { throw new HttpResponseException(HttpStatusCode.Unauthorized); } var principal = User as ClaimsPrincipal; Claim cl = principal.Claims.FirstOrDefault(c => c.Type == ClaimTypes.NameIdentifier); return cl?.Value; } } This is a little longer than the original one-liner, but it's more accurate. This means that when I've forgotten what this particular method does in six months time, it will still do the right thing in all conditions. Use Test-Driven Development There is a whole school of thought on how to develop using testing as the driver known as Test Driven Development or TDD. In this school of thought, you write the tests first, ensuring you have 100% of the cases covered. Your code is correct when the tests pass. This method provides for very rapid development, but you do spend most of your time developing tests rather than code. The other big class of testing to do is against custom APIs. You can test these the same way. For example, the standard scaffolding for an Azure Mobile Apps server contains a ValuesController.cs , which I have modified: using System.Web.Http; using Microsoft.Azure.Mobile.Server.Config; namespace Backend.Controllers { // Use the MobileAppController attribute for each ApiController you want to use // from your mobile clients [MobileAppController] public class ValuesController : ApiController { // GET api/values public string Get(string user) { return $\"Hello {user}!\"; } } } I can test this with the following: using Backend.Controllers; using Xunit; namespace Backend.Tests.Controllers { public class ValuesControllerTests { [Fact] public void Get_Name_Works() { var controller = new ValuesController(); var result = controller.Get(\"adrian\"); Assert.Equal(\"Hello adrian!\", result); } } } As with all other testing, ensure you think about all the things that could happen here, and test them all. Ensure that the appropriate response is always returned and that you are never leave your server or a connected client in a bad state. A big example of this in the case of mobile apps: If a response is meant to be a JSON encoded version of an object on your client, ensure it can be deserialized to that object under all conditions.","title":"Unit Testing"},{"location":"chapter8/testing/#testing-your-mobile-client","text":"Testing your mobile client will generally be a multi-part affair: Implement mock data services and test the UI in isolation. Implement unit tests for the non-UI components. Do end-to-end tests to ensure both client and server work together. Unit tests for non-UI code is the same as the server-side code. You need to write the tests in a unit test framework like xUnit or MSTest . Use Test-driven development to ensure that your code is living up to its contract.","title":"Testing your Mobile Client"},{"location":"chapter8/testing/#using-mock-data-services","text":"Unfortunately, setting up repeatable unit tests becomes increasingly difficult in a client-server application such as a cloud-enabled mobile app. For these aspects, you want to mock the data services. If you have followed along from the beginning, we've actually done a lot of the hard work for this. Create an Interface that represents the interface to the data service. Create a concrete implementation of that interface. Use a dependency injection service to inject the concrete implementation. The whole idea here is that changing just one line of code will enable you to update from the mock implementation to the cloud implementation. This allows you to develop the UI independently of the backend communication code, and allows you to do repeatable UI tests later on. Let's take a look at an example. In my Chapter8 project , I've got the Xamarin Forms application from the very first chapter . In the shared TaskList project, there is an Abstractions folder that contains the definitions for ICloudService : namespace TaskList.Abstractions { public interface ICloudService { ICloudTable<T> GetTable<T>() where T : TableData; } } There is also a definition for ICloudTable : using System.Collections.Generic; using System.Threading.Tasks; namespace TaskList.Abstractions { public interface ICloudTable<T> where T : TableData { Task<T> CreateItemAsync(T item); Task<T> ReadItemAsync(string id); Task<T> UpdateItemAsync(T item); Task DeleteItemAsync(T item); Task<ICollection<T>> ReadAllItemsAsync(); } } The important part of this is this. The only place where the concrete edition, AzureCloudService() , is mentioned is in the App.cs file: using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { CloudService = new AzureCloudService(); MainPage = new NavigationPage(new Pages.EntryPage()); } } } Everywhere else uses the ICloudService interface and does not mention the concrete version. The application sets up the cloud service and every other class uses it. This allows us to set up a mock cloud service as follows. First, let's define the MockCloudService : using System.Collections.Generic; using TaskList.Abstractions; namespace TaskList.Services { public class MockCloudService : ICloudService { public Dictionary<string, object> tables = new Dictionary<string, object>(); public ICloudTable<T> GetTable<T>() where T : TableData { var tableName = typeof(T).Name; if (!tables.ContainsKey(tableName)) { var table = new MockCloudTable<T>(); tables[tableName] = table; } return (ICloudTable<T>)tables[tableName]; } } } It's very similar to the AzureCloudService class, but there is no MobileServiceClient . Instead, we store the cloud table instances in a dictionary to ensure successive calls to GetTable<>() return the same singleton reference. We aren't using the backend service. Similarly, we use a Dictionary<> to hold the items instead of the backend service in the MockCloudTable class: using Microsoft.WindowsAzure.MobileServices; using System; using System.Collections.Generic; using System.Text; using System.Threading.Tasks; using TaskList.Abstractions; #pragma warning disable CS1998 // Async method lacks 'await' operators and will run synchronously namespace TaskList.Services { public class MockCloudTable<T> : ICloudTable<T> where T : TableData { private Dictionary<string, T> items = new Dictionary<string, T>(); private int currentVersion = 1; public async Task<T> CreateItemAsync(T item) { item.Id = Guid.NewGuid().ToString(\"N\"); item.CreatedAt = DateTimeOffset.Now; item.UpdatedAt = DateTimeOffset.Now; item.Version = ToVersionString(currentVersion++); items.Add(item.Id, item); return item; } public async Task DeleteItemAsync(T item) { if (item.Id == null) { throw new NullReferenceException(); } if (items.ContainsKey(item.Id)) { items.Remove(item.Id); } else { throw new MobileServiceInvalidOperationException(\"Not Found\", null, null); } } public async Task<ICollection<T>> ReadAllItemsAsync() { List<T> allItems = new List<T>(items.Values); return allItems; } public async Task<T> ReadItemAsync(string id) { if (items.ContainsKey(id)) { return items[id]; } else { throw new MobileServiceInvalidOperationException(\"Not Found\", null, null); } } public async Task<T> UpdateItemAsync(T item) { if (item.Id == null) { throw new NullReferenceException(); } if (items.ContainsKey(item.Id)) { item.UpdatedAt = DateTimeOffset.Now; item.Version = ToVersionString(currentVersion++); items[item.Id] = item; return item; } else { throw new MobileServiceInvalidOperationException(\"Not Found\", null, null); } } private byte[] ToVersionString(int i) { byte[] b = BitConverter.GetBytes(i); string str = Convert.ToBase64String(b); return Encoding.ASCII.GetBytes(str); } } } The mock service is instantiated within the App.cs file: using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { #if USE_MOCK_SERVICES CloudService = new MockCloudService(); #else CloudService = new AzureCloudService(); #endif MainPage = new NavigationPage(new Pages.EntryPage()); } } } Finally, I need to actually define USE_MOCK_SERVICES somewhere. Right-click the project and select Properties . Click Build . Add the USE_MOCK_SERVICES to the Conditional compilation symbols (which is a semi-colon separated list). Save the properties then rebuild the project you modified. You can run this version without any backend at all. It will not persist the data, but that's the point of mock data services. Use a new Configuration Another, more advanced, way of accomplishing this is to set up a new configuration. You can see the configuration is \"Active (Debug)\". You can add another configuration to this list called \"Mock Services\" by using the Build > Configuration Manager... . When you select that configuration, the mock services will automatically be brought in.","title":"Using Mock Data Services"},{"location":"chapter8/testing/#ui-testing","text":"The mock services are a tool to enable UI unit testing. UI testing is unit testing for your UI. These are small tests that are executed on a real device and check to see if your main UI flows work as expected. There are actually a few ways of creating tests. I'm going to produce a simple test. In the test, I will simulate clicking on the entry button and ensuring that the task list page is produced. This test can then be run against one or more devices. Let's start by creating a TaskList.Tests project: Right-click the solution and select Add -> New Project... In the Installed > Visual C# > Cross-Platform node of the tree, select UI Test App (Xamarin.UITest | Cross-Platform) . Give it a snappy name, like TaskList.Tests , then click OK . Wait for the project to be created. Right-click the References node in the newly created project and select Add Reference... . Click Projects in the left hand side-bar. Click TaskList.Android . Ensure there is a checked box next to the TaskList.Android project. Click OK . We are only going to test the Android edition of the project in this walkthrough, mostly because I do most of my work on a PC. The same methodology can be used for iOS, however. Use NUnit v2.x Xamarin.UITest does not support NUnit 3.x - make sure you do not upgrade the NUnit framework beyond the latest v2.x release when updating your NuGet packages. The project contains two source files - AppInitializer.cs and Tests.cs . This latter file is where we are going to write the tests. The most efficient way of writing tests is to use the Xamarin Test Recorder . The workflow for using Xamarin Test Recorder is: If needed, export the mobile client so that it can be used on a device. Start Xamarin Test Recorder Specify the application to be tested and the device to run the aplication on. Interact with the application. The Test Recorder will create a C# test method. Incorporate the test into a Xamarin UITest project. We already have the UITest project, so let's walk through the process of creating a simple test case for the process of creating a new task. Open the Tests.cs file. Note the \"lightning\" icon next to each [TestFixture] attribute: Switch to a Release build in the configuration manager (or on the toolbar), then right-click the TaskList.Android project and select Build to build the project. Now that you have a working app, click the lightning icon next to the TestFixture for the Android platform, then select Record new test -> Build TaskList.Android project . This will start your project in the selected device - normally an emulator. It will also create a NewTest() method. In the emulator, click on the Login button, followed by Add New Item, followed by Save. When you are done, switch back to the Visual Studio instance, click on the spanner next at the bottom of the file and select Stop Recording . The following code will be generated: [Test] public void NewTest() { app.Tap(x => x.Text(\"Login\")); app.Tap(x => x.Text(\"Add New Item\")); app.Tap(x => x.Text(\"Save\")); } You can take screen shots, by adding app.Screenshot(\"description\"); in between each step: [Test] public void NewTest() { app.Tap(x => x.Text(\"Login\")); app.Screenshot(\"Logged in - initial list of items\"); app.Tap(x => x.Text(\"Add New Item\")); app.Screenshot(\"Empty detail record\"); app.Tap(x => x.Text(\"Save\")); app.Screenshot(\"Back at list of items\"); } Platform Support Xamarin Test Recorder supports iOS and Android. You cannot record an iOS UITest on the PC. If you want to use one platform for recording tests, use a Mac. You can run this as long as you adjust the AppInitializer.cs file as follows: if (platform == Platform.Android) { return ConfigureApp .Android .InstalledApp(\"tasklist.tasklist\") .StartApp(); } Replace the tasklist.tasklist with the package name of your app. You can retrieve this in the Properties > Android Manifest page for the TaskList.Android app. Use the Test Explorer to run the test. You will see that the clicks are performed as expected. Let's take this a little further. Let's say that rather than just clicking a few times, we wanted to ensure that the text box was filled with the text that is expected via an assertion. To uniquely identify a view, we need to add an AutomationId to the view. Adjust the TaskDetail.xaml file in the shared project as follows: <?xml version=\"1.0\" encoding=\"utf-8\" ?> <ContentPage x:Class=\"TaskList.Pages.TaskDetail\" xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" Title=\"{Binding Title}\"> <ContentPage.Content> <StackLayout Padding=\"10\" Spacing=\"10\"> <Label Text=\"What should I be doing?\" /> <Entry AutomationId=\"entrytext\" Text=\"{Binding Item.Text}\" /> <Label Text=\"Completed?\" /> <Switch IsToggled=\"{Binding Item.Complete}\" /> <StackLayout VerticalOptions=\"CenterAndExpand\" /> <StackLayout Orientation=\"Vertical\" VerticalOptions=\"End\"> <StackLayout HorizontalOptions=\"Center\" Orientation=\"Horizontal\"> <Button BackgroundColor=\"#A6E55E\" Command=\"{Binding SaveCommand}\" Text=\"Save\" TextColor=\"White\" /> <Button BackgroundColor=\"Red\" Command=\"{Binding DeleteCommand}\" Text=\"Delete\" TextColor=\"White\" /> </StackLayout> </StackLayout> </StackLayout> </ContentPage.Content> </ContentPage> Note line 10. I've explicitly added an AutomationId to the entry text. I can now adjust the test to use this: [Test] public void NewTest() { app.Tap(x => x.Text(\"Login\")); app.Screenshot(\"Logged in - initial list of items\"); app.Tap(x => x.Text(\"Add New Item\")); app.Screenshot(\"Empty detail record\"); AppResult[] results = app.Query(\"entrytext\"); Assert.AreEqual(1, results.Length); Assert.AreEqual(\"New Item\", results[0].Text); app.Tap(x => x.Text(\"Save\")); app.Screenshot(\"Back at list of items\"); } Run tests on Visual Studio Mobile Center Although this process is good, it requires that you own a large number of devices and you manually test. Visual Studio Mobile Center (introduced below) has a test facility that allows you to run the same tests on hundreds of real devices and get reports of failures. Combine this with crash and analytics reporting and you will have a robust release testing mechanism for your product. You can now create a complete set of tests: Unit tests for the custom code in the mobile backend. Unit tests with a mocked backend for the mobile client. UI tests with a mocked backend for the mobile client. End to End UI tests with a test backend and known dataset. When you are in the normal development cycle, you should be doing the first three on a very regular basis throughout the day. You should perform end to end tests with the known test backend across a wide variety of devices before every release to the public app store.","title":"UI Testing"},{"location":"chapter8/testing/#distributing-your-mobile-client-to-beta-users","text":"There are a number of services that will distribute your client applications to your beta users. Since I am working primarily within Azure and the Microsoft family of products, I'm going to use Visual Studio Mobile Center . As of writing, Visual Studio Mobile Center is in preview, but already supports a large number of highly desirable features. One of these features is beta distribution. To create an application: Sign in to Visual Studio Mobile Center . You will need to create an account if you have not already done so. Click Add new -> Add new app . Give your app a name and select the OS (iOS or Android) and Platform (Xamarin). Click Add new app at the bottom of the page. Create two apps if you distribute both iOS and Android apps If you are distributing both an iOS and an Android app, you will need to create two apps - one for the iOS edition and one for the Android edition. At this point, the cloud infrastructure necessary to handle the mobile client app is created. We can move on to the distribution phase. We need to create a group and invite some beta users to our group before doing a distribution: Click the Distribute icon (it looks like three arrow pointing up if your menu is collapsed). Click New Group . Name your group something like Beta Testers . Enter a comma-delimited list of email addresses to invite users to join the group. Click Create Group . Collect and register UDID for iOS Distribution Another quirk of the iOS ecosystem. You will need to register the UDID for each iOS beta tester. Apple does not allow software to be installed on non-registered devices except via the App Store. A group called Collaborators is created and maintained by default. This contains a list of people whom you are collaborating with via Visual Studio Mobile Center. The APK or IPA file must be generated first. Open your solution in Visual Studio 2017. Select the Release , Any CPU , TaskList.Android configuration in the menu bar (for Android). Right-click the TaskList.Android project and select Rebuild . Once the build is complete, click Build -> Archive... . Once the archive is created, click Open Folder . The process is similar for iOS. You must build for a device and will only be able to distribute to registered devices. Now that we have an APK file: Return to the Visual Studio Mobile Center and select your app. Click Distribute in the left-hand sidebar. Click the group you created. Click Distribute new release . Drag and drop the APK file from the open folder to the landing area (or click upload and find your APK file). Wait for the APK file to be uploaded. A thin blue bar above the landing area is used to indicate progress. While you wait, type in some release note into the box provided. Click Next . Click Next again, as the group is already selected. Click Distribute . Your beta testers will now get an email telling them there is a new app available and how to download it.","title":"Distributing your Mobile Client to Beta Users"},{"location":"chapter9/appsvc/","text":"Safe Deployments \u00b6 At some point, you will have a production app that needs updating. Hopefully, you will have many thousands of users by that point and they are clamouring for an update. If this is you, congratulations. You have a successful mobile app. Now, how do you update your app without your users noticing? A big part of the success of any update process is to ensure your Entity Framework code is appropriately updating the database and that the database can handle both the old schema and new schema. That means being able to cope with optional fields, and using default values. It also means ensuring that you are using automatic code-first migrations, which I covered way back in Chapter 3 . In this section, we will cover two topics: How do I upgrade my mobile backend without my users noticing? How do I scale my site to elastically manage load? Fortunately, Azure App Service has built-in features for both these questions. Using Slots \u00b6 Let's start with upgrading the mobile backend. The feature that we are going to use is Deployment Slots . A slot is a second (or third or fourth) copy of your mobile backend that shares all (or most) of the settings of the original, but runs different code. Slots is available in Standard and above App Service Plans. A Standard App Service has 5 slots and a Premium App Service has 20 slots. If your site is not running on a Standard service, use Scale Up to change the App Service Plan to a Standard or better plan. Check your App Service Plan Each App Service Plan has a number of slots. This is the number of web sites or mobile backends you can run on the same App Service Plan. Remember an App Service Plan is a set of virtual machines that run the same set of web sites. The slot is also a web site. Let's say you want to deploy your site with zero downtime. Establish a slot called \"staging\": Open your App Service in the Azure portal . Click Deployment slots . Click Add Slot . Enter staging in the Name field. Click OK . This is a one-time activity. If the main site is called mywebsite.azurewebsites.net , then the new slot will be called mywebsite-staging.azurewebsites.net . We can deploy the server to the new slot in almost exactly the same way as before. Instead of selecting your website, expand the website, select Deployment Slots then the name of the slot that was created: ![Select the slot][img] Update your client to use -staging You can produce a test version of your mobile client that uses the staging slot as the URL. This allows you to do ad-hoc testing prior to doing the update to production. Once the mobile backend is deployed to the staging slot, you can do any testing required before deploying to production. The next step is to turn staging into production: Open your App Service in the Azure portal . Click Deployment slots . Click Swap . Select the Source and Destination slots to swap. Click OK . At this point, the internal routing will swap the two services. Existing requests that are already being handled by the old server will be completed on the old server, but new requests will be handled by the new server. Once the swap is complete, there will be no more requests being dealt with on the old server. If your test client (configured to use the -staging URL) is run, it will be using the old server. The old server is in the -staging slot now. Best Practices: 3 Slots \u00b6 I recommend using three slots: Production Staging LastKnownGood When you swap the slot, do two swaps: First, deploy your new server to Staging . Then, swap Production with Staging . Your new server is now in production. Finally, swap Staging with LastKnownGood . This saves the (working) production server. If your new server runs into problems and you have to switch back to the old server, it is stored away in the LastKnownGood slot. You can restore the old (working) server by swapping LastKnownGood with Production . Best Practices: Continuous Deployment \u00b6 I also recommend linking a Continuous Deployment stream to the staging slot, rather than deploying via Visual Studio. Continuous Deployment automatically deploys code from a source code repository like GitHub or Visual Studio Team Services. To link staging to a GitHub repository: Open your App Service in the Azure portal . Click Deployment slots . Click the staging slot name. Click Deployment options . Click Choose Source , then select GitHub. If necessary, click Authorization to log in to GitHub. If necessary, click Choose your organization to select a GitHub organization. Click Choose project and select your GitHub repository. Click Choose branch and select the branch you wish to deploy. Click OK . The service will download the latest source code from the specified branch, build the server and deploy it. You can check the status of the deployment through Deployment options on the staging slot as well. As a best practice, create a new branch (I call my branch \"azure\") and deploy that branch. When you wish to update the deployed server, merge from the main branch to the azure branch and then push to the remote repository. This will then be picked up by your App Service and the new service will be deployed. By combining continuous deployment with slots, you can have control over the deployment to staging and control the production deployment easily. This is a powerful \"DevOps\" combination of features. Scaling your Site \u00b6 I hope that your mobile app is wildly successful. If it is, one copy of the server will not be enough to handle the connections. You will need to scale both the mobile backend and the database. How will you know when to scale? For the App Service, keep an eye on the Metrics per Instance (App Service plan) . If your production service is getting close to 80% CPU or close to memory limits, it's probably time to scale the site. For the database, check the Query Performance Insight . Your database is limited to a certain number of DTUs (database transaction units - a blend of CPU and I/O operations). In a production app, you want to scale your mobile backend according to demand. This means that the service spins up additional copies of the App Service as needed. To set up automatic scaling for an App Service: Open your App Service in the Azure portal . Click **Scale out (App Service plan). Select CPU Percentage in the Scale by drop-down. Select a minimum and maximum number of instances (for example, 1-10 for a standard App Service Plan). Select a target range for CPU (for example, 50-75%) Optionally, fill in the Notifications section. Click Save . The App Service Plan will try and keep an appropriate number of instances such that the CPU is kept in the specified range. If the average CPU drops below the threshold, the App Service Plan will take one of the instances out of server. No new connections will be sent to the instance, the existing requests will be allowed to complete. Once all connections are complete, the instance will be turned off. You only pay for the instances that are actually running. You can choose to send an email when a scale event happens. You can also configure a Webhook so that your operations group is notified more broadly. For example, you could use this facility to run an Azure Function that sends a notification to a Slack channel when an instance is added, or when more the last instance is added (indicating extreme load where your users are going to be impacted). Best Practices for Production Apps \u00b6 We've mentioned a few best practices here, so here is a wrap-up: Run production apps on Standard or Premium App Services. Set up automatic scaling based on CPU percentage. Set up alerting so that you are informed when the last instance is added. Use three slots (production, staging and lastknowngood) to ensure no downtime. Use continuous deployment to the staging slot rather than deploy from Visual Studio. Follow these best practices and you will ensure your users are not limited by mobile backend deployments or load.","title":"Safe Deployments"},{"location":"chapter9/appsvc/#safe-deployments","text":"At some point, you will have a production app that needs updating. Hopefully, you will have many thousands of users by that point and they are clamouring for an update. If this is you, congratulations. You have a successful mobile app. Now, how do you update your app without your users noticing? A big part of the success of any update process is to ensure your Entity Framework code is appropriately updating the database and that the database can handle both the old schema and new schema. That means being able to cope with optional fields, and using default values. It also means ensuring that you are using automatic code-first migrations, which I covered way back in Chapter 3 . In this section, we will cover two topics: How do I upgrade my mobile backend without my users noticing? How do I scale my site to elastically manage load? Fortunately, Azure App Service has built-in features for both these questions.","title":"Safe Deployments"},{"location":"chapter9/appsvc/#using-slots","text":"Let's start with upgrading the mobile backend. The feature that we are going to use is Deployment Slots . A slot is a second (or third or fourth) copy of your mobile backend that shares all (or most) of the settings of the original, but runs different code. Slots is available in Standard and above App Service Plans. A Standard App Service has 5 slots and a Premium App Service has 20 slots. If your site is not running on a Standard service, use Scale Up to change the App Service Plan to a Standard or better plan. Check your App Service Plan Each App Service Plan has a number of slots. This is the number of web sites or mobile backends you can run on the same App Service Plan. Remember an App Service Plan is a set of virtual machines that run the same set of web sites. The slot is also a web site. Let's say you want to deploy your site with zero downtime. Establish a slot called \"staging\": Open your App Service in the Azure portal . Click Deployment slots . Click Add Slot . Enter staging in the Name field. Click OK . This is a one-time activity. If the main site is called mywebsite.azurewebsites.net , then the new slot will be called mywebsite-staging.azurewebsites.net . We can deploy the server to the new slot in almost exactly the same way as before. Instead of selecting your website, expand the website, select Deployment Slots then the name of the slot that was created: ![Select the slot][img] Update your client to use -staging You can produce a test version of your mobile client that uses the staging slot as the URL. This allows you to do ad-hoc testing prior to doing the update to production. Once the mobile backend is deployed to the staging slot, you can do any testing required before deploying to production. The next step is to turn staging into production: Open your App Service in the Azure portal . Click Deployment slots . Click Swap . Select the Source and Destination slots to swap. Click OK . At this point, the internal routing will swap the two services. Existing requests that are already being handled by the old server will be completed on the old server, but new requests will be handled by the new server. Once the swap is complete, there will be no more requests being dealt with on the old server. If your test client (configured to use the -staging URL) is run, it will be using the old server. The old server is in the -staging slot now.","title":"Using Slots"},{"location":"chapter9/appsvc/#best-practices-3-slots","text":"I recommend using three slots: Production Staging LastKnownGood When you swap the slot, do two swaps: First, deploy your new server to Staging . Then, swap Production with Staging . Your new server is now in production. Finally, swap Staging with LastKnownGood . This saves the (working) production server. If your new server runs into problems and you have to switch back to the old server, it is stored away in the LastKnownGood slot. You can restore the old (working) server by swapping LastKnownGood with Production .","title":"Best Practices: 3 Slots"},{"location":"chapter9/appsvc/#best-practices-continuous-deployment","text":"I also recommend linking a Continuous Deployment stream to the staging slot, rather than deploying via Visual Studio. Continuous Deployment automatically deploys code from a source code repository like GitHub or Visual Studio Team Services. To link staging to a GitHub repository: Open your App Service in the Azure portal . Click Deployment slots . Click the staging slot name. Click Deployment options . Click Choose Source , then select GitHub. If necessary, click Authorization to log in to GitHub. If necessary, click Choose your organization to select a GitHub organization. Click Choose project and select your GitHub repository. Click Choose branch and select the branch you wish to deploy. Click OK . The service will download the latest source code from the specified branch, build the server and deploy it. You can check the status of the deployment through Deployment options on the staging slot as well. As a best practice, create a new branch (I call my branch \"azure\") and deploy that branch. When you wish to update the deployed server, merge from the main branch to the azure branch and then push to the remote repository. This will then be picked up by your App Service and the new service will be deployed. By combining continuous deployment with slots, you can have control over the deployment to staging and control the production deployment easily. This is a powerful \"DevOps\" combination of features.","title":"Best Practices: Continuous Deployment"},{"location":"chapter9/appsvc/#scaling-your-site","text":"I hope that your mobile app is wildly successful. If it is, one copy of the server will not be enough to handle the connections. You will need to scale both the mobile backend and the database. How will you know when to scale? For the App Service, keep an eye on the Metrics per Instance (App Service plan) . If your production service is getting close to 80% CPU or close to memory limits, it's probably time to scale the site. For the database, check the Query Performance Insight . Your database is limited to a certain number of DTUs (database transaction units - a blend of CPU and I/O operations). In a production app, you want to scale your mobile backend according to demand. This means that the service spins up additional copies of the App Service as needed. To set up automatic scaling for an App Service: Open your App Service in the Azure portal . Click **Scale out (App Service plan). Select CPU Percentage in the Scale by drop-down. Select a minimum and maximum number of instances (for example, 1-10 for a standard App Service Plan). Select a target range for CPU (for example, 50-75%) Optionally, fill in the Notifications section. Click Save . The App Service Plan will try and keep an appropriate number of instances such that the CPU is kept in the specified range. If the average CPU drops below the threshold, the App Service Plan will take one of the instances out of server. No new connections will be sent to the instance, the existing requests will be allowed to complete. Once all connections are complete, the instance will be turned off. You only pay for the instances that are actually running. You can choose to send an email when a scale event happens. You can also configure a Webhook so that your operations group is notified more broadly. For example, you could use this facility to run an Azure Function that sends a notification to a Slack channel when an instance is added, or when more the last instance is added (indicating extreme load where your users are going to be impacted).","title":"Scaling your Site"},{"location":"chapter9/appsvc/#best-practices-for-production-apps","text":"We've mentioned a few best practices here, so here is a wrap-up: Run production apps on Standard or Premium App Services. Set up automatic scaling based on CPU percentage. Set up alerting so that you are informed when the last instance is added. Use three slots (production, staging and lastknowngood) to ensure no downtime. Use continuous deployment to the staging slot rather than deploy from Visual Studio. Follow these best practices and you will ensure your users are not limited by mobile backend deployments or load.","title":"Best Practices for Production Apps"},{"location":"chapter9/arm/","text":"Repeatable Deployments \u00b6 You've finally got to the point where you want to release your app to the general public. Congratulations! I'm not going to cover releasing to the App Store in this section. Each App Store has their own rules and requirements for apps and they change all the time. You will need to follow their instructions. If you haven't already, make sure you have a paid developer account for this activity. That doesn't mean that you can't deal with the backend. Now that your app is in production, you want your backend in production as well. That means being as hands-off as possible and ensuring that human error does not cause a significant downtime. We've heard about various outages as a result of human error. Make sure that doesn't happen to you. Fortunately, Azure has a number of facilities to ensure that you can automate the process. Options for Deployment \u00b6 When deploying the backend, you want a repeatable process. Ideally, you want a set of scripts that you check into source code that describes the deployment of Azure resources. Once you have set up your environment, you can use continuous deployment (in the next section) to actually perform the deployments. However, you still want to automate the deployment of a new set of resources. Fortunately, Azure (in common with any reasonable cloud service) has tools to do this. In essence, the tools consist of a file - the Azure Resource Manager Template, and a set of PowerShell or command-line tools to deploy a new service. Every time you create a mobile app in Azure App Service, Azure Resource Manager handles the actual provisioning behind the scenes. It is much faster to use an ARM template rather than the point-and-click mechanism we have been using thus far. Creating an ARM Template \u00b6 Let's start by examining a typical application - our Task List backend. We've set this up a number of times within this book, so you should be familiar with it. It has these resources within a resource group: Azure App Service with type=mobile Azure App Service Plan SQL Azure Logical Server SQL Azure Database ARM templates have three distinct sections: Parameters provide information about the deployment set in a single place. Variables are computed parameters. Resources list the resources necessary to be deployed. A typical (empty) ARM template looks like this: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { }, \"variables\": { }, \"resources\": [ ] } There is also an outputs section, but I find most templates do not require this section. Parameters and Variables \u00b6 If I am creating a deployment called zumobook , then I always name the resources like this: Resource Group = zumobook Azure App Service = zumobook App Service Plan = zumobook-plan SQL Azure Logical Server = zumobook-sql SQL Azure Database = zumobook-db All the resources are based on the root name of the deployment. Ideally, the deployment name would be a parameter (something I specify) and the other names would be computed. In addition, I always make the administrator login for the SQL Azure Logical Server be azure and I specify the password. Let's take a look at what that would look like: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"sqlServerAdminLogin\": { \"type\": \"string\", \"defaultValue\": \"azure\" }, \"sqlServerAdminPassword\": { \"type\": \"securestring\" } }, \"variables\": { \"appServiceName\": \"parameters('serviceName')\", \"appServicePlanName\": \"[concat(parameters('serviceName), '-plan')]\", \"sqlServerName\": \"[concat(parameters('serviceName), '-sql')]\", \"sqlDbName\": \"[concat(parameters('serviceName), '-db')]\" }, \"resources\": [ ] } Variable Functions Note the concat() is a template function. You can find the large list of template functions in the Azure Resource Manager documentation . As a best practice, I abstract anything I would normally enter in creating the resource and make it either a parameter or a variable, depending on the circumstance. I try to reduce the number of parameters and make as much as I can either a default value or a variable. For example, one of the things that we need to set when configuring a SQL Azure database is the database size. I could just specify this in bytes, but specifying the size in Mb is a better experience: \"parameters\": { \"sqlDbSizeMb\" { \"type\": \"int\", \"minValue\": 1, \"maxValue\": 20, \"description\": \"Size of the database in Mb\", \"defaultValue\": 5 } }, \"variables\": { \"sqlDbSize\": \"[mul(parameters('sqlDbSizeMb'), 1048576)]\" } Resources \u00b6 Resources describe the deployment of individual resources and their dependency. There are, inevitably, more resources than you expect. Each resource has an apiVersion , formatted as a date, that describes the format of the resource. Each resource also has a name and a type plus a number of other elements that describe configuration elements. Finally, there is an optional dependsOn that ensures that resources are not configured until their dependent resources are deployed. If, for example, I look at my deployment, I can represent this as a tree: Resource Group App Service Plan App Service App Settings Connection Strings Continuous Deployment Source SQL Azure Logical Server SQL Database SQL Server Firewall Rules Let's look at each resource in turn. The SQL Database \u00b6 There are actually three resources for the SQL Database - the server, the database and the firewall rule to allow other resources to connect to it: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"location\": { \"type\": \"string\", \"defaultValue\": \"South Central US\" }, \"sqlServerAdminLogin\": { \"type\": \"string\", \"defaultValue\": \"azure\" }, \"sqlServerAdminPassword\": { \"type\": \"securestring\" }, \"sqlDbSizeMb\" { \"type\": \"int\", \"minValue\": 1, \"maxValue\": 20, \"description\": \"Size of the database in Mb\", \"defaultValue\": 2 }, \"sqlDbCollation\": { \"type\": \"string\", \"defaultValue\": \"SQL_Latin1_General_CP1_CI_AS\" }, \"sqlDbEdition\": { \"type\": \"string\", \"defaultValue\": \"Basic\" }, \"sqlDbServiceObjectiveId\": { \"type\": \"string\", \"defaultValue\": \"dd6d99bb-f193-4ec1-86f2-43d3bccbc49c\" }, }, \"variables\": { \"sqlDbSize\": \"[mul(parameters('sqlDbSizeMb'), 1048576)]\", \"sqlServerName\": \"[concat(parameters('serviceName), '-sql')]\", \"sqlDbName\": \"[concat(parameters('serviceName), '-db')]\" }, \"resources\": [ { \"apiVersion\": \"2014-04-01-preview\", \"name\": \"[variables('sqlServerName')]\", \"type\": \"Microsoft.Sql/servers\", \"location\": \"[parameters('location')]\", \"properties\": { \"administratorLogin\": \"[parameters('sqlServerAdminLogin')]\", \"administratorPassword\": \"[parameters('sqlServerAdminPassword')]\" }, \"resources\": [ { \"apiVersion\": \"2015-05-01-preview\", \"name\": \"[variables('sqlDbName')]\", \"type\": \"databases\", \"location\": \"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\" ], \"properties\": { \"edition\": \"[parameters('sqlDbEdition')]\", \"collation\": \"[parameters('sqlDbCollation')]\", \"maxSizeBytes\": \"[variables('sqlDbSize')]\", \"requestedServiceObjectiveId\": \"[parameters('sqlDbServiceObjectiveId')]\" } }, { \"apiVersion\": \"2014-04-01-preview\", \"name\": \"SQLServerFirewallRules\", \"type\": \"firewallrules\", \"location\": \"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\" ], \"properties\": { \"endIpAddress\": \"0.0.0.0\", \"startIpAddress\": \"0.0.0.0\" } } ] }, ] } This is a fairly lengthy example, but it incorporates a few things. Firstly, note that I use the [variables()] and [parameters()] to bring in the variables and parameters respectively. I also have embedded resources here. The resource heirarchy is a tree-structure. Each embedded resource has a dependsOn array that has the parent as a dependent resource. You are probably wondering where you can get this information for any resource. Microsoft Azure has a site called resources.azure.com . This contains the resources that you currently have configured on your subscription. You can set the deployment up by hand and then look at the resources to get the details: In this example, I'm showing the resource for the database. I can extract the various pieces based on what I have set to get to this point - everything else is defaulted. In addition, note that I can see the apiVersion at the top of the resource manager within the URL for the resource definition. The Azure App Service Plan \u00b6 My next resource is the App Service Plan. Unlike the database (which has a definite heirarchy), the App Service Plan is separate because many App Services can use the same App Service Plan. You can think of the App Service Plan as the set of virtual machines that belong in a single scale set, and the App Services as the web sites running on top of that scale set. The App Service Plan looks like this: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"location\": { \"type\": \"string\", \"defaultValue\": \"South Central US\" }, \"sku\": { \"type\": \"string\", \"allowedValues\": [ \"Free\", \"Shared\", \"Basic\", \"Standard\" ], \"defaultValue\": \"Basic\" }, \"workerSize\": { \"type\": \"string\", \"allowedValues\": [ \"0\", \"1\", \"2\" ], \"defaultValue\": \"0\" } }, \"variables\": { \"appServicePlanName\": \"[concat(parameters('serviceName), '-plan')]\", }, \"resources\": [ { \"apiVersion\": \"2015-08-01\", \"name\": \"[variables('appServicePlanName')]\", \"type\": \"Microsoft.Web/serverFarms\", \"location\": \"[parameters('location')]\", \"properties\": { \"name\": \"[parameters('appServicePlanName')]\", \"sku\": \"[parameters('sku')]\", \"workerSize\": \"[parameters('workerSize')]\", \"numberOfWorkers\": 1 } } ] } The hostingPlanName is the name of the hosting plan. This is normally something like F1, B2, or P3. The first letter indicates the SKU and the number indicates the size of the worker. The size of the worker is always one less than the number that is displayed within the Azure portal. The Azure App Service \u00b6 The Azure App Service is { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"location\": { \"type\": \"string\", \"defaultValue\": \"South Central US\" }, \"sqlServerAdminLogin\": { \"type\": \"string\", \"defaultValue\": \"azure\" }, \"sqlServerAdminPassword\": { \"type\": \"securestring\" }, \"deploymentRepoUrl\": { \"type\": \"string\" }, \"deploymentRepoBranch\": { \"type\": \"string\", \"defaultValue\": \"main\" } }, \"variables\": { \"appServiceName\": \"parameters('serviceName')\", \"appServicePlanName\": \"[concat(parameters('serviceName), '-plan')]\", \"sqlServerName\": \"[concat(parameters('serviceName), '-sql')]\", \"sqlDbName\": \"[concat(parameters('serviceName), '-db')]\" }, \"resources\": [ { \"apiVersion\": \"2015-04-01\", \"name\": \"[variables('appServiceName')]\", \"type\": \"Microsoft.Web/sites\", \"location\": \"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/serverFarms', parameters('appServicePlanName'))]\" ], \"properties\": { \"serverFarmId\": \"[parameters('appServicePlanName')]\" }, \"resources\": [ { \"apiVersion\": \"2015-04-01\", \"name\": \"connectionstrings\", \"type\": \"config\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/Sites', variables('appServiceName'))]\", \"[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\" ], \"properties\": { \"MS_TableConnectionString\": { \"value\": \"[concat('Data Source=tcp:', reference(concat('Microsoft.Sql/servers/', variables('sqlServerName'))).fullyQualifiedDomainName, ',1433;Initial Catalog=', variables('sqlDbName'), ';User Id=', parameters('sqlServerAdminLogin'), '@', parameters('sqlServerName'), ';Password=', parameters('sqlServerAdminPassword'), ';')]\", \"type\": \"SQLAzure\" } } }, { \"apiVersion\": \"2015-04-01\", \"name\": \"web\", \"type\": \"sourcecontrols\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/Sites', variables('appServiceName'))]\", \"[resourceId('Microsoft.Web/Sites/config', variables('appServiceName'), 'connectionstrings')]\" ], \"properties\": { \"RepoUrl\": \"[parameters('deploymentRepoUrl')]\", \"branch\": \"[parameters('deploymentRepoBranch')]\" } } ] }, ] } In this case, we've got two dependent resources - one to set up the connection string to the database and the other to set up continuous deployment. Both of those dependent resources have the normal dependsOn terms to ensure that the resources linked exist. You can combine all three sections - the SQL Server, App Service Plan and App Service together. There are overlapping parameters and variables. You just need one copy. Deploying an ARM Template on Windows \u00b6 Creating an Azure Resource Manager template on Windows can be done several ways, but my favorite is to use Visual Studio. Start up Visual Studio 2017 and open your solution. Right-click your solution and select Add -> New Project... . Select Installed > Visual C# > Cloud > Azure Resource Group . Give the project a name, then click OK Select a template (I use Web app + SQL normally as a starting point), then click OK . Three files are created within the project: Deploy-AzureResourceGroup.ps1 WebSiteSQLDatabase.json (or similar) WebSiteSQLDatbase.parameters.json (or similar) The WebSiteSQLDatabase.json is the ARM template. The .parameters.json file contains the parameters for an unattended deployment. Since the parameters contains the SQL server password, we need to be careful to ensure that the database password does not end up checked into source code. Finally, the Deploy-AzureResourceGroup.ps1 file is a handy PowerShell script for deploying to Azure. Running it will be the equivalent of doing a deployment via Visual Studio. You can now edit the ARM template to include any additional resources you need. You can also edit the .parameters.json file to set standard values for any parameters you wish, if they don't change very often, although you won't need to do this as there is an easier way of setting parameters as we shall see. There are two steps to deploying this ARM template: Validation and Deployment. To validate the template, right-click the project and select Validate . Click into the Resource group and select Create new . This allows you to select a resource group name and a location. Once you have done that, click Create . The next step is to click Edit Parameters... . This edit the .parameters.json file: Note that we need to set a hostingPlanName , as indicated by the red exclamation error mark. We can also set any other values we want. Anything that has a null value should be entered. Click Save when complete. Do not save password as plain text It's likely that these files will end up in source control. You should never save passwords as plain text in any file that is going to end up in a source control repository. Finally, click Validate . You may get validation errors - fix the validation errors before continuing. Once you have validated successfully, right-click the deployment project and select Deploy , then select the resource group you entered during validation. You may have to type the SQL administrator password multiple times. Watch the output window for a progress report on your deployment. Once complete, log into the Azure portal to check the resources. Also check that the resources match what you were expecting within the Azure Resource Explorer . Deploying an ARM Template on MacOS \u00b6 The Mac does not come with the tools necessary to deploy ARM templates, so you will need to download and install them first. In a Terminal window: adrian$ curl -L https://aka.ms/InstallAzureCli | bash adrian$ exec -l $SHELL The first command does the actual installation. It will ask you questions about where to install and confirm that it can write to files. In general, pressing Enter will do the right thing. The second command restarts your shell. After the installation is complete, run az in your Terminal window to ensure the installation was successful. The next step is to login to Azure with the CLI. Use the az login command from the Terminal window. This command will output a code to use in the next step. Note the code that is produced. Now go to http://aka.ms/devicelogin and enter the code in the box provided. Enter the code, press enter, then click Continue . You will be asked to authenticate to Azure. Once that is complete, the terminal window will continue and you will be logged into Azure. The az login command will provide a little piece of JSON to show you the subscription information to which you connected. Deploying a set of resources is a two-step process on the command-line. Firstly, you need to create a resource group. Then you need to deploy the resources within that resource group. To create a resource group, use: az group create -l westus -n MyResourceGroup You can get a list of locations that are available to you using: az account list-locations | grep name Take a look at the response from the az group create command. Ensure that the provisioningState property is something that indicates success. Now that we have a resource group, we can deploy our resources. Edit the .parameters.json file to contain the values for your parameters. For example: { \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"hostingPlanName\": { \"value\": \"Chapter8-plan\" }, \"skuName\": { \"value\": \"B1\" }, \"skuCapacity\": { \"value\": 1 }, \"administratorLogin\": { \"value\": \"azure\" }, \"administratorLoginPassword\": { \"value\": \"myPassword\" }, \"databaseName\": { \"value\": \"ZUMOAPPDB\" }, \"edition\": { \"value\": \"Basic\" } } } Do not check in passwords! If your parameters file contains password, do not check it in to source code control. You can now deploy the service with the following: az group deployment create --resource-group MyResourceGroup --mode Complete --parameters @WebSiteSQLDatabase.parameters.json --template-file WebSiteSQLDatabase.json --verbose Although I have split this command over multiple lines, you should enter this command on one line. Use --mode to maintain deployments There are two modes for ARM deployments. Complete will remove any resources that are not listed in your ARM template. Incremental will only add resources to the resource group. You can use these two modes to effectively maintain the resources in the resource group using only the command line. At this point, the deployment will kick off. It's a long running process, so expect a delay of a few minutes before it returns information. This deployment is (deliberately) going to fail so we can see what happens when it goes wrong and how to fix it. The response after this first run is this: The deployment operation failed, because it was unable to cleanup. Please see https://aka.ms/arm-debug for usage details. Correlation ID: aed1850d-22ca-4e28-b7fa-83a2ac587867 Start by logging into the Azure portal , then go to the resource group was just created, then click Deployments . Select the latest deployment. Click on the failed request: The failed request is a lot clearer than the original failure message. In this case, the password we entered for the SQL database did not meet the complexity requirements. This is easily updated within the parameters file. Then re-run the deployment. Creating the ARM Template \u00b6 One of the issues with ARM templates is their actual creation. There are three methods of creating ARM templates: Create a new Resource Group within Visual Studio (covered above) Export a template from a Resource Group. Log in to the Azure portal . Select your resource group. In the menu, click Automation Scripts . Click Download . Download a starter template from GitHub . Which ever mechanism you choose to create the template, you will still need to edit the result by hand for your individual requirements.","title":"Repeatable Deployments"},{"location":"chapter9/arm/#repeatable-deployments","text":"You've finally got to the point where you want to release your app to the general public. Congratulations! I'm not going to cover releasing to the App Store in this section. Each App Store has their own rules and requirements for apps and they change all the time. You will need to follow their instructions. If you haven't already, make sure you have a paid developer account for this activity. That doesn't mean that you can't deal with the backend. Now that your app is in production, you want your backend in production as well. That means being as hands-off as possible and ensuring that human error does not cause a significant downtime. We've heard about various outages as a result of human error. Make sure that doesn't happen to you. Fortunately, Azure has a number of facilities to ensure that you can automate the process.","title":"Repeatable Deployments"},{"location":"chapter9/arm/#options-for-deployment","text":"When deploying the backend, you want a repeatable process. Ideally, you want a set of scripts that you check into source code that describes the deployment of Azure resources. Once you have set up your environment, you can use continuous deployment (in the next section) to actually perform the deployments. However, you still want to automate the deployment of a new set of resources. Fortunately, Azure (in common with any reasonable cloud service) has tools to do this. In essence, the tools consist of a file - the Azure Resource Manager Template, and a set of PowerShell or command-line tools to deploy a new service. Every time you create a mobile app in Azure App Service, Azure Resource Manager handles the actual provisioning behind the scenes. It is much faster to use an ARM template rather than the point-and-click mechanism we have been using thus far.","title":"Options for Deployment"},{"location":"chapter9/arm/#creating-an-arm-template","text":"Let's start by examining a typical application - our Task List backend. We've set this up a number of times within this book, so you should be familiar with it. It has these resources within a resource group: Azure App Service with type=mobile Azure App Service Plan SQL Azure Logical Server SQL Azure Database ARM templates have three distinct sections: Parameters provide information about the deployment set in a single place. Variables are computed parameters. Resources list the resources necessary to be deployed. A typical (empty) ARM template looks like this: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { }, \"variables\": { }, \"resources\": [ ] } There is also an outputs section, but I find most templates do not require this section.","title":"Creating an ARM Template"},{"location":"chapter9/arm/#parameters-and-variables","text":"If I am creating a deployment called zumobook , then I always name the resources like this: Resource Group = zumobook Azure App Service = zumobook App Service Plan = zumobook-plan SQL Azure Logical Server = zumobook-sql SQL Azure Database = zumobook-db All the resources are based on the root name of the deployment. Ideally, the deployment name would be a parameter (something I specify) and the other names would be computed. In addition, I always make the administrator login for the SQL Azure Logical Server be azure and I specify the password. Let's take a look at what that would look like: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"sqlServerAdminLogin\": { \"type\": \"string\", \"defaultValue\": \"azure\" }, \"sqlServerAdminPassword\": { \"type\": \"securestring\" } }, \"variables\": { \"appServiceName\": \"parameters('serviceName')\", \"appServicePlanName\": \"[concat(parameters('serviceName), '-plan')]\", \"sqlServerName\": \"[concat(parameters('serviceName), '-sql')]\", \"sqlDbName\": \"[concat(parameters('serviceName), '-db')]\" }, \"resources\": [ ] } Variable Functions Note the concat() is a template function. You can find the large list of template functions in the Azure Resource Manager documentation . As a best practice, I abstract anything I would normally enter in creating the resource and make it either a parameter or a variable, depending on the circumstance. I try to reduce the number of parameters and make as much as I can either a default value or a variable. For example, one of the things that we need to set when configuring a SQL Azure database is the database size. I could just specify this in bytes, but specifying the size in Mb is a better experience: \"parameters\": { \"sqlDbSizeMb\" { \"type\": \"int\", \"minValue\": 1, \"maxValue\": 20, \"description\": \"Size of the database in Mb\", \"defaultValue\": 5 } }, \"variables\": { \"sqlDbSize\": \"[mul(parameters('sqlDbSizeMb'), 1048576)]\" }","title":"Parameters and Variables"},{"location":"chapter9/arm/#resources","text":"Resources describe the deployment of individual resources and their dependency. There are, inevitably, more resources than you expect. Each resource has an apiVersion , formatted as a date, that describes the format of the resource. Each resource also has a name and a type plus a number of other elements that describe configuration elements. Finally, there is an optional dependsOn that ensures that resources are not configured until their dependent resources are deployed. If, for example, I look at my deployment, I can represent this as a tree: Resource Group App Service Plan App Service App Settings Connection Strings Continuous Deployment Source SQL Azure Logical Server SQL Database SQL Server Firewall Rules Let's look at each resource in turn.","title":"Resources"},{"location":"chapter9/arm/#the-sql-database","text":"There are actually three resources for the SQL Database - the server, the database and the firewall rule to allow other resources to connect to it: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"location\": { \"type\": \"string\", \"defaultValue\": \"South Central US\" }, \"sqlServerAdminLogin\": { \"type\": \"string\", \"defaultValue\": \"azure\" }, \"sqlServerAdminPassword\": { \"type\": \"securestring\" }, \"sqlDbSizeMb\" { \"type\": \"int\", \"minValue\": 1, \"maxValue\": 20, \"description\": \"Size of the database in Mb\", \"defaultValue\": 2 }, \"sqlDbCollation\": { \"type\": \"string\", \"defaultValue\": \"SQL_Latin1_General_CP1_CI_AS\" }, \"sqlDbEdition\": { \"type\": \"string\", \"defaultValue\": \"Basic\" }, \"sqlDbServiceObjectiveId\": { \"type\": \"string\", \"defaultValue\": \"dd6d99bb-f193-4ec1-86f2-43d3bccbc49c\" }, }, \"variables\": { \"sqlDbSize\": \"[mul(parameters('sqlDbSizeMb'), 1048576)]\", \"sqlServerName\": \"[concat(parameters('serviceName), '-sql')]\", \"sqlDbName\": \"[concat(parameters('serviceName), '-db')]\" }, \"resources\": [ { \"apiVersion\": \"2014-04-01-preview\", \"name\": \"[variables('sqlServerName')]\", \"type\": \"Microsoft.Sql/servers\", \"location\": \"[parameters('location')]\", \"properties\": { \"administratorLogin\": \"[parameters('sqlServerAdminLogin')]\", \"administratorPassword\": \"[parameters('sqlServerAdminPassword')]\" }, \"resources\": [ { \"apiVersion\": \"2015-05-01-preview\", \"name\": \"[variables('sqlDbName')]\", \"type\": \"databases\", \"location\": \"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\" ], \"properties\": { \"edition\": \"[parameters('sqlDbEdition')]\", \"collation\": \"[parameters('sqlDbCollation')]\", \"maxSizeBytes\": \"[variables('sqlDbSize')]\", \"requestedServiceObjectiveId\": \"[parameters('sqlDbServiceObjectiveId')]\" } }, { \"apiVersion\": \"2014-04-01-preview\", \"name\": \"SQLServerFirewallRules\", \"type\": \"firewallrules\", \"location\": \"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\" ], \"properties\": { \"endIpAddress\": \"0.0.0.0\", \"startIpAddress\": \"0.0.0.0\" } } ] }, ] } This is a fairly lengthy example, but it incorporates a few things. Firstly, note that I use the [variables()] and [parameters()] to bring in the variables and parameters respectively. I also have embedded resources here. The resource heirarchy is a tree-structure. Each embedded resource has a dependsOn array that has the parent as a dependent resource. You are probably wondering where you can get this information for any resource. Microsoft Azure has a site called resources.azure.com . This contains the resources that you currently have configured on your subscription. You can set the deployment up by hand and then look at the resources to get the details: In this example, I'm showing the resource for the database. I can extract the various pieces based on what I have set to get to this point - everything else is defaulted. In addition, note that I can see the apiVersion at the top of the resource manager within the URL for the resource definition.","title":"The SQL Database"},{"location":"chapter9/arm/#the-azure-app-service-plan","text":"My next resource is the App Service Plan. Unlike the database (which has a definite heirarchy), the App Service Plan is separate because many App Services can use the same App Service Plan. You can think of the App Service Plan as the set of virtual machines that belong in a single scale set, and the App Services as the web sites running on top of that scale set. The App Service Plan looks like this: { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"location\": { \"type\": \"string\", \"defaultValue\": \"South Central US\" }, \"sku\": { \"type\": \"string\", \"allowedValues\": [ \"Free\", \"Shared\", \"Basic\", \"Standard\" ], \"defaultValue\": \"Basic\" }, \"workerSize\": { \"type\": \"string\", \"allowedValues\": [ \"0\", \"1\", \"2\" ], \"defaultValue\": \"0\" } }, \"variables\": { \"appServicePlanName\": \"[concat(parameters('serviceName), '-plan')]\", }, \"resources\": [ { \"apiVersion\": \"2015-08-01\", \"name\": \"[variables('appServicePlanName')]\", \"type\": \"Microsoft.Web/serverFarms\", \"location\": \"[parameters('location')]\", \"properties\": { \"name\": \"[parameters('appServicePlanName')]\", \"sku\": \"[parameters('sku')]\", \"workerSize\": \"[parameters('workerSize')]\", \"numberOfWorkers\": 1 } } ] } The hostingPlanName is the name of the hosting plan. This is normally something like F1, B2, or P3. The first letter indicates the SKU and the number indicates the size of the worker. The size of the worker is always one less than the number that is displayed within the Azure portal.","title":"The Azure App Service Plan"},{"location":"chapter9/arm/#the-azure-app-service","text":"The Azure App Service is { \"$schema\": \"http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"serviceName\": { \"type\": \"string\" }, \"location\": { \"type\": \"string\", \"defaultValue\": \"South Central US\" }, \"sqlServerAdminLogin\": { \"type\": \"string\", \"defaultValue\": \"azure\" }, \"sqlServerAdminPassword\": { \"type\": \"securestring\" }, \"deploymentRepoUrl\": { \"type\": \"string\" }, \"deploymentRepoBranch\": { \"type\": \"string\", \"defaultValue\": \"main\" } }, \"variables\": { \"appServiceName\": \"parameters('serviceName')\", \"appServicePlanName\": \"[concat(parameters('serviceName), '-plan')]\", \"sqlServerName\": \"[concat(parameters('serviceName), '-sql')]\", \"sqlDbName\": \"[concat(parameters('serviceName), '-db')]\" }, \"resources\": [ { \"apiVersion\": \"2015-04-01\", \"name\": \"[variables('appServiceName')]\", \"type\": \"Microsoft.Web/sites\", \"location\": \"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/serverFarms', parameters('appServicePlanName'))]\" ], \"properties\": { \"serverFarmId\": \"[parameters('appServicePlanName')]\" }, \"resources\": [ { \"apiVersion\": \"2015-04-01\", \"name\": \"connectionstrings\", \"type\": \"config\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/Sites', variables('appServiceName'))]\", \"[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\" ], \"properties\": { \"MS_TableConnectionString\": { \"value\": \"[concat('Data Source=tcp:', reference(concat('Microsoft.Sql/servers/', variables('sqlServerName'))).fullyQualifiedDomainName, ',1433;Initial Catalog=', variables('sqlDbName'), ';User Id=', parameters('sqlServerAdminLogin'), '@', parameters('sqlServerName'), ';Password=', parameters('sqlServerAdminPassword'), ';')]\", \"type\": \"SQLAzure\" } } }, { \"apiVersion\": \"2015-04-01\", \"name\": \"web\", \"type\": \"sourcecontrols\", \"dependsOn\": [ \"[resourceId('Microsoft.Web/Sites', variables('appServiceName'))]\", \"[resourceId('Microsoft.Web/Sites/config', variables('appServiceName'), 'connectionstrings')]\" ], \"properties\": { \"RepoUrl\": \"[parameters('deploymentRepoUrl')]\", \"branch\": \"[parameters('deploymentRepoBranch')]\" } } ] }, ] } In this case, we've got two dependent resources - one to set up the connection string to the database and the other to set up continuous deployment. Both of those dependent resources have the normal dependsOn terms to ensure that the resources linked exist. You can combine all three sections - the SQL Server, App Service Plan and App Service together. There are overlapping parameters and variables. You just need one copy.","title":"The Azure App Service"},{"location":"chapter9/arm/#deploying-an-arm-template-on-windows","text":"Creating an Azure Resource Manager template on Windows can be done several ways, but my favorite is to use Visual Studio. Start up Visual Studio 2017 and open your solution. Right-click your solution and select Add -> New Project... . Select Installed > Visual C# > Cloud > Azure Resource Group . Give the project a name, then click OK Select a template (I use Web app + SQL normally as a starting point), then click OK . Three files are created within the project: Deploy-AzureResourceGroup.ps1 WebSiteSQLDatabase.json (or similar) WebSiteSQLDatbase.parameters.json (or similar) The WebSiteSQLDatabase.json is the ARM template. The .parameters.json file contains the parameters for an unattended deployment. Since the parameters contains the SQL server password, we need to be careful to ensure that the database password does not end up checked into source code. Finally, the Deploy-AzureResourceGroup.ps1 file is a handy PowerShell script for deploying to Azure. Running it will be the equivalent of doing a deployment via Visual Studio. You can now edit the ARM template to include any additional resources you need. You can also edit the .parameters.json file to set standard values for any parameters you wish, if they don't change very often, although you won't need to do this as there is an easier way of setting parameters as we shall see. There are two steps to deploying this ARM template: Validation and Deployment. To validate the template, right-click the project and select Validate . Click into the Resource group and select Create new . This allows you to select a resource group name and a location. Once you have done that, click Create . The next step is to click Edit Parameters... . This edit the .parameters.json file: Note that we need to set a hostingPlanName , as indicated by the red exclamation error mark. We can also set any other values we want. Anything that has a null value should be entered. Click Save when complete. Do not save password as plain text It's likely that these files will end up in source control. You should never save passwords as plain text in any file that is going to end up in a source control repository. Finally, click Validate . You may get validation errors - fix the validation errors before continuing. Once you have validated successfully, right-click the deployment project and select Deploy , then select the resource group you entered during validation. You may have to type the SQL administrator password multiple times. Watch the output window for a progress report on your deployment. Once complete, log into the Azure portal to check the resources. Also check that the resources match what you were expecting within the Azure Resource Explorer .","title":"Deploying an ARM Template on Windows"},{"location":"chapter9/arm/#deploying-an-arm-template-on-macos","text":"The Mac does not come with the tools necessary to deploy ARM templates, so you will need to download and install them first. In a Terminal window: adrian$ curl -L https://aka.ms/InstallAzureCli | bash adrian$ exec -l $SHELL The first command does the actual installation. It will ask you questions about where to install and confirm that it can write to files. In general, pressing Enter will do the right thing. The second command restarts your shell. After the installation is complete, run az in your Terminal window to ensure the installation was successful. The next step is to login to Azure with the CLI. Use the az login command from the Terminal window. This command will output a code to use in the next step. Note the code that is produced. Now go to http://aka.ms/devicelogin and enter the code in the box provided. Enter the code, press enter, then click Continue . You will be asked to authenticate to Azure. Once that is complete, the terminal window will continue and you will be logged into Azure. The az login command will provide a little piece of JSON to show you the subscription information to which you connected. Deploying a set of resources is a two-step process on the command-line. Firstly, you need to create a resource group. Then you need to deploy the resources within that resource group. To create a resource group, use: az group create -l westus -n MyResourceGroup You can get a list of locations that are available to you using: az account list-locations | grep name Take a look at the response from the az group create command. Ensure that the provisioningState property is something that indicates success. Now that we have a resource group, we can deploy our resources. Edit the .parameters.json file to contain the values for your parameters. For example: { \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"hostingPlanName\": { \"value\": \"Chapter8-plan\" }, \"skuName\": { \"value\": \"B1\" }, \"skuCapacity\": { \"value\": 1 }, \"administratorLogin\": { \"value\": \"azure\" }, \"administratorLoginPassword\": { \"value\": \"myPassword\" }, \"databaseName\": { \"value\": \"ZUMOAPPDB\" }, \"edition\": { \"value\": \"Basic\" } } } Do not check in passwords! If your parameters file contains password, do not check it in to source code control. You can now deploy the service with the following: az group deployment create --resource-group MyResourceGroup --mode Complete --parameters @WebSiteSQLDatabase.parameters.json --template-file WebSiteSQLDatabase.json --verbose Although I have split this command over multiple lines, you should enter this command on one line. Use --mode to maintain deployments There are two modes for ARM deployments. Complete will remove any resources that are not listed in your ARM template. Incremental will only add resources to the resource group. You can use these two modes to effectively maintain the resources in the resource group using only the command line. At this point, the deployment will kick off. It's a long running process, so expect a delay of a few minutes before it returns information. This deployment is (deliberately) going to fail so we can see what happens when it goes wrong and how to fix it. The response after this first run is this: The deployment operation failed, because it was unable to cleanup. Please see https://aka.ms/arm-debug for usage details. Correlation ID: aed1850d-22ca-4e28-b7fa-83a2ac587867 Start by logging into the Azure portal , then go to the resource group was just created, then click Deployments . Select the latest deployment. Click on the failed request: The failed request is a lot clearer than the original failure message. In this case, the password we entered for the SQL database did not meet the complexity requirements. This is easily updated within the parameters file. Then re-run the deployment.","title":"Deploying an ARM Template on MacOS"},{"location":"chapter9/arm/#creating-the-arm-template","text":"One of the issues with ARM templates is their actual creation. There are three methods of creating ARM templates: Create a new Resource Group within Visual Studio (covered above) Export a template from a Resource Group. Log in to the Azure portal . Select your resource group. In the menu, click Automation Scripts . Click Download . Download a starter template from GitHub . Which ever mechanism you choose to create the template, you will still need to edit the result by hand for your individual requirements.","title":"Creating the ARM Template"},{"location":"chapter9/monitoring/","text":"Monitoring and Troubleshooting \u00b6 The last two topics are joined together - monitoring and troubleshooting. There are many reasons for monitoring: To gain marketing information into your userbase. To detect potential security threats. To detect problems in your code. This latter reason is the reason most developers want to monitor their mobile app. However, there is much to be gained from monitoring your mobile app for marketing purposes, and security should be a top-of-mind issue for everyone with an app that others use. Security is generally monitored at the mobile backend. Marketing information is gleaned from monitoring the usage of the mobile client, and problems can occur anywhere in your code base. You should always monitor as close to your mobile backend as possible. If you monitor your mobile backend from another provider (for example), you will be paying for network charges needlessly. Except in rare cases (such as the recently advertised Amazon outages), the cloud provider will keep the cloud operating without any involvement from you. Even in those cases where there is an outage at the cloud provider, you will not be able to prevent the outage simply by having monitoring elsewhere. Since the preferred hosting platform for Azure Mobile Apps is Azure App Service, this means using the analytics platform provided by Azure. This consists of two parts: Application Insights for monitoring and troubleshooting the mobile backend. Visual Studio Mobile Center for monitoring and troubleshooting the mobile client. Monitoring your Backend \u00b6 There is a lot of monitoring built into Azure App Service . For these tools, you don't have to do anything. They are always available. However, there are certain pieces of information that are not going to be available. For instance, you will not be able to correlate uploads with users since Azure App Service really doesn't know anything about users. For this reason, it's a good idea to integrate Application Insights into your mobile backend. This is a two-step process: Create an Application Insights instance in your Azure resource group. Add Application Insights to your mobile backend project. There is more to Application Insights than the basics we are going to cover here, including custom events , filtering , and integration with other platforms . Application Insights is a powerful service and we will only be able to scratch the surface of it. Automate Application Insights creation with ARM Just like all other Azure resources, you can create an Application Insights with an automated deployment via an ARM template. Create an Application Insights Instance \u00b6 Start by opening the Azure portal , then: Open your resource group. Click Add . Search for then click Application Insights . Click Create . Enter a name for the resource. Pick ASP.NET web application for the application type. Select the nearest location to your mobile backend. Application Insights isn't everywhere Application Insights can only be placed in larger regions where storage and compute capacity is large. That means your Application Insights will probably not be in the same location as your mobile backend. Once you have set the required configuration, click Create . There won't be any data until you have connected your mobile backend to the Application Insights resource. Add Application Insights to your Mobile Backend \u00b6 Let's continue with our cloud backend that we developed for Chapter 8 . Open your solution in Visual Studio, right-click the project and select Add -> Application Insights Telemetry... . Once added, the configuration process will start. Click Start Free . If you need to, select and authenticate to the right Azure account and select your subscription. In the Resource drop-down, select the Application Insights resource you just created. Then click Register . This will add the appropriate NuGet packages to your project and configure most of what is required in the source code. Once the process is complete, click Finish . You can optionally enable trace collection. This allows you to search the output from System.Diagnostics , enabling you to capture even more information. Diagnostic data is generally verbose and slows the service down. The free tier of Application Insights is limited to 1Gb/month, so this step is optional. Your last remaining step is to publish your site to Azure App Service, which we have done many times during the course of the book. Viewing Application Insights Data \u00b6 The Application Insights data is available through the Azure portal or within Visual Studio. Before we go any further, generate some Application Insights data by using your mobile app. Once done, you can review the Application Insights data by right-clicking on your backend project and choosing Application Insights -> Open Application Insights Portal . The same page is available by clicking the Application Insights resource in your resource group. Application Insights is constantly changing, so take some time to learn about the metrics and reports you can view using the portal. Monitoring your Mobile Clients \u00b6 Monitoring your mobile client is important to gather usage and marketing data about your users. You can also find out how your users are actually using your mobile app. This allows you to target additional improvements in areas that people find useful and perhaps remove or improve areas that are less used. To integrate monitoring, first create a Visual Studio Mobile Center app as we described in the Testing section. Return to the Visual Studio Mobile Center and select your app. If you were at another beacon, click Getting Started . Click Manage app . Copy the App secret somewhere accessible - we will need it in a few moments. Open the mobile app project in Visual Studio 2017 or Visual Studio for Mac. Right-click the solution and select Manage NuGet Packages for Solution... . Click Browse , then search for Microsoft.Azure.Mobile.Analytics . Install the Microsoft.Azure.Mobile.Analytics package into the Android and iOS projects. Once the package(s) are installed, open App.cs in the shared project. Add the MobileCenter.Start line at the start of the constructor: using Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { MobileCenter.Start(\"android=609b2734-0353-4e71-a654-fedd9df1632a\", typeof(Analytics)); #if USE_MOCK_SERVICES CloudService = new MockCloudService(); #else CloudService = new AzureCloudService(); #endif MainPage = new NavigationPage(new Pages.EntryPage()); } } } When you start your app, the analytics service will send a session start message to Visual Studio Mobile Center, allowing you to get basic demographics about your mobile app usage, such as how many distinct users you have, where they are located, and what device(s) they are using. To get this level of information, visit the Analytics page in the Visual Studio Mobile Center portal. You can get better analytics by defining custom events. An event is a custom interaction with the content in your app that allows you to better understand your user's behavior. For example, you may want to understand what sort of things your users are uploading - are they uploading from the camera or picking a picture? You will not be able to get this information from the server. The server will only be able to tell that the user uploaded a picture. To use custom events, you can use the trackEvent() method, like this: // at the top of the file: // using Microsoft.Azure.Mobile.Analytics Analytics.TrackEvent(\"FILE_UPLOAD\", new Dictionary<string, string> { { \"Source\", \"Camera\" }, { \"FileName\", \"pic1.jpg\" } }); To analyze custom events, go to the Events page in the Visual Studio Mobile Center portal. The events are attached to a session so you can report on the number of distinct users using a camera for upload, for example. Custom Events are limited by your imagination, but they do have a cost associated with them - each event does cost a small amount of bandwidth to the user. Other Analytics Limits You can only produce 200 custom event types per app. The \"FILE_UPLOAD\" is an event type. There is a total maximum of 256 characters per event type. Each property name and value must be less than 64 characters. Crash Reporting \u00b6 Once your application is out in the wild, whether it be with your beta testers, your employees or the general public via a public app store, you want to ensure it is working. You can do all the testing available to you and there will still be some combination of factors that may cause your application to crash. Your users will most likely just delete your application at that point, so it's a good idea to collect the cause of the crash before they do that. To integrate crash reporting, add the Mobile Center SDK to your project, then add a single line of code to your application. We've already created a Visual Studio Mobile Center app for beta distribution (which we covered above), so we'll use the same application. Return to the Visual Studio Mobile Center and select your app. If you were at another beacon, click Getting Started . Click Manage app . Copy the App secret somewhere accessible - we will need it in a few moments. Open the mobile app project in Visual Studio 2017 or Visual Studio for Mac. Right-click the solution and select Manage NuGet Packages for Solution... . Click Browse , then search for Microsoft.Azure.Mobile.Crashes . Install the Microsoft.Azure.Mobile.Crashes package into the Android and iOS projects. If you have both an iOS and Android app, make note of BOTH app secrets - one from each app within Mobile Center. Once the package(s) are installed, open App.cs in the shared project. Add the MobileCenter.Start line at the start of the constructor: using Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using Microsoft.Azure.Mobile.Crashes; using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { MobileCenter.Start(\"android=609b2734-0353-4e71-a654-fedd9df1632a\", typeof(Analytics), typeof(Crashes)); #if USE_MOCK_SERVICES CloudService = new MockCloudService(); #else CloudService = new AzureCloudService(); #endif MainPage = new NavigationPage(new Pages.EntryPage()); } } } I am combining analytics with crash reporting here. You can choose to integrate analytics, crash reporting or both by adjusting the MobileCenter.Start() call. Replace the app secret placeholders with your app secrets. Eventually, your app is going to crash. When this happens, the crash will appear in the Crashes service within Mobile Center, allowing you to see what type of mobile device was being used, the version of the app being run and the stack trace of the app at that point. Visual Studio Mobile Center also groups like crashes together so that you can see commonality between crashes and better target your bug fixing. Best Practices \u00b6 There is, thankfully, a short list of best practices to close out the book: Integrate Analytics and Crash Reporting in every mobile app. Integrate Application Insights into your mobile backend. Investigate every single crash reported by your mobile app. Use UI Testing to re-create the crash. Use a bug tracking system such as the one integrated into GitHub or Visual Studio Team Services. Investigate every 500 Internal Server Error produced by your mobile backend. It means your server crashed. And in closing \u00b6 Thank you for taking the time to read this book. I hope you found it informative. Now go and make awesome mobile apps!","title":"Monitoring and Troubleshooting"},{"location":"chapter9/monitoring/#monitoring-and-troubleshooting","text":"The last two topics are joined together - monitoring and troubleshooting. There are many reasons for monitoring: To gain marketing information into your userbase. To detect potential security threats. To detect problems in your code. This latter reason is the reason most developers want to monitor their mobile app. However, there is much to be gained from monitoring your mobile app for marketing purposes, and security should be a top-of-mind issue for everyone with an app that others use. Security is generally monitored at the mobile backend. Marketing information is gleaned from monitoring the usage of the mobile client, and problems can occur anywhere in your code base. You should always monitor as close to your mobile backend as possible. If you monitor your mobile backend from another provider (for example), you will be paying for network charges needlessly. Except in rare cases (such as the recently advertised Amazon outages), the cloud provider will keep the cloud operating without any involvement from you. Even in those cases where there is an outage at the cloud provider, you will not be able to prevent the outage simply by having monitoring elsewhere. Since the preferred hosting platform for Azure Mobile Apps is Azure App Service, this means using the analytics platform provided by Azure. This consists of two parts: Application Insights for monitoring and troubleshooting the mobile backend. Visual Studio Mobile Center for monitoring and troubleshooting the mobile client.","title":"Monitoring and Troubleshooting"},{"location":"chapter9/monitoring/#monitoring-your-backend","text":"There is a lot of monitoring built into Azure App Service . For these tools, you don't have to do anything. They are always available. However, there are certain pieces of information that are not going to be available. For instance, you will not be able to correlate uploads with users since Azure App Service really doesn't know anything about users. For this reason, it's a good idea to integrate Application Insights into your mobile backend. This is a two-step process: Create an Application Insights instance in your Azure resource group. Add Application Insights to your mobile backend project. There is more to Application Insights than the basics we are going to cover here, including custom events , filtering , and integration with other platforms . Application Insights is a powerful service and we will only be able to scratch the surface of it. Automate Application Insights creation with ARM Just like all other Azure resources, you can create an Application Insights with an automated deployment via an ARM template.","title":"Monitoring your Backend"},{"location":"chapter9/monitoring/#create-an-application-insights-instance","text":"Start by opening the Azure portal , then: Open your resource group. Click Add . Search for then click Application Insights . Click Create . Enter a name for the resource. Pick ASP.NET web application for the application type. Select the nearest location to your mobile backend. Application Insights isn't everywhere Application Insights can only be placed in larger regions where storage and compute capacity is large. That means your Application Insights will probably not be in the same location as your mobile backend. Once you have set the required configuration, click Create . There won't be any data until you have connected your mobile backend to the Application Insights resource.","title":"Create an Application Insights Instance"},{"location":"chapter9/monitoring/#add-application-insights-to-your-mobile-backend","text":"Let's continue with our cloud backend that we developed for Chapter 8 . Open your solution in Visual Studio, right-click the project and select Add -> Application Insights Telemetry... . Once added, the configuration process will start. Click Start Free . If you need to, select and authenticate to the right Azure account and select your subscription. In the Resource drop-down, select the Application Insights resource you just created. Then click Register . This will add the appropriate NuGet packages to your project and configure most of what is required in the source code. Once the process is complete, click Finish . You can optionally enable trace collection. This allows you to search the output from System.Diagnostics , enabling you to capture even more information. Diagnostic data is generally verbose and slows the service down. The free tier of Application Insights is limited to 1Gb/month, so this step is optional. Your last remaining step is to publish your site to Azure App Service, which we have done many times during the course of the book.","title":"Add Application Insights to your Mobile Backend"},{"location":"chapter9/monitoring/#viewing-application-insights-data","text":"The Application Insights data is available through the Azure portal or within Visual Studio. Before we go any further, generate some Application Insights data by using your mobile app. Once done, you can review the Application Insights data by right-clicking on your backend project and choosing Application Insights -> Open Application Insights Portal . The same page is available by clicking the Application Insights resource in your resource group. Application Insights is constantly changing, so take some time to learn about the metrics and reports you can view using the portal.","title":"Viewing Application Insights Data"},{"location":"chapter9/monitoring/#monitoring-your-mobile-clients","text":"Monitoring your mobile client is important to gather usage and marketing data about your users. You can also find out how your users are actually using your mobile app. This allows you to target additional improvements in areas that people find useful and perhaps remove or improve areas that are less used. To integrate monitoring, first create a Visual Studio Mobile Center app as we described in the Testing section. Return to the Visual Studio Mobile Center and select your app. If you were at another beacon, click Getting Started . Click Manage app . Copy the App secret somewhere accessible - we will need it in a few moments. Open the mobile app project in Visual Studio 2017 or Visual Studio for Mac. Right-click the solution and select Manage NuGet Packages for Solution... . Click Browse , then search for Microsoft.Azure.Mobile.Analytics . Install the Microsoft.Azure.Mobile.Analytics package into the Android and iOS projects. Once the package(s) are installed, open App.cs in the shared project. Add the MobileCenter.Start line at the start of the constructor: using Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { MobileCenter.Start(\"android=609b2734-0353-4e71-a654-fedd9df1632a\", typeof(Analytics)); #if USE_MOCK_SERVICES CloudService = new MockCloudService(); #else CloudService = new AzureCloudService(); #endif MainPage = new NavigationPage(new Pages.EntryPage()); } } } When you start your app, the analytics service will send a session start message to Visual Studio Mobile Center, allowing you to get basic demographics about your mobile app usage, such as how many distinct users you have, where they are located, and what device(s) they are using. To get this level of information, visit the Analytics page in the Visual Studio Mobile Center portal. You can get better analytics by defining custom events. An event is a custom interaction with the content in your app that allows you to better understand your user's behavior. For example, you may want to understand what sort of things your users are uploading - are they uploading from the camera or picking a picture? You will not be able to get this information from the server. The server will only be able to tell that the user uploaded a picture. To use custom events, you can use the trackEvent() method, like this: // at the top of the file: // using Microsoft.Azure.Mobile.Analytics Analytics.TrackEvent(\"FILE_UPLOAD\", new Dictionary<string, string> { { \"Source\", \"Camera\" }, { \"FileName\", \"pic1.jpg\" } }); To analyze custom events, go to the Events page in the Visual Studio Mobile Center portal. The events are attached to a session so you can report on the number of distinct users using a camera for upload, for example. Custom Events are limited by your imagination, but they do have a cost associated with them - each event does cost a small amount of bandwidth to the user. Other Analytics Limits You can only produce 200 custom event types per app. The \"FILE_UPLOAD\" is an event type. There is a total maximum of 256 characters per event type. Each property name and value must be less than 64 characters.","title":"Monitoring your Mobile Clients"},{"location":"chapter9/monitoring/#crash-reporting","text":"Once your application is out in the wild, whether it be with your beta testers, your employees or the general public via a public app store, you want to ensure it is working. You can do all the testing available to you and there will still be some combination of factors that may cause your application to crash. Your users will most likely just delete your application at that point, so it's a good idea to collect the cause of the crash before they do that. To integrate crash reporting, add the Mobile Center SDK to your project, then add a single line of code to your application. We've already created a Visual Studio Mobile Center app for beta distribution (which we covered above), so we'll use the same application. Return to the Visual Studio Mobile Center and select your app. If you were at another beacon, click Getting Started . Click Manage app . Copy the App secret somewhere accessible - we will need it in a few moments. Open the mobile app project in Visual Studio 2017 or Visual Studio for Mac. Right-click the solution and select Manage NuGet Packages for Solution... . Click Browse , then search for Microsoft.Azure.Mobile.Crashes . Install the Microsoft.Azure.Mobile.Crashes package into the Android and iOS projects. If you have both an iOS and Android app, make note of BOTH app secrets - one from each app within Mobile Center. Once the package(s) are installed, open App.cs in the shared project. Add the MobileCenter.Start line at the start of the constructor: using Microsoft.Azure.Mobile; using Microsoft.Azure.Mobile.Analytics; using Microsoft.Azure.Mobile.Crashes; using TaskList.Abstractions; using TaskList.Services; using Xamarin.Forms; namespace TaskList { public class App : Application { public static ICloudService CloudService { get; set; } public App() { MobileCenter.Start(\"android=609b2734-0353-4e71-a654-fedd9df1632a\", typeof(Analytics), typeof(Crashes)); #if USE_MOCK_SERVICES CloudService = new MockCloudService(); #else CloudService = new AzureCloudService(); #endif MainPage = new NavigationPage(new Pages.EntryPage()); } } } I am combining analytics with crash reporting here. You can choose to integrate analytics, crash reporting or both by adjusting the MobileCenter.Start() call. Replace the app secret placeholders with your app secrets. Eventually, your app is going to crash. When this happens, the crash will appear in the Crashes service within Mobile Center, allowing you to see what type of mobile device was being used, the version of the app being run and the stack trace of the app at that point. Visual Studio Mobile Center also groups like crashes together so that you can see commonality between crashes and better target your bug fixing.","title":"Crash Reporting"},{"location":"chapter9/monitoring/#best-practices","text":"There is, thankfully, a short list of best practices to close out the book: Integrate Analytics and Crash Reporting in every mobile app. Integrate Application Insights into your mobile backend. Investigate every single crash reported by your mobile app. Use UI Testing to re-create the crash. Use a bug tracking system such as the one integrated into GitHub or Visual Studio Team Services. Investigate every 500 Internal Server Error produced by your mobile backend. It means your server crashed.","title":"Best Practices"},{"location":"chapter9/monitoring/#and-in-closing","text":"Thank you for taking the time to read this book. I hope you found it informative. Now go and make awesome mobile apps!","title":"And in closing"}]}